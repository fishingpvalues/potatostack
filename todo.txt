rsync -avhP --info=progress2 /mnt/g/ daniel@192.168.178.158:/mnt/storage2/media/adult/

- check if the beets integration has happened proerly and better it Now I have all the information needed. Here's the comprehensive integration plan:
---
Beets Integration Plan for PotatoStack
Overview
Add beets (music library manager) to:
1. Organize/tag music from downloads → music library
2. Integrate with existing Navidrome (streaming)
3. Integrate with existing slskd (Soulseek sharing)
4. Add to Homer dashboard
5. Add to Tailscale HTTPS serve
---
1. Docker Compose Service
Location: Add after navidrome service (around line 1973)
  # Beets - Music library manager and tagger
  beets:
    image: lscr.io/linuxserver/beets:${BEETS_TAG:-latest}
    container_name: beets
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:8337:8337"
    environment:
      <<: *common-env
    volumes:
      - /mnt/ssd/docker-data/beets:/config
      - /mnt/storage/media/music:/music        # Library destination (RW)
      - /mnt/storage2/downloads/slskd:/downloads/slskd:ro
      - /mnt/storage2/downloads/torrents:/downloads/torrents:ro
      - /mnt/storage2/downloads/aria2:/downloads/aria2:ro
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f beets || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 128M
    labels:
      - "wud.trigger.docker.update=true"
Key decisions:
- Port 8337 (beets default WebUI port - not used in your stack)
- RW mount for /music (beets needs to write organized files)
- RO mounts for download sources (safer, beets only reads)
- Regular network (not behind gluetun - no VPN needed for tagging)
---
2. Beets Configuration (config/beets/config.yaml)
directory: /music
library: /config/library.db
import:
  copy: yes
  write: yes
  move: no
  timid: no
  autotag: yes
  quiet: no
  log: /config/beets.log
musicbrainz:
  searchlimit: 5
paths:
  default: $albumartist/$year - $album/$track - $title
  singleton: Singles/$artist - $title
  comp: Compilations/$album/$track - $title
item_fields:
  albumartist_multivalued: albumartist
plugins:
  - fetchart
  - embedart
  - lyrics
  - lastgenre
  - mbsync
  - web
  - duplicates
fetchart:
  auto: yes
  sources: coverart musicbrainz amazon albumart
  cover_names: cover folder front
web:
  host: 0.0.0.0
  port: 8337
replace:
  '[\\/]': _
  '^\.': _
  '[\x00-\x1f]': _
  '[<>:"\?\*\|]': _
  '\.$': ''
  '\s+$': ''
  '^\s+': ''
per_disc_numbering: yes
---
3. Tailscale HTTPS Integration
File: scripts/init/tailscale-serve-https.sh (no changes needed)
Update docker-compose.yml in two places:
tailscale-https-setup (line ~546):
TAILSCALE_SERVE_PORTS: "7575,8088,3000,3001,3002,8086,8096,8989,7878,8686,9696,6767,13378,4533,9900,8282,8076,2234,8097,8008,2283,8090,8095,8384,3004,3006,8888,8093,5006,9090,3100,9093,10903,10902,8087,6060,8001,8788,8060,8889,8081,3010,3013,8180,8181,9925,5984,9898,8123,8191,3333,3335,5656,8085,8337"
tailscale-https-monitor (line ~575):
TAILSCALE_SERVE_PORTS: 7575,8088,3000,3001,3002,8086,8096,8989,7878,8686,9696,6767,13378,4533,9900,8282,8076,2234,8097,8008,2283,8090,8095,8384,3004,3006,8888,8093,5006,9090,3100,9093,10903,10902,8087,6060,8001,8788,8060,8889,8081,3010,3013,8180,8181,9925,5984,9898,8123,8191,3333,3335,5656,8085,8337
URL: https://potatostack.tale-iwato.ts.net:8337
---
4. Homer Dashboard
File: config/homer/config.yml
Add to the "Media" section (after Navidrome):
      - name: "Beets"
        logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/beets.png"
        subtitle: "Music Library Manager"
        url: "https://potatostack.tale-iwato.ts.net:8337"
        target: "_blank"
---
5. Storage Init Script
File: scripts/init/init-storage.sh - add beets data directory creation
Add to the SSD docker-data section:
mkdir -p /mnt/ssd/docker-data/beets
---
6. Environment Variables
File: .env.example - add tag:
# Music Management
BEETS_TAG=latest
---
7. Workflow Integration
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│   slskd         │────▶│   downloads/    │────▶│     beets       │
│   (Soulseek)    │     │   slskd/        │     │   (tag/organize)│
└─────────────────┘     └─────────────────┘     └────────┬────────┘
                                                         │
┌─────────────────┐     ┌─────────────────┐              │
│   unpackerr     │────▶│   extracted     │──────────────┘
│   (archives)    │     │   files         │
└─────────────────┘     └─────────────────┘
                                                         │
                        ┌─────────────────┐              ▼
                        │ /mnt/storage/   │     ┌─────────────────┐
                        │ media/music/    │◀────│ organized FLAC  │
                        │ (library)       │     │ Artist/Album    │
                        └────────┬────────┘     └─────────────────┘
                                 │
              ┌──────────────────┼──────────────────┐
              │                  │                  │
              ▼                  ▼                  ▼
     ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐
     │   Navidrome     │ │     slskd       │ │    Jellyfin     │
     │   (streaming)   │ │   (share:ro)    │ │   (streaming)   │
     └─────────────────┘ └─────────────────┘ └─────────────────┘
Key integration points:
1. beets imports from /mnt/storage2/downloads/slskd (slskd downloads)
2. beets organizes into /mnt/storage/media/music (library)
3. Navidrome reads /mnt/storage/media/music:ro (already configured)
4. slskd shares /mnt/storage/media/music:ro (already configured)
5. Jellyfin already mounts music library
---
8. Files to Create/Modify
| File | Action |
|------|--------|
| docker-compose.yml | Add beets service |
| docker-compose.yml | Add 8337 to TAILSCALE_SERVE_PORTS (2 places) |
| config/beets/config.yaml | CREATE - beets configuration |
| config/homer/config.yml | Add beets entry to Media section |
| scripts/init/init-storage.sh | Add beets directory creation |
| .env.example | Add BEETS_TAG=latest |
---
Questions Before Implementation
→ Asked 2 questions

# Beets + Navidrome for music, not video integration plan

**Session ID:** ses_37e046bc9ffePEKekhJF2WvDrl
**Created:** 2/21/2026, 9:54:34 PM
**Updated:** 2/21/2026, 10:01:12 PM

---

## User

plan and fully integrate this into my stack in homer, tailscale etc everywhere **No, Navidrome cannot catalog, tag, order, or create folders for videos.**  

Navidrome is strictly a **music-only** server (Subsonic/OpenSubsonic compatible). Official docs explicitly state it will not implement any video functionality—it's focused 100% on audio files (FLAC, MP3, Opus, etc.). It scans your existing music folders using tags/metadata but does **not** rewrite tags, rename files, or auto-create/reorganize folders. It just reads what you already have on disk.

### Best Lightweight Linux Docker Tool for Your Needs
**beets** (via the official `linuxserver/beets` Docker image) is the clear SOTA choice here.  

- **Why it fits perfectly**:  
  - Super lightweight (Alpine-based, low RAM/CPU).  
  - Runs headless in Docker on Linux.  
  - Catalogs your entire library.  
  - Fetches missing/completes titles, artists, years, genres, album art, etc. from **MusicBrainz** (the best free, open, community-driven metadata database/API—no limits, no keys needed for normal use).  
  - Uses AcoustID fingerprinting (also free) if any files need matching beyond existing tags.  
  - Automatically moves/renames/creates folders based on your template (even if files are already tagged/lossless).  
  - Handles huge libraries without issues.  

Your music is already lossless + mostly tagged, so beets will mostly just **standardize + fill gaps** (titles, years, art, etc.) and reorganize everything cleanly. No paid APIs required.

**Quick Docker setup example** (docker-compose.yml):
```yaml
services:
  beets:
    image: linuxserver/beets:latest
    container_name: beets
    volumes:
      - ./config:/config
      - /path/to/your/music:/music
      - /path/to/downloads/staging:/downloads  # optional for new stuff
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    restart: unless-stopped
```

Mount your music dir, edit `/config/config.yaml` with your preferred path format (see below), then run `beet import /music` (or `-A` for auto). Done.

### SOTA Folder Structure for Soulseek (Maximizes Search + Discoverability)
Soulseek users search by keywords in filenames/paths and love **browsing clean, consistent artist/album folders**. Messy or flat structures get ignored or complained about.

**Recommended structure** (enforceable in beets config):
```
/Music
  /Artist Name
    /2023 - Album Name
      01 - Track Title.flac
      02 - Track Title.flac
      ...
      folder.jpg          # album art (beets auto-fetches)
    /1998 - Another Album
      ...
  /Various Artists      # for compilations
    /2024 - Cool Compilation
      01 - Artist - Track.flac
```

**Beets path format** (put this in `config.yaml`):
```yaml
paths:
  default: $albumartist/$year - $album/$track - $title
  singleton: Singles/$artist - $title
  comp: Compilations/$album/$track - $title
```

- Use `$albumartist` (not just `$artist`) for correct sorting on collaborations/compilations.  
- Add year in folder name → super helpful for users.  
- Numbered tracks + clean titles → easy filename searches.  
- Keep FLAC lossless (beets preserves it).  
- beets can also embed art or keep `folder.jpg`/`cover.jpg`.

This is the community gold standard in 2025/2026 (seen across Reddit/Soulseek forums, MusicBee/Picard users, etc.). It makes your share look professional—users will queue more from you and respect the library.

### Full Recommended Stack (All Lightweight Docker)
- **beets** → organize + tag your collection once.  
- **Navidrome** → stream it yourself beautifully (web + apps).  
- **slskd** (or Nicotine++ if you prefer GUI) → headless Soulseek sharing. Share just your `/Music` root folder. slskd has a nice web UI + REST API, perfect for servers.  

There's even **Soulbeet** (newer project) that auto-integrates slskd downloads straight into beets for ongoing management.

### Quick Tips to Maximize Soulseek Finds
- Complete albums only (users hate partials).  
- Good album art in every folder.  
- Consistent spelling (MusicBrainz fixes this).  
- Share the whole clean root folder (not scattered subfolders).  
- Optional: genre subfolders if you want (`/Genre/Artist/...`), but most power users prefer pure artist/album.  

MusicBrainz + beets is still the undisputed free metadata king in 2026—no better open source alternative exists for accuracy + bulk power.

Run beets once on your collection and you'll have the cleanest, most searchable Soulseek share possible. Let me know if you want the exact beets config snippet or compose files! 

---

## Assistant (Plan · glm-5-free · 162.3s)

_Thinking:_

The user wants to integrate beets into their existing PotatoStack infrastructure. This is a comprehensive task that involves:

1. Adding beets to docker-compose.yml
2. Configuring beets properly for their music library
3. Integrating with Navidrome (already in stack)
4. Integrating with slskd (already in stack for Soulseek)
5. Adding to Homer dashboard
6. Potentially adding to Tailscale if they want external access
7. Setting up proper folder structures and mounts

Let me first explore their existing stack to understand:
- Current docker-compose.yml structure
- Where Navidrome is configured and what paths it uses
- Where slskd is configured
- Homer configuration
- Tailscale setup
- Their storage layout (SSD vs HDD)
- Existing music folder locations

I should read the relevant configuration files to understand the current setup.

I'll explore your existing stack to understand how beets should be integrated with Navidrome, slskd, Homer, and Tailscale.

**Tool: task**

**Input:**
```json
{
  "description": "Explore stack for beets integration",
  "prompt": "I need to understand the current PotatoStack infrastructure to plan beets integration. Please find and report on:\n\n1. **docker-compose.yml** - Find the file and identify:\n   - Navidrome service configuration (volumes, ports, networks)\n   - slskd service configuration \n   - Any existing music-related services\n   - The gluetun VPN configuration (since slskd uses it)\n   - Homer service configuration\n   - Tailscale configuration\n\n2. **Storage paths** - Look for:\n   - Where music/media is currently stored (check .env.example or docker-compose for volume mounts)\n   - The storage layout (SSD at /mnt/ssd, HDD at /mnt/storage)\n\n3. **Homer configuration** - Find the homer config file and show the current dashboard structure\n\n4. **Any existing beets configuration** - Check if beets is already partially configured\n\nReturn a structured summary of:\n- Current music stack services and their configurations\n- Existing volume mount paths for music\n- Homer dashboard structure\n- Network configuration (especially for services behind gluetun)\n- Any gaps or issues to address",
  "subagent_type": "explore"
}
```

**Output:**
```
task_id: ses_37e03fdd6ffedRw7CyEdwke3U6 (for resuming to continue this task if needed)

<task_result>
Now I have a complete picture. Here's a structured summary of the PotatoStack infrastructure for beets integration planning:

---

## PotatoStack Infrastructure Summary for Beets Integration

### 1. Current Music Stack Services

#### **Navidrome** (Music Streaming Server)
| Property | Value |
|----------|-------|
| Image | `deluan/navidrome:${NAVIDROME_TAG:-latest}` |
| Port | `4533` (Tailscale HTTPS enabled) |
| Config | `/mnt/ssd/docker-data/navidrome:/data` |
| Music Library | `/mnt/storage/media/music:/music:ro` (READ-ONLY) |
| Network | `potatostack` (regular Docker network) |
| Scan Schedule | Every 1 hour |

**Key Environment Variables:**
- `ND_SCANSCHEDULE: 1h`
- `ND_ENABLETRANSCODINGCONFIG: "true"`
- `ND_ENABLESHARING: "true"`

---

#### **slskd** (Soulseek P2P Client)
| Property | Value |
|----------|-------|
| Image | `ghcr.io/slskd/slskd:${SLSKD_TAG:-latest}` |
| Network Mode | `service:gluetun` (BEHIND VPN) |
| WebUI Port | `2234` (via gluetun) |
| Peer Port | `50000` (via gluetun) |
| Downloads | `/mnt/storage2/downloads/slskd` |
| Incomplete | `/mnt/storage2/downloads/incomplete/slskd` |
| Shared Files | `/mnt/storage/slskd-shared` |
| **Music Share** | `/mnt/storage/media/music:/music:ro` (READ-ONLY) |
| **Audiobook Share** | `/mnt/storage/media/audiobooks:/audiobooks:ro` |

**Important:** slskd uses `network_mode: "service:gluetun"` which means:
- It cannot resolve Docker container hostnames (e.g., `postgres`, `redis-cache`)
- Uses gluetun's VPN DNS, not Docker's internal DNS
- Any beets integration must account for this network isolation

---

#### **SpotiFLAC** (Spotify to FLAC Downloader)
| Property | Value |
|----------|-------|
| Network Mode | `service:gluetun` (BEHIND VPN) |
| Port | `8097` (via gluetun) |
| Downloads | `/mnt/storage2/downloads` |

---

### 2. Gluetun VPN Configuration

```yaml
gluetun:
  image: qmcgaw/gluetun:${GLUETUN_TAG:-latest}
  cap_add:
    - NET_ADMIN
  ports:
    - "8008:8008"    # Control server
    - "8282:8282"    # qBittorrent
    - "2234:2234"    # slskd WebUI
    - "50000:50000"  # slskd peer
    - "8097:8080"    # SpotiFLAC
  environment:
    VPN_SERVICE_PROVIDER: ${VPN_PROVIDER:-surfshark}
    VPN_TYPE: wireguard
    FIREWALL_OUTBOUND_SUBNETS: ${LAN_NETWORK:-192.168.178.0/24},172.16.0.0/12
    FIREWALL_VPN_INPUT_PORTS: ${VPN_INPUT_PORTS:-51413,50000,6888,3333,3334}
  extra_hosts:
    - "host.docker.internal:host-gateway"
```

**Killswitch Rules** (`config/gluetun/post-rules.txt`):
- Hardened to prevent VPN leaks
- Allows ESTABLISHED traffic only via `tun0` or private subnets

---

### 3. Tailscale Configuration

```yaml
tailscale:
  image: tailscale/tailscale:${TAILSCALE_TAG:-latest}
  hostname: potatostack
  network_mode: host
  environment:
    TS_AUTHKEY: ${TAILSCALE_AUTHKEY:-}
    TS_USERSPACE: "false"
```

**HTTPS Ports Exposed via Tailscale Serve** (including music services):
- `7575` - Homer
- `4533` - Navidrome
- `2234` - slskd
- `8097` - SpotiFLAC
- `8282` - qBittorrent

---

### 4. Storage Layout

| Mount Point | Purpose | Content |
|-------------|---------|---------|
| `/mnt/storage` | HDD (Primary) | Media, downloads, caches |
| `/mnt/storage2` | HDD (16TB) | Movies, TV, adult media |
| `/mnt/ssd` | SSD | Docker data, databases, configs |
| `/mnt/cachehdd` | HDD | Caches, metrics |

**Music-Specific Paths:**
| Path | Usage |
|------|-------|
| `/mnt/storage/media/music` | **PRIMARY MUSIC LIBRARY** - Read by Navidrome, shared via slskd |
| `/mnt/storage2/downloads/slskd` | slskd downloads destination |
| `/mnt/ssd/docker-data/navidrome` | Navidrome database/config |

**Current Music Library Contents (example):**
```
/mnt/storage/media/music/
├── Beatport Top 100 - House (January 2026)/
├── Beatport Top 100 - Melodic House & Techno (January 2026)/
├── Beatport Top 100 - Tech House/
├── Beatport Top 100 - Tech House (January 2026)/
└── Beatport Top 100 - Techno (Peak Time Driving) (January 2026)/
```

---

### 5. Homer Dashboard Structure

**File:** `/home/daniel/potatostack/config/homer/config.yml`

**Media Section:**
- Jellyfin - `https://potatostack.tale-iwato.ts.net:8096`
- Audiobookshelf - `https://potatostack.tale-iwato.ts.net:13378`
- **Navidrome - `https://potatostack.tale-iwato.ts.net:4533`**
- Stash - `https://potatostack.tale-iwato.ts.net:9900`

**Downloads Section:**
- qBittorrent - `https://potatostack.tale-iwato.ts.net:8282`
- **slskd - `https://potatostack.tale-iwato.ts.net:2234`**
- **SpotiFLAC - `https://potatostack.tale-iwato.ts.net:8097`**
- pyLoad - `https://potatostack.tale-iwato.ts.net:8076`

---

### 6. Existing Beets Configuration

**Result: NONE FOUND**

- No `beets` service in docker-compose.yml
- No beets config files in `config/` directory
- No beets-related volumes

---

### 7. Gaps & Issues to Address for Beets Integration

| Issue | Impact | Recommended Solution |
|-------|--------|---------------------|
| **No beets service exists** | Need to add new service | Add beets container to docker-compose.yml |
| **slskd downloads to `/mnt/storage2/downloads/slskd`** | Music scattered across downloads | Configure beets to import from this path |
| **Navidrome reads `/mnt/storage/media/music` as READ-ONLY** | Need organized library | Beets should organize TO this path |
| **slskd behind gluetun VPN** | Cannot access Docker DNS | If beets needs to talk to slskd API, use `172.16.0.0/12` IP or `host.docker.internal` |
| **No Lidarr enabled** | No automated music management | Beets can fill this gap for tagging/organization |
| **Unpackerr monitors `/mnt/storage2/downloads/slskd`** | Archives auto-extracted | Good - beets can import extracted files |

---

### 8. Recommended Beets Integration Points

1. **Music Library Path:** `/mnt/storage/media/music` (read-write for beets, read-only for Navidrome/slskd)

2. **Import Sources:**
   - `/mnt/storage2/downloads/slskd` - Soulseek downloads
   - `/mnt/storage2/downloads/torrents` - Torrent downloads
   - `/mnt/storage2/downloads/aria2` - Direct downloads

3. **Network Considerations:**
   - Beets should run on regular `potatostack` network (NOT behind gluetun)
   - Can access slskd via `host.docker.internal:2234` if API integration needed
   - Can access Navidrome for Subsonic API integration

4. **Database Options:**
   - Use existing PostgreSQL (`postgres:5432`)
   - Or embedded SQLite in beets config volume

5. **Homer Dashboard Entry Needed:**
   ```yaml
   - name: "Beets"
     logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/beets.png"
     subtitle: "Music Library Manager"
     url: "https://potatostack.tale-iwato.ts.net:<PORT>"
   ```

---

### 9. Disabled Music Services (Can Reference for Config)

The `lidarr` service is disabled but shows the intended music organization pattern:
```yaml
# lidarr (DISABLED):
#   volumes:
#     - /mnt/storage/media/music:/music
#     - /mnt/storage2/downloads:/downloads
```

---

This gives you a complete picture of the current music infrastructure. The main integration path would be:
1. Beets imports from `/mnt/storage2/downloads/slskd` 
2. Beets organizes into `/mnt/storage/media/music`
3. Navidrome scans `/mnt/storage/media/music` (already configured)
4. slskd shares `/mnt/storage/media/music` (already configured)
</task_result>
```

---

## User

Explore
    linuxserver

    beets

linuxserver/beets
Sponsored OSS

By linuxserver.io
•Updated about 4 hours ago

A Beets container, brought to you by LinuxServer.io.
Image
160

50M+
linuxserver/beets repository overview

linuxserver.io

Blog Discord Discourse GitHub Open Collective

The LinuxServer.io team brings you another container release featuring:

    regular and timely application updates
    easy user mappings (PGID, PUID)
    custom base image with s6 overlay
    weekly base OS updates with common layers across the entire LinuxServer.io ecosystem to minimise space usage, down time and bandwidth
    regular security updates

Find us at:

    Blog - all the things you can do with our containers including How-To guides, opinions and much more!
    Discord - realtime support / chat with the community and the team.
    Discourse - post on our community forum.
    GitHub - view the source for all of our repositories.
    Open Collective - please consider helping us by either donating or contributing to our budget

linuxserver/beets

GitHub Stars GitHub Release GitHub Package Repository GitLab Container Registry Quay.io Docker Pulls Docker Stars Jenkins Build LSIO CI

Beets is a music library manager and not, for the most part, a music player. It does include a simple player plugin and an experimental Web-based player, but it generally leaves actual sound-reproduction to specialized tools.

beets
Supported Architectures

We utilise the docker manifest for multi-platform awareness. More information is available from docker here and our announcement here.

Simply pulling lscr.io/linuxserver/beets:latest should retrieve the correct image for your arch, but you can also pull specific arch images via tags.

The architectures supported by this image are:
Architecture	Available	Tag
x86-64	✅	amd64-<version tag>
arm64	✅	arm64v8-<version tag>
Version Tags

This image provides various versions that are available via tags. Please read the descriptions carefully and exercise caution when using unstable or development tags.
Tag	Available	Description
latest	✅	Stable Beets Releases
nightly	✅	Built against head of Beets git, generally considered unstable but a likely choice for power users of the application.
Application Setup

Edit the config file in /config

To edit the config from within the container use beet config -e

For a command prompt as user abc docker exec -it -u abc beets bash

See Beets for more info.

Contains beets-extrafiles plugin, configuration details
Read-Only Operation

This image can be run with a read-only container filesystem. For details please read the docs.
Non-Root Operation

This image can be run with a non-root user. For details please read the docs.
Usage

To help you get started creating a container from this image you can either use docker-compose or the docker cli.

    [!NOTE] Unless a parameter is flagged as 'optional', it is mandatory and a value must be provided.

docker-compose (recommended, click here for more info)

---
services:
  beets:
    image: lscr.io/linuxserver/beets:latest
    container_name: beets
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - /path/to/beets/config:/config
      - /path/to/music/library:/music
      - /path/to/ingest:/downloads
    ports:
      - 8337:8337
    restart: unless-stopped

docker cli (click here for more info)

docker run -d \
  --name=beets \
  -e PUID=1000 \
  -e PGID=1000 \
  -e TZ=Etc/UTC \
  -p 8337:8337 \
  -v /path/to/beets/config:/config \
  -v /path/to/music/library:/music \
  -v /path/to/ingest:/downloads \
  --restart unless-stopped \
  lscr.io/linuxserver/beets:latest

Parameters

Containers are configured using parameters passed at runtime (such as those above). These parameters are separated by a colon and indicate <external>:<internal> respectively. For example, -p 8080:80 would expose port 80 from inside the container to be accessible from the host's IP on port 8080 outside the container.
Parameter	Function
-p 8337:8337	Application WebUI
-e PUID=1000	for UserID - see below for explanation
-e PGID=1000	for GroupID - see below for explanation
-e TZ=Etc/UTC	specify a timezone to use, see this list.
-v /config	Persistent config files
-v /music	Music library
-v /downloads	Non processed music
--read-only=true	Run container with a read-only filesystem. Please read the docs.
--user=1000:1000	Run container with a non-root user. Please read the docs.
Environment variables from files (Docker secrets)

You can set any environment variable from a file by using a special prepend FILE__.

As an example:

-e FILE__MYVAR=/run/secrets/mysecretvariable

Will set the environment variable MYVAR based on the contents of the /run/secrets/mysecretvariable file.
Umask for running applications

For all of our images we provide the ability to override the default umask settings for services started within the containers using the optional -e UMASK=022 setting. Keep in mind umask is not chmod it subtracts from permissions based on it's value it does not add. Please read up here before asking for support.
User / Group Identifiers

When using volumes (-v flags), permissions issues can arise between the host OS and the container, we avoid this issue by allowing you to specify the user PUID and group PGID.

Ensure any volume directories on the host are owned by the same user you specify and any permissions issues will vanish like magic.

In this instance PUID=1000 and PGID=1000, to find yours use id your_user as below:

id your_user

Example output:

uid=1000(your_user) gid=1000(your_user) groups=1000(your_user)

Docker Mods

Docker Mods Docker Universal Mods

We publish various Docker Mods to enable additional functionality within the containers. The list of Mods available for this image (if any) as well as universal mods that can be applied to any one of our images can be accessed via the dynamic badges above.
Support Info

    Shell access whilst the container is running:

    docker exec -it beets /bin/bash

    To monitor the logs of the container in realtime:

    docker logs -f beets

    Container version number:

    docker inspect -f '{{ index .Config.Labels "build_version" }}' beets

    Image version number:

    docker inspect -f '{{ index .Config.Labels "build_version" }}' lscr.io/linuxserver/beets:latest

Updating Info

Most of our images are static, versioned, and require an image update and container recreation to update the app inside. With some exceptions (noted in the relevant readme.md), we do not recommend or support updating apps inside the container. Please consult the Application Setup section above to see if it is recommended for the image.

Below are the instructions for updating containers:
Via Docker Compose

    Update images:

        All images:

        docker-compose pull

        Single image:

        docker-compose pull beets

    Update containers:

        All containers:

        docker-compose up -d

        Single container:

        docker-compose up -d beets

    You can also remove the old dangling images:

    docker image prune

Via Docker Run

    Update the image:

    docker pull lscr.io/linuxserver/beets:latest

    Stop the running container:

    docker stop beets

    Delete the container:

    docker rm beets

    Recreate a new container with the same docker run parameters as instructed above (if mapped correctly to a host folder, your /config folder and settings will be preserved)

    You can also remove the old dangling images:

    docker image prune

Image Update Notifications - Diun (Docker Image Update Notifier)

    [!TIP] We recommend Diun for update notifications. Other tools that automatically update containers unattended are not recommended or supported.

Building locally

If you want to make local modifications to these images for development purposes or just to customize the logic:

git clone https://github.com/linuxserver/docker-beets.git
cd docker-beets
docker build \
  --no-cache \
  --pull \
  -t lscr.io/linuxserver/beets:latest .

The ARM variants can be built on x86_64 hardware and vice versa using lscr.io/linuxserver/qemu-static

docker run --rm --privileged lscr.io/linuxserver/qemu-static --reset

Once registered you can define the dockerfile to use with -f Dockerfile.aarch64.
Versions

    01.02.26: - Rebase to Alpine 3.23.
    27.01.25: - Rebase to Alpine 3.21.
    01.10.24: - Add packages required for Discogs plugin.
    28.08.24: - Rebase to Alpine 3.20, switch from Pillow to Imagemagick.
    23.12.23: - Rebase to Alpine 3.19.
    25.08.23: - Rebase to Alpine 3.18, pin Pillow to 9.5.0.
    07.07.23: - Deprecate armhf. As announced here
    25.03.23: - Add requests_oauthlib required for beatport plugin.
    24.02.23: - Rebase to Alpine 3.17, migrate to s6v3, install chromaprint from Alpine repos, add beetcamp plugin.
    15.01.22: - Rebasing to alpine 3.15.
    19.12.19: - Rebasing to alpine 3.11.
    28.06.19: - Rebasing to alpine 3.10.
    12.05.19: - Add flac and mp3val binaries required for badfiles plugin.
    12.04.19: - Rebase to Alpine 3.9.
    23.03.19: - Switching to new Base images, shift to arm32v7 tag.
    11.03.19: - Swap copyartifacts for extrafiles, update endpoints with nightly tag.
    01.03.19: - Switch to python3.
    07.02.19: - Add fftw-dev build dependency for chromaprint.
    28.01.19: - Add pipeline logic and multi arch.
    15.08.18: - Rebase to alpine 3.8, use alpine repo version of pylast.
    12.08.18: - Add requests pip package.
    04.03.18: - Upgrade mp3gain to 1.6.1.
    02.01.18: - Deprecate cpu_core routine lack of scaling.
    27.12.17: - Add beautifulsoup4 pip package.
    06.12.17: - Rebase to alpine linux 3.7.
    25.05.17: - Rebase to alpine linux 3.6.
    06.02.17: - Rebase to alpine linux 3.5.
    16.01.17: - Add packages required for replaygain.
    24.12.16: - Add beets-copyartifacts plugin.
    07.12.16: - Edit cmake options for chromaprint, should now build and install fpcalc, add gstreamer lib
    14.10.16: - Add version layer information.
    01.10.16: - Add nano and editor variable to allow editing of the config from the container command line.
    30.09.16: - Fix umask.
    24.09.16: - Rebase to alpine linux.
    10.09.16: - Add layer badges to README.
    05.01.16: - Change ffpmeg repository, other version crashes container
    06.11.15: - Initial Release
    29.11.15: - Take out term setting, causing issues with key entry for some users

Tag summary
Recent tags

Content type

Image

Digest

sha256:7420760fa…

Size

222.4 MB

Last updated

about 4 hours ago

docker pull linuxserver/beets:nightly-version-07d3e05a

This week's pulls

Pulls:

8,643

Last week
Learn more
Why
Overview
What is a Container
Products
Product Overview
Product Offerings
Docker Desktop
Docker Hub
Features
Container Runtime
Developer Tools
Docker App
Kubernetes
Developers
Getting Started
Play with Docker
Community
Open Source
Documentation
Company
About Us
Resources
Blog
Customers
Partners
Newsroom
Events and Webinars
Careers
Contact Us
System Status

© 2026 Docker, Inc. All rights reserved. |
Terms of Service
|
Subscription Service Agreement
|
Privacy
|
Legal 

---

## Assistant (Plan · glm-5-free · 15.1s)

_Thinking:_

The user wants me to plan the integration of beets into their PotatoStack. They've shared the linuxserver/beets Docker Hub documentation. I need to create a comprehensive plan for:

1. Adding beets service to docker-compose.yml
2. Configuring beets properly for their music library organization
3. Integrating with Navidrome (existing)
4. Integrating with slskd (existing)
5. Adding to Homer dashboard
6. Adding to Tailscale serve configuration
7. Creating proper beets config.yaml

Let me read some key files to understand the current setup better before creating the plan.

Let me read the key configuration files to create a detailed integration plan.

**Tool: read**

**Input:**
```json
{
  "filePath": "/home/daniel/potatostack/docker-compose.yml"
}
```

**Output:**
```
<path>/home/daniel/potatostack/docker-compose.yml</path>
<type>file</type>
<content>1: ################################################################################
2: # PotatoStack Main - Optimized for Mini PC (16GB RAM)
3: # Full-featured self-hosted stack with monitoring, automation, and media
4: # Target: Mini PC with 16GB RAM, 4+ core CPU, 1GB ethernet
5: ################################################################################
6: 
7: x-logging: &default-logging
8:   driver: "json-file"
9:   options:
10:     max-size: "10m"
11:     max-file: "3"
12:     compress: "true"
13: 
14: x-common-env: &common-env
15:   TZ: Europe/Berlin
16:   PUID: 1000
17:   PGID: 1000
18: 
19: services:
20:   ################################################################################
21:   # Storage Init - Creates required directories on startup
22:   ################################################################################
23:   storage-init:
24:     image: alpine:${ALPINE_TAG:-latest}
25:     container_name: storage-init
26:     command: sh /init-storage.sh
27:     privileged: true
28:     environment:
29:       <<: *common-env
30:       SLSKD_API_KEY: ${SLSKD_API_KEY:-}
31:       SYNCTHING_API_KEY: ${SYNCTHING_API_KEY:-}
32:       ARIA2_RPC_SECRET: ${ARIA2_RPC_SECRET:-}
33:       SNAPSHOT_CRON_SCHEDULE: ${SNAPSHOT_CRON_SCHEDULE:-0 3 * * *}
34:       SNAPSHOT_PATHS: ${SNAPSHOT_PATHS:-/data}
35:       SNAPSHOT_LOG_FILE: ${SNAPSHOT_LOG_FILE:-/mnt/storage/stack-snapshot.log}
36:     volumes:
37:       - /mnt/storage:/mnt/storage
38:       - /mnt/storage2:/mnt/storage2
39:       - /mnt/ssd/docker-data:/mnt/ssd/docker-data
40:       - ./scripts/init/init-storage.sh:/init-storage.sh:ro
41:       - ./scripts/backup/stack-snapshot.sh:/stack-snapshot.sh:ro
42:       - shared-keys:/keys
43:     network_mode: none
44:     restart: "no"
45:     deploy:
46:       resources:
47:         limits:
48:           memory: 256M
49: 
50:   ################################################################################
51:   # CORE DATABASES
52:   ################################################################################
53: 
54:   # PostgreSQL - Primary database with pgvector for embeddings (SOTA 2025)
55:   postgres:
56:     image: pgvector/pgvector:${POSTGRES_TAG:-pg16}
57:     container_name: postgres
58:     logging: *default-logging
59:     shm_size: 1gb
60:     environment:
61:       <<: *common-env
62:       POSTGRES_USER: postgres
63:       POSTGRES_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
64:       POSTGRES_MULTIPLE_DATABASES: ${POSTGRES_DATABASES:-gitea,immich,healthchecks,atuin,miniflux,grafana,freqtrade,ghostfolio,baikal}
65:     entrypoint: sh /postgres-wrapper.sh
66:     command:
67:       - "postgres"
68:       - "-c"
69:       - "shared_buffers=512MB"
70:       - "-c"
71:       - "effective_cache_size=768MB"
72:       - "-c"
73:       - "work_mem=16MB"
74:       - "-c"
75:       - "maintenance_work_mem=512MB"
76:       - "-c"
77:       - "max_connections=200"
78:       - "-c"
79:       - "checkpoint_completion_target=0.9"
80:       - "-c"
81:       - "wal_buffers=16MB"
82:       - "-c"
83:       - "default_statistics_target=100"
84:       - "-c"
85:       - "random_page_cost=1.1"
86:       - "-c"
87:       - "effective_io_concurrency=200"
88:       - "-c"
89:       - "max_worker_processes=8"
90:       - "-c"
91:       - "max_parallel_workers=4"
92:       - "-c"
93:       - "max_parallel_workers_per_gather=2"
94:     ports:
95:       - "${HOST_BIND:-127.0.0.1}:5432:5432"
96:     volumes:
97:       - /mnt/ssd/docker-data/postgres:/var/lib/postgresql/data
98:       - ./scripts/init/init-postgres-multiple-dbs.sh:/docker-entrypoint-initdb.d/init-postgres-multiple-dbs.sh:ro
99:       - ./scripts/init/postgres-wrapper.sh:/postgres-wrapper.sh:ro
100:     tmpfs:
101:       - /tmp:exec,mode=1777
102:       - /var/run/postgresql:exec,mode=1777
103:     networks:
104:       - potatostack
105:     restart: unless-stopped
106:     healthcheck:
107:       test: ["CMD-SHELL", "pg_isready -U postgres"]
108:       interval: 30s
109:       timeout: 10s
110:       retries: 5
111:     deploy:
112:       resources:
113:         limits:
114:           cpus: "1.5"
115:           memory: 1536M
116:         reservations:
117:           cpus: "0.5"
118:           memory: 1024M
119:     labels:
120:       - "potatostack.alerts=critical"
121:       - "wud.trigger.docker.update=false"
122: 
123:   # PgBouncer - PostgreSQL connection pooling (SOTA 2025)
124:   pgbouncer:
125:     image: edoburu/pgbouncer:${PGBOUNCER_TAG:-latest}
126:     container_name: pgbouncer
127:     logging: *default-logging
128:     environment:
129:       DATABASE_URL: postgres://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/postgres
130:       POOL_MODE: transaction
131:       DEFAULT_POOL_SIZE: 50
132:       MAX_CLIENT_CONN: 200
133:       MAX_DB_CONNECTIONS: 100
134:     networks:
135:       - potatostack
136:     depends_on:
137:       - postgres
138:     restart: unless-stopped
139:     healthcheck:
140:       test: ["CMD", "pg_isready", "-h", "127.0.0.1", "-p", "5432"]
141:       interval: 30s
142:       timeout: 10s
143:       retries: 5
144:     deploy:
145:       resources:
146:         limits:
147:           cpus: "0.25"
148:           memory: 128M
149:         reservations:
150:           cpus: "0.1"
151:           memory: 64M
152:     labels:
153:       - "potatostack.alerts=critical"
154:       - "wud.trigger.docker.update=true"
155: 
156:   # MongoDB - Document database (SOTA 2025)
157:   mongo:
158:     image: mongo:${MONGO_TAG:-7-jammy}
159:     container_name: mongo
160:     logging: *default-logging
161:     command:
162:       - "mongod"
163:       - "--wiredTigerCacheSizeGB=0.5"
164:       - "--wiredTigerJournalCompressor=snappy"
165:       - "--bind_ip_all"
166:     environment:
167:       <<: *common-env
168:       MONGO_INITDB_ROOT_USERNAME: root
169:       MONGO_INITDB_ROOT_PASSWORD: ${MONGO_ROOT_PASSWORD}
170:     volumes:
171:       - /mnt/ssd/docker-data/mongo:/data/db
172:       - /mnt/ssd/docker-data/mongo-config:/data/configdb
173:     tmpfs:
174:       - /tmp:size=64M # Prevents stale socket file on restart
175:     networks:
176:       - potatostack
177:     restart: unless-stopped
178:     healthcheck:
179:       test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
180:       interval: 30s
181:       timeout: 10s
182:       retries: 5
183:     deploy:
184:       resources:
185:         limits:
186:           cpus: "2.0"
187:           memory: 512M
188:         reservations:
189:           cpus: "0.5"
190:           memory: 512M
191:     labels:
192:       - "potatostack.alerts=critical"
193:       - "wud.trigger.docker.update=false"
194: 
195:   # Redis Shared Cache - SOTA 2025 consolidated (N8n, Gitea, Immich, Paperless, Sentry)
196:   redis-cache:
197:     image: redis:${REDIS_TAG:-7-alpine}
198:     container_name: redis-cache
199:     logging: *default-logging
200:     command: redis-server --maxmemory 384mb --maxmemory-policy allkeys-lfu --databases 16 --activedefrag yes --lazyfree-lazy-eviction yes --lazyfree-lazy-expire yes --lazyfree-lazy-server-del yes --save 60 1000 --appendonly yes --appendfsync everysec
201:     volumes:
202:       - /mnt/ssd/docker-data/redis-cache:/data
203:     tmpfs:
204:       - /tmp:exec,mode=1777
205:     networks:
206:       - potatostack
207:     restart: unless-stopped
208:     healthcheck:
209:       test: ["CMD", "redis-cli", "ping"]
210:       interval: 30s
211:       timeout: 10s
212:       retries: 5
213:     deploy:
214:       resources:
215:         limits:
216:           cpus: "0.5"
217:           memory: 256M
218:         reservations:
219:           cpus: "0.25"
220:           memory: 128M
221:     labels:
222:       - "potatostack.alerts=critical"
223:       - "wud.trigger.docker.update=false"
224: 
225:   ################################################################################
226:   # SECURITY & INTRUSION PREVENTION
227:   ################################################################################
228: 
229:   # CrowdSec - Modern IPS/IDS with community threat intelligence (SOTA 2025)
230:   crowdsec:
231:     image: crowdsecurity/crowdsec:${CROWDSEC_TAG:-latest}
232:     container_name: crowdsec
233:     logging: *default-logging
234:     ports:
235:       - "${HOST_BIND:-127.0.0.1}:6060:6060" # Prometheus metrics
236:     environment:
237:       <<: *common-env
238:       COLLECTIONS: "crowdsecurity/http-cve crowdsecurity/whitelist-good-actors crowdsecurity/nginx crowdsecurity/linux"
239:       GID: "1000"
240:       METRICS_ENABLED: "true"
241:     volumes:
242:       - /mnt/ssd/docker-data/crowdsec-db:/var/lib/crowdsec/data/
243:       - /mnt/ssd/docker-data/crowdsec-config:/etc/crowdsec/
244:       - /var/log:/var/log:ro
245:     networks:
246:       - potatostack
247:     restart: unless-stopped
248:     healthcheck:
249:       test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:8080/health || exit 1"]
250:       interval: 60s
251:       timeout: 10s
252:       retries: 3
253:     deploy:
254:       resources:
255:         limits:
256:           cpus: "0.25"
257:           memory: 128M
258:         reservations:
259:           cpus: "0.1"
260:           memory: 128M
261:     labels:
262:       - "wud.trigger.docker.update=true"
263: 
264:   ################################################################################
265:   # AUTHENTICATION & SECURITY
266:   ################################################################################
267: 
268:   # Authentik - SSO and 2FA provider (DISABLED)
269:   # authentik-server:
270:   #   image: ghcr.io/goauthentik/server:${AUTHENTIK_TAG:-latest}
271:   #   container_name: authentik-server
272:   #   logging: *default-logging
273:   #   command: server
274:   #   ports:
275:   #     - "${HOST_BIND:-127.0.0.1}:9000:9000"
276:   #     - "${HOST_BIND:-127.0.0.1}:9443:9443"
277:   #   environment:
278:   #     <<: *common-env
279:   #     AUTHENTIK_POSTGRESQL__HOST: postgres
280:   #     AUTHENTIK_POSTGRESQL__USER: postgres
281:   #     AUTHENTIK_POSTGRESQL__NAME: authentik
282:   #     AUTHENTIK_POSTGRESQL__PASSWORD: ${POSTGRES_SUPER_PASSWORD}
283:   #     AUTHENTIK_REDIS__HOST: redis-cache
284:   #     AUTHENTIK_SECRET_KEY: ${AUTHENTIK_SECRET_KEY}
285:   #     AUTHENTIK_ERROR_REPORTING__ENABLED: "false"
286:   #   volumes:
287:   #     - authentik-media:/media
288:   #     - authentik-custom-templates:/templates
289:   #   networks:
290:   #     - potatostack
291:   #   depends_on:
292:   #     - postgres
293:   #     - redis-cache
294:   #   restart: unless-stopped
295:   #   healthcheck:
296:   #     test:
297:   #       [
298:   #         "CMD",
299:   #         "python3",
300:   #         "-c",
301:   #         "import requests; requests.get('http://127.0.0.1:9000/-/health/live/')",
302:   #       ]
303:   #     interval: 30s
304:   #     timeout: 10s
305:   #     retries: 3
306:   #     start_period: 60s
307:   #   deploy:
308:   #     resources:
309:   #       limits:
310:   #         cpus: "2.0"
311:   #         memory: 512M
312:   #   labels:
313: 
314:   # authentik-worker:
315:   #   image: ghcr.io/goauthentik/server:${AUTHENTIK_TAG:-latest}
316:   #   container_name: authentik-worker
317:   #   logging: *default-logging
318:   #   command: worker
319:   #   environment:
320:   #     <<: *common-env
321:   #     AUTHENTIK_POSTGRESQL__HOST: postgres
322:   #     AUTHENTIK_POSTGRESQL__USER: postgres
323:   #     AUTHENTIK_POSTGRESQL__NAME: authentik
324:   #     AUTHENTIK_POSTGRESQL__PASSWORD: ${POSTGRES_SUPER_PASSWORD}
325:   #     AUTHENTIK_REDIS__HOST: redis-cache
326:   #     AUTHENTIK_SECRET_KEY: ${AUTHENTIK_SECRET_KEY}
327:   #   volumes:
328:   #     - authentik-media:/media
329:   #     - authentik-certs:/certs
330:   #   networks:
331:   #     - potatostack
332:   #   depends_on:
333:   #     - postgres
334:   #     - redis-cache
335:   #   restart: unless-stopped
336:   #   deploy:
337:   #     resources:
338:   #       limits:
339:   #         cpus: "1.0"
340:   #         memory: 512M
341: 
342:   # Vaultwarden - Password manager and 2FA aggregator
343:   vaultwarden:
344:     image: vaultwarden/server:${VAULTWARDEN_TAG:-latest}
345:     container_name: vaultwarden
346:     logging: *default-logging
347:     ports:
348:       - "${HOST_BIND:-127.0.0.1}:8888:80"
349:       - "${HOST_BIND:-127.0.0.1}:8090:80"
350:       - "${HOST_BIND:-127.0.0.1}:3012:3012"
351:     environment:
352:       <<: *common-env
353:       DOMAIN: https://vault.${HOST_DOMAIN:-local.domain}
354:       ROCKET_PORT: 80
355:       WEBSOCKET_ENABLED: "true"
356:       WEBSOCKET_PORT: 3012
357:       SIGNUPS_ALLOWED: ${VAULTWARDEN_SIGNUPS_ALLOWED:-false}
358:       INVITATIONS_ALLOWED: ${VAULTWARDEN_INVITATIONS_ALLOWED:-true}
359:       ADMIN_TOKEN: ${VAULTWARDEN_ADMIN_TOKEN}
360:       DATABASE_URL: /data/db.sqlite3
361:       ICON_CACHE_TTL: 2592000
362:       LOG_LEVEL: warn
363:     volumes:
364:       - /mnt/ssd/docker-data/vaultwarden:/data
365:     networks:
366:       - potatostack
367:     labels:
368:       - "wud.trigger.docker.update=true"
369:     restart: unless-stopped
370:     healthcheck:
371:       test: ["CMD-SHELL", "curl -sf http://127.0.0.1:80/alive || exit 1"]
372:       interval: 60s
373:       timeout: 10s
374:       retries: 5
375:     deploy:
376:       resources:
377:         limits:
378:           cpus: "0.25"
379:           memory: 128M
380: 
381:   ################################################################################
382:   # DNS & AD BLOCKING
383:   ################################################################################
384: 
385:   # AdGuard Home - DNS-level ad blocking with encrypted DNS (SOTA 2025)
386:   # DISABLED: Commented out per user request
387:   # adguardhome:
388:   #   image: adguard/adguardhome:${ADGUARD_TAG:-latest}
389:   #   container_name: adguardhome
390:   #   logging: *default-logging
391:   #   ports:
392:   #     - "53:53/tcp"
393:   #     - "53:53/udp"
394:   #     - "${HOST_BIND:-127.0.0.1}:3053:3000"
395:   #     - "${HOST_BIND:-127.0.0.1}:8053:80"
396:   #   environment:
397:   #     <<: *common-env
398:   #   volumes:
399:   #     - adguard-work:/opt/adguardhome/work
400:   #     - adguard-conf:/opt/adguardhome/conf
401:   #   networks:
402:   #     - potatostack
403:   #   restart: unless-stopped
404:   #   healthcheck:
405:   #     test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:80/ || exit 1"]
406:   #     interval: 60s
407:   #     timeout: 10s
408:   #     retries: 3
409:   #   deploy:
410:   #     resources:
411:   #       limits:
412:   #         cpus: "0.25"
413:   #         memory: 128M
414:   #       reservations:
415:   #         cpus: "0.1"
416:   #         memory: 128M
417:   #   labels:
418: 
419:   ################################################################################
420:   # VPN & NETWORKING
421:   ################################################################################
422: 
423:   # Gluetun - VPN client with killswitch
424:   gluetun:
425:     image: qmcgaw/gluetun:${GLUETUN_TAG:-latest}
426:     container_name: gluetun
427:     logging: *default-logging
428:     cap_add:
429:       - NET_ADMIN
430:     devices:
431:       - /dev/net/tun:/dev/net/tun
432:     extra_hosts:
433:       - "host.docker.internal:host-gateway"
434:     ports:
435:       - "${HOST_BIND:-127.0.0.1}:8008:8008" # Gluetun control (moved from 8000)
436:       # - "${HOST_BIND:-127.0.0.1}:8787:8787" # Bookshelf (ebook manager) - DISABLED
437:       - "${HOST_BIND:-127.0.0.1}:8282:8282" # qBittorrent WebUI
438:       - "${HOST_BIND:-127.0.0.1}:51413:51413" # qBittorrent peer
439:       - "${HOST_BIND:-127.0.0.1}:51413:51413/udp"
440:       - "${HOST_BIND:-127.0.0.1}:6800:6800" # aria2 RPC
441:       - "${HOST_BIND:-127.0.0.1}:6888:6888" # aria2 BT peer
442:       - "${HOST_BIND:-127.0.0.1}:6888:6888/udp"
443:       - "${HOST_BIND:-127.0.0.1}:2234:2234" # slskd WebUI
444:       - "${HOST_BIND:-127.0.0.1}:50000:50000" # slskd peer
445:       - "${HOST_BIND:-127.0.0.1}:8097:8080" # SpotiFLAC
446:       # - "${HOST_BIND:-127.0.0.1}:8945:8945" # Pinchflat
447:       - "${HOST_BIND:-127.0.0.1}:8076:8000" # pyLoad WebUI (internal 8000)
448:       - "${HOST_BIND:-127.0.0.1}:9666:9666" # pyLoad Click'n'Load
449:       - "${HOST_BIND:-127.0.0.1}:9900:9900" # Stash
450: 
451:     environment:
452:       <<: *common-env
453:       VPN_SERVICE_PROVIDER: ${VPN_PROVIDER:-surfshark}
454:       VPN_TYPE: ${VPN_TYPE:-wireguard}
455:       WIREGUARD_PRIVATE_KEY: ${WIREGUARD_PRIVATE_KEY}
456:       WIREGUARD_ADDRESSES: ${WIREGUARD_ADDRESSES}
457:       SERVER_COUNTRIES: ${VPN_COUNTRY:-Germany}
458:       LOG_LEVEL: ${VPN_LOG_LEVEL:-info}
459:       FIREWALL_OUTBOUND_SUBNETS: ${LAN_NETWORK:-192.168.178.0/24},172.16.0.0/12
460:       FIREWALL_VPN_INPUT_PORTS: ${VPN_INPUT_PORTS:-51413,50000,6888,3333,3334}
461:       FIREWALL: "on"
462:       DNS_ADDRESS: ${VPN_DNS:-1.1.1.1}
463:       DOT: "off"
464:       HTTPPROXY: "off"
465:       SHADOWSOCKS: "off"
466:       HTTP_CONTROL_SERVER_ADDRESS: :8008 # Changed from 8000 to avoid pyload conflict
467:       HTTP_CONTROL_SERVER_LOG: "off"
468:       HTTP_CONTROL_SERVER_AUTH: "off"
469:       IPV6: "off"
470:       UPDATER_PERIOD: 24h
471:       HEALTH_VPN_DURATION_INITIAL: 60s
472:       HEALTH_VPN_DURATION_ADDITION: 10s
473:     volumes:
474:       - gluetun-config:/gluetun
475:       - ./config/gluetun/post-rules.txt:/iptables/post-rules.txt:ro
476:     networks:
477:       - potatostack
478:     restart: unless-stopped
479:     healthcheck:
480:       test: ["CMD", "/gluetun-entrypoint", "healthcheck"]
481:       interval: 120s
482:       timeout: 10s
483:       retries: 3
484:       start_period: 120s
485:     deploy:
486:       resources:
487:         limits:
488:           cpus: "0.5"
489:           memory: 256M
490:         reservations:
491:           cpus: "0.1"
492:           memory: 128M
493:     labels:
494:       - "autoheal=true"
495:       - "potatostack.alerts=critical"
496:       # aria2 has no web UI - RPC only on port 6800
497:       - "wud.trigger.docker.update=false"
498: 
499:   # Tailscale - Mesh VPN for remote access (SOTA 2025 - easiest option)
500:   tailscale:
501:     image: tailscale/tailscale:${TAILSCALE_TAG:-latest}
502:     container_name: tailscale
503:     logging: *default-logging
504:     hostname: potatostack
505:     environment:
506:       <<: *common-env
507:       TS_AUTHKEY: ${TAILSCALE_AUTHKEY:-}
508:       TS_STATE_DIR: /var/lib/tailscale
509:       TS_USERSPACE: "false"
510:     volumes:
511:       - tailscale-data:/var/lib/tailscale
512:       - /dev/net/tun:/dev/net/tun
513:       - tailscale-https-marker:/https-marker
514:     cap_add:
515:       - NET_ADMIN
516:       - SYS_MODULE
517:     network_mode: host
518:     labels:
519:       - "wud.trigger.docker.update=false"
520:     restart: unless-stopped
521:     healthcheck:
522:       test: ["CMD-SHELL", 'tailscale status --json 2>/dev/null | grep -q ''"Self"'' || exit 1']
523:       interval: 30s
524:       timeout: 10s
525:       retries: 5
526:       start_period: 60s
527:     deploy:
528:       resources:
529:         limits:
530:           cpus: "0.5"
531:           memory: 384M
532:         reservations:
533:           cpus: "0.25"
534:           memory: 256M
535: 
536:   # Tailscale HTTPS Setup - Wraps HTTP ports with Tailscale TLS certificates
537:   tailscale-https-setup:
538:     image: alpine:${ALPINE_TAG:-latest}
539:     container_name: tailscale-https-setup
540:     logging: *default-logging
541:     entrypoint: ["/bin/sh", "/init.sh"]
542:     environment:
543:       <<: *common-env
544:       TAILSCALE_CONTAINER: tailscale
545:       # Homer (7575), WUD (3000), home-assistant (8123), flaresolverr (8191) added; removed: karakeep (9091), homarr
546:       TAILSCALE_SERVE_PORTS: "7575,8088,3000,3001,3002,8086,8096,8989,7878,8686,9696,6767,13378,4533,9900,8282,8076,2234,8097,8008,2283,8090,8095,8384,3004,3006,8888,8093,5006,9090,3100,9093,10903,10902,8087,6060,8001,8788,8060,8889,8081,3010,3013,8180,8181,9925,5984,9898,8123,8191,3333,3335,5656,8085"
547:       # Special port mappings (external:internal)
548:       TAILSCALE_SERVE_SPECIAL: ""
549:       TAILSCALE_MARKER_FILE: /https-marker/setup-complete
550:     volumes:
551:       - ./scripts/init/tailscale-serve-https.sh:/init.sh:ro
552:       - /var/run/docker.sock:/var/run/docker.sock:ro
553:       - tailscale-https-marker:/https-marker
554:     network_mode: host
555:     depends_on:
556:       tailscale:
557:         condition: service_healthy
558:     restart: "no"
559:     deploy:
560:       resources:
561:         limits:
562:           cpus: "0.1"
563:           memory: 64M
564: 
565:   # Tailscale HTTPS Monitor - Periodically re-applies HTTPS serve rules
566:   tailscale-https-monitor:
567:     image: alpine:${ALPINE_TAG:-latest}
568:     container_name: tailscale-https-monitor
569:     logging: *default-logging
570:     entrypoint: ["/bin/sh", "/init.sh"]
571:     environment:
572:       <<: *common-env
573:       TAILSCALE_CONTAINER: tailscale
574:       # Homer (7575), WUD (3000), home-assistant (8123), flaresolverr (8191) added; removed: karakeep (9091), homarr
575:       TAILSCALE_SERVE_PORTS: 7575,8088,3000,3001,3002,8086,8096,8989,7878,8686,9696,6767,13378,4533,9900,8282,8076,2234,8097,8008,2283,8090,8095,8384,3004,3006,8888,8093,5006,9090,3100,9093,10903,10902,8087,6060,8001,8788,8060,8889,8081,3010,3013,8180,8181,9925,5984,9898,8123,8191,3333,3335,5656,8085
576:       # Special port mappings (external:internal)
577:       TAILSCALE_SERVE_SPECIAL: ""
578:       TAILSCALE_SERVE_LOOP: "true"
579:       TAILSCALE_SERVE_INTERVAL: "300"
580:       TAILSCALE_MARKER_FILE: /https-marker/monitor-alive
581:     volumes:
582:       - ./scripts/init/tailscale-serve-https.sh:/init.sh:ro
583:       - /var/run/docker.sock:/var/run/docker.sock:ro
584:       - tailscale-https-marker:/https-marker
585:     network_mode: host
586:     depends_on:
587:       tailscale:
588:         condition: service_healthy
589:     restart: unless-stopped
590:     deploy:
591:       resources:
592:         limits:
593:           cpus: "0.1"
594:           memory: 64M
595: 
596:   ################################################################################
597:   # CLOUD STORAGE & FILE SYNC
598:   ################################################################################
599: 
600:   # Syncthing - P2P file sync
601:   syncthing:
602:     image: lscr.io/linuxserver/syncthing:${SYNCTHING_TAG:-latest}
603:     container_name: syncthing
604:     logging: *default-logging
605:     entrypoint: bash /syncthing-init.sh
606:     ports:
607:       - "${HOST_BIND:-127.0.0.1}:8384:8384"
608:       - "${HOST_BIND:-127.0.0.1}:22000:22000/tcp"
609:       - "${HOST_BIND:-127.0.0.1}:22000:22000/udp"
610:       - "${HOST_BIND:-127.0.0.1}:21027:21027/udp"
611:     environment:
612:       <<: *common-env
613:       # Disable NAT traversal to prevent NAT-PMP errors in Docker
614:       STNATPMPENABLED: "false"
615:       STGLOBALFOLDERS: "false"
616:     volumes:
617:       - syncthing-config:/config
618:       - /mnt/storage/syncthing:/data
619:       - /mnt/storage/cache/syncthing-versions:/data/.stversions
620:       - shared-keys:/keys:ro
621:       - ./scripts/init/syncthing-init.sh:/syncthing-init.sh:ro
622:     networks:
623:       - potatostack
624:     depends_on:
625:       - storage-init
626:     restart: unless-stopped
627:     healthcheck:
628:       test: ["CMD", "curl", "-f", "http://127.0.0.1:8384/rest/noauth/health"]
629:       interval: 120s
630:     deploy:
631:       resources:
632:         limits:
633:           cpus: "1.0"
634:           memory: 128M
635:     labels:
636:       - "wud.trigger.docker.update=true"
637: 
638:   # Filebrowser - Web-based file manager with full RW access
639:   filebrowser:
640:     image: filebrowser/filebrowser:${FILEBROWSER_TAG:-latest}
641:     container_name: filebrowser
642:     logging: *default-logging
643:     ports:
644:       - "${HOST_BIND:-127.0.0.1}:8086:80"
645:     environment:
646:       <<: *common-env
647:       FILEBROWSER_USER: ${FILEBROWSER_USER:-admin}
648:       FILEBROWSER_PASSWORD: ${FILEBROWSER_PASSWORD:-change_this}
649:     entrypoint: sh /filebrowser-init.sh
650:     volumes:
651:       - /mnt/storage:/srv/storage
652:       - /mnt/storage2:/srv/storage2
653:       - /mnt/ssd/docker-data:/srv/docker-data
654:       - /mnt/ssd/docker-data/filebrowser:/config
655:       - ./scripts/init/filebrowser-init.sh:/filebrowser-init.sh:ro
656:     networks:
657:       - potatostack
658:     depends_on:
659:       - storage-init
660:     restart: unless-stopped
661:     healthcheck:
662:       test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:80/health || exit 1"]
663:       interval: 60s
664:       timeout: 10s
665:       retries: 3
666:       start_period: 30s
667:     deploy:
668:       resources:
669:         limits:
670:           cpus: "0.25"
671:           memory: 64M
672:     labels:
673:       - "wud.trigger.docker.update=true"
674: 
675:   # Filestash - Advanced web file manager with multi-protocol support (SOTA 2025)
676:   # Supports: Local, SFTP, FTP, S3, WebDAV, Git, LDAP, Dropbox, Google Drive
677:   # Features: Image/video/music previews, office doc viewer, text editor
678:   # Docs: https://www.filestash.app/docs/
679:   filestash:
680:     image: machines/filestash:${FILESTASH_TAG:-latest}
681:     container_name: filestash
682:     logging: *default-logging
683:     ports:
684:       - "${HOST_BIND:-127.0.0.1}:8095:8334"
685:     environment:
686:       <<: *common-env
687:       # Disable cloud integrations
688:       GDRIVE_CLIENT_ID: ""
689:       DROPBOX_CLIENT_ID: ""
690:     # Hardware acceleration for video playback
691:     devices:
692:       - /dev/dri/renderD128:/dev/dri/renderD128
693:       - /dev/dri/card0:/dev/dri/card0
694:     group_add:
695:       - "109"
696:     volumes:
697:       # Full storage access (same as filebrowser)
698:       - /mnt/storage:/mnt/storage
699:       - /mnt/storage2:/mnt/storage2
700:       - /mnt/ssd/docker-data:/mnt/docker-data:ro
701:       # Config persistence
702:       - /mnt/ssd/docker-data/filestash:/app/data/state
703:       # Plugins directory
704:       - /mnt/ssd/docker-data/filestash/plugins:/app/data/state/plugins
705:     networks:
706:       - potatostack
707:     depends_on:
708:       - storage-init
709:     labels:
710:       - "wud.trigger.docker.update=true"
711:     restart: unless-stopped
712:     healthcheck:
713:       test: ["CMD-SHELL", "curl -sf http://127.0.0.1:8334/ || exit 1"]
714:       interval: 60s
715:       timeout: 10s
716:       retries: 3
717:       start_period: 30s
718:     deploy:
719:       resources:
720:         limits:
721:           cpus: "0.5"
722:           memory: 256M
723:         reservations:
724:           cpus: "0.1"
725:           memory: 128M
726: 
727:   # Cloudflared - Quick tunnel for public access to FileBrowser
728:   # Get public URL: docker logs cloudflared 2>&1 | grep trycloudflare.com
729:   cloudflared:
730:     image: cloudflare/cloudflared:${CLOUDFLARED_TAG:-latest}
731:     container_name: cloudflared
732:     logging: *default-logging
733:     command: tunnel --no-autoupdate --url http://filebrowser:80
734:     networks:
735:       - potatostack
736:     depends_on:
737:       - filebrowser
738:     labels:
739:       - "wud.trigger.docker.update=true"
740:     restart: unless-stopped
741:     deploy:
742:       resources:
743:         limits:
744:           cpus: "0.25"
745:           memory: 128M
746:         reservations:
747:           cpus: "0.05"
748:           memory: 64M
749: 
750:     ################################################################################
751:     # KNOWLEDGE MANAGEMENT
752:     ################################################################################
753: 
754:   # Obsidian LiveSync - CouchDB backend for Obsidian note synchronization
755:   obsidian-livesync:
756:     image: oleduc/docker-obsidian-livesync-couchdb:${OBSIDIAN_LIVESYNC_TAG:-latest}
757:     container_name: obsidian-livesync
758:     logging: *default-logging
759:     ports:
760:       - "127.0.0.1:5984:5984"
761:     environment:
762:       <<: *common-env
763:       COUCHDB_USER: ${COUCHDB_USER:-obsidian}
764:       COUCHDB_PASSWORD: ${COUCHDB_PASSWORD}
765:       COUCHDB_DATABASE: ${COUCHDB_DATABASE:-obsidian-vault}
766:       COUCHDB_CORS_ORIGINS: https://obsidian.${HOST_DOMAIN:-local.domain},app://obsidian.md
767:     volumes:
768:       - /mnt/ssd/docker-data/obsidian-couchdb:/opt/couchdb/data
769:     networks:
770:       - potatostack
771:     depends_on:
772:       - storage-init
773:     labels:
774:       - "wud.trigger.docker.update=true"
775:     restart: unless-stopped
776:     healthcheck:
777:       test: ["CMD-SHELL", "curl -f -u $$COUCHDB_USER:$$COUCHDB_PASSWORD http://127.0.0.1:5984/_up || exit 1"]
778:       interval: 30s
779:       timeout: 10s
780:       retries: 5
781:     deploy:
782:       resources:
783:         limits:
784:           cpus: "0.5"
785:           memory: 256M
786:         reservations:
787:           cpus: "0.1"
788:           memory: 128M
789: 
790:   # Obsidian LiveSync Init - Single-node CouchDB bootstrap
791:   obsidian-livesync-init:
792:     image: alpine:${ALPINE_TAG:-latest}
793:     container_name: obsidian-livesync-init
794:     logging: *default-logging
795:     entrypoint:
796:       - "/bin/sh"
797:       - "-c"
798:       - "apk add --no-cache bash curl >/dev/null 2>&1 && /bin/bash /init-obsidian-livesync.sh"
799:     environment:
800:       <<: *common-env
801:       COUCHDB_HOST: "obsidian-livesync"
802:       COUCHDB_PORT: "5984"
803:       COUCHDB_USER: ${COUCHDB_USER:-obsidian}
804:       COUCHDB_PASSWORD: ${COUCHDB_PASSWORD}
805:       COUCHDB_DATABASE: ${COUCHDB_DATABASE:-obsidian-vault}
806:     volumes:
807:       - ./scripts/init/obsidian-livesync-init.sh:/init-obsidian-livesync.sh:ro
808:     networks:
809:       - potatostack
810:     depends_on:
811:       - obsidian-livesync
812:     restart: "no"
813: 
814:     ################################################################################
815:     # RSS & NEWS AGGREGATION
816:     ################################################################################
817: 
818:   # Miniflux - Minimalist RSS reader with Postgres backend (SOTA 2025)
819:   miniflux:
820:     image: miniflux/miniflux:${MINIFLUX_TAG:-latest}
821:     container_name: miniflux
822:     logging: *default-logging
823:     ports:
824:       - "${HOST_BIND:-127.0.0.1}:8093:8080"
825:     environment:
826:       <<: *common-env
827:       DATABASE_URL: postgres://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/miniflux?sslmode=disable
828:       RUN_MIGRATIONS: "1"
829:       CREATE_ADMIN: "1"
830:       ADMIN_USERNAME: ${MINIFLUX_ADMIN_USER:-admin}
831:       ADMIN_PASSWORD: ${MINIFLUX_ADMIN_PASSWORD}
832:       BASE_URL: https://rss.${HOST_DOMAIN:-local.domain}
833:       POLLING_FREQUENCY: 60
834:       BATCH_SIZE: 100
835:       WORKER_POOL_SIZE: 5
836:       METRICS_COLLECTOR: "1"
837:       METRICS_ALLOWED_NETWORKS: 172.22.0.0/16
838:       CLEANUP_FREQUENCY_HOURS: "1"
839:       CLEANUP_ARCHIVE_READ_DAYS: "7"
840:       CLEANUP_ARCHIVE_UNREAD_DAYS: "30"
841:     networks:
842:       - potatostack
843:     depends_on:
844:       - postgres
845:       - pgbouncer
846:     restart: unless-stopped
847:     healthcheck:
848:       test: ["CMD", "/usr/bin/miniflux", "-healthcheck", "auto"]
849:       interval: 60s
850:       timeout: 10s
851:       retries: 3
852:     deploy:
853:       resources:
854:         limits:
855:           cpus: "0.25"
856:           memory: 128M
857:         reservations:
858:           cpus: "0.1"
859:           memory: 64M
860:     labels:
861:       - "wud.trigger.docker.update=true"
862: 
863:   # Miniflux webhook to ntfy bridge
864:   miniflux-webhook:
865:     image: python:3.12-alpine
866:     container_name: miniflux-webhook
867:     logging: *default-logging
868:     command: ["python", "/app/miniflux-webhook.py"]
869:     environment:
870:       <<: *common-env
871:       MINIFLUX_NTFY_PORT: 8083
872:       NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
873:       NTFY_TOPIC_INFO: ${NTFY_TOPIC_INFO:-potatostack-info}
874:       NTFY_TOKEN: ${NTFY_TOKEN:-}
875:     volumes:
876:       - ./scripts/webhooks/miniflux-webhook.py:/app/miniflux-webhook.py:ro
877:     networks:
878:       - potatostack
879:     depends_on:
880:       - ntfy
881:     restart: unless-stopped
882:     deploy:
883:       resources:
884:         limits:
885:           cpus: "0.1"
886:           memory: 64M
887:         reservations:
888:           cpus: "0.05"
889:           memory: 32M
890: 
891:   ################################################################################
892:   # FINANCE
893:   ################################################################################
894: 
895:   # Actual Budget - Modern budgeting with bank sync (SOTA 2025)
896:   actual-budget:
897:     image: actualbudget/actual-server:${ACTUAL_TAG:-latest}
898:     container_name: actual-budget
899:     logging: *default-logging
900:     # NOTE: Access via Traefik HTTPS (https://budget.danielhomelab.local) for SharedArrayBuffer support
901:     # Direct port access won't work as browsers require COOP/COEP headers only sent via secure context
902:     ports:
903:       - "${HOST_BIND:-127.0.0.1}:5006:5006"
904:     environment:
905:       <<: *common-env
906:     volumes:
907:       - /mnt/ssd/docker-data/actual-budget:/data
908:     networks:
909:       - potatostack
910:     labels:
911:       - "wud.trigger.docker.update=true"
912:     restart: unless-stopped
913:     healthcheck:
914:       test: ["CMD-SHELL", "timeout 5 bash -c 'cat < /dev/null > /dev/tcp/127.0.0.1/5006' || exit 1"]
915:       interval: 60s
916:       timeout: 10s
917:       retries: 3
918:     deploy:
919:       resources:
920:         limits:
921:           cpus: "0.25"
922:           memory: 128M
923:         reservations:
924:           cpus: "0.1"
925:           memory: 128M
926:       # Use shared-array-chain for SharedArrayBuffer support (COOP/COEP headers)
927: 
928:   # Freqtrade - Algorithmic trading bot with Adaptive Regime Strategy
929:   freqtrade-bot:
930:     build:
931:       context: ${FREQTRADE_STRATEGY_PATH:-./.related_deps/freqtrader-strategy}
932:       dockerfile: Dockerfile
933:       target: runtime
934:     image: freqtrader-strategy:runtime
935:     container_name: freqtrade-bot
936:     logging: *default-logging
937:     ports:
938:       - "${HOST_BIND:-127.0.0.1}:8181:8080"
939:     command: ["freqtrade", "trade", "--config", "/app/config.json", "--strategy", "AdaptiveRegimeStrategy", "--strategy-path", "/app/strategies/"]
940:     environment:
941:       <<: *common-env
942:       DATABASE_URL: postgresql://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/freqtrade
943:       REDIS_URL: redis://redis-cache:6379/10
944:       NTFY_URL: http://ntfy:80
945:       NTFY_TOPIC: ${FREQTRADE_NTFY_TOPIC:-freqtrade-regime}
946:       KRAKEN_API_KEY: ${FREQTRADE_KRAKEN_API_KEY:-}
947:       KRAKEN_API_SECRET: ${FREQTRADE_KRAKEN_API_SECRET:-}
948:       FRED_API_KEY: ${FREQTRADE_FRED_API_KEY:-}
949:       COINMETRICS_API_KEY: ${FREQTRADE_COINMETRICS_API_KEY:-}
950:       SOSOVALUE_API_KEY: ${FREQTRADE_SOSOVALUE_API_KEY:-}
951:       COINGLASS_API_KEY: ${FREQTRADE_COINGLASS_API_KEY:-}
952:       REGIME_UPDATE_INTERVAL_SEC: ${FREQTRADE_REGIME_UPDATE_INTERVAL_SEC:-300}
953:       REGIME_LAZINESS_WINDOW: ${FREQTRADE_REGIME_LAZINESS_WINDOW:-5}
954:     volumes:
955:       - ${FREQTRADE_STRATEGY_PATH:-./.related_deps/freqtrader-strategy}/config.json:/app/config.json:ro
956:       - ${FREQTRADE_STRATEGY_PATH:-./.related_deps/freqtrader-strategy}/strategies:/app/strategies:ro
957:       - /mnt/ssd/docker-data/freqtrade/user_data:/app/user_data
958:     networks:
959:       - potatostack
960:     depends_on:
961:       postgres:
962:         condition: service_healthy
963:       redis-cache:
964:         condition: service_healthy
965:     restart: unless-stopped
966:     healthcheck:
967:       test: ["CMD-SHELL", "timeout 5 bash -c 'cat < /dev/null > /dev/tcp/127.0.0.1/8080' || exit 1"]
968:       interval: 60s
969:       timeout: 10s
970:       retries: 3
971:     deploy:
972:       resources:
973:         limits:
974:           cpus: "1.0"
975:           memory: 512M
976:         reservations:
977:           cpus: "0.25"
978:           memory: 256M
979:     labels:
980:       - "wud.trigger.docker.update=false"
981: 
982:   # Regime Classifier - Companion daemon for freqtrade regime detection
983:   regime-classifier:
984:     build:
985:       context: ${FREQTRADE_STRATEGY_PATH:-./.related_deps/freqtrader-strategy}
986:       dockerfile: Dockerfile
987:       target: runtime
988:     image: freqtrader-strategy:runtime
989:     container_name: regime-classifier
990:     logging: *default-logging
991:     command: ["python", "-m", "services.classifier_service"]
992:     environment:
993:       <<: *common-env
994:       DATABASE_URL: postgresql://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/freqtrade
995:       REDIS_URL: redis://redis-cache:6379/10
996:       NTFY_URL: http://ntfy:80
997:       NTFY_TOPIC: ${FREQTRADE_NTFY_TOPIC:-freqtrade-regime}
998:       KRAKEN_API_KEY: ${FREQTRADE_KRAKEN_API_KEY:-}
999:       KRAKEN_API_SECRET: ${FREQTRADE_KRAKEN_API_SECRET:-}
1000:       FRED_API_KEY: ${FREQTRADE_FRED_API_KEY:-}
1001:       COINMETRICS_API_KEY: ${FREQTRADE_COINMETRICS_API_KEY:-}
1002:       SOSOVALUE_API_KEY: ${FREQTRADE_SOSOVALUE_API_KEY:-}
1003:       COINGLASS_API_KEY: ${FREQTRADE_COINGLASS_API_KEY:-}
1004:       REGIME_UPDATE_INTERVAL_SEC: ${FREQTRADE_REGIME_UPDATE_INTERVAL_SEC:-300}
1005:       REGIME_LAZINESS_WINDOW: ${FREQTRADE_REGIME_LAZINESS_WINDOW:-5}
1006:     networks:
1007:       - potatostack
1008:     depends_on:
1009:       postgres:
1010:         condition: service_healthy
1011:       redis-cache:
1012:         condition: service_healthy
1013:     restart: unless-stopped
1014:     deploy:
1015:       resources:
1016:         limits:
1017:           cpus: "1.0"
1018:           memory: 512M
1019:         reservations:
1020:           cpus: "0.25"
1021:           memory: 256M
1022:     labels:
1023:       - "wud.trigger.docker.update=false"
1024: 
1025:   # Ghostfolio - Wealth management & portfolio tracker
1026:   ghostfolio:
1027:     image: ghostfolio/ghostfolio:${GHOSTFOLIO_TAG:-latest}
1028:     container_name: ghostfolio
1029:     logging: *default-logging
1030:     ports:
1031:       - "${HOST_BIND:-127.0.0.1}:3335:3333"
1032:     environment:
1033:       <<: *common-env
1034:       NODE_ENV: production
1035:       NODE_OPTIONS: "--max-old-space-size=640"
1036:       DATABASE_URL: postgresql://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/ghostfolio
1037:       REDIS_HOST: redis-cache
1038:       REDIS_PORT: 6379
1039:       REDIS_PASSWORD: ""
1040:       REDIS_DB: 11
1041:       ACCESS_TOKEN_SALT: ${GHOSTFOLIO_ACCESS_TOKEN_SALT}
1042:       JWT_SECRET_KEY: ${GHOSTFOLIO_JWT_SECRET_KEY}
1043:     volumes:
1044:       - /mnt/storage/financial-data:/import:ro
1045:     networks:
1046:       - potatostack
1047:     depends_on:
1048:       postgres:
1049:         condition: service_healthy
1050:       redis-cache:
1051:         condition: service_healthy
1052:     restart: unless-stopped
1053:     healthcheck:
1054:       test: ["CMD-SHELL", "curl -f http://localhost:3333/api/v1/health || exit 1"]
1055:       interval: 60s
1056:       timeout: 10s
1057:       retries: 5
1058:     deploy:
1059:       resources:
1060:         limits:
1061:           cpus: "0.5"
1062:           memory: 768M
1063:         reservations:
1064:           cpus: "0.1"
1065:           memory: 256M
1066:     labels:
1067:       - "wud.trigger.docker.update=true"
1068: 
1069:   ################################################################################
1070:   # MEDIA MANAGEMENT - *ARR STACK
1071:   ################################################################################
1072: 
1073:   # Prowlarr - Indexer manager (DISABLED)
1074:   # prowlarr:
1075:   #   image: lscr.io/linuxserver/prowlarr:${PROWLARR_TAG:-latest}
1076:   #   container_name: prowlarr
1077:   #   logging: *default-logging
1078:   #   network_mode: "service:gluetun"
1079:   #   environment:
1080:   #     <<: *common-env
1081:   #   volumes:
1082:   #     - prowlarr-config:/config
1083:   #   depends_on:
1084:   #     gluetun:
1085:   #       condition: service_healthy
1086:   #       restart: true
1087:   #     storage-init:
1088:   #       condition: service_completed_successfully
1089:   #   restart: unless-stopped
1090:   #   healthcheck:
1091:   #     test: ["CMD-SHELL", "curl -f http://127.0.0.1:9696/ || exit 1"]
1092:   #     interval: 60s
1093:   #     timeout: 10s
1094:   #     retries: 3
1095:   #     start_period: 30s
1096:   #   deploy:
1097:   #     resources:
1098:   #       limits:
1099:   #         cpus: "0.25"
1100:   #         memory: 512M
1101:   #       reservations:
1102:   #         cpus: "0.1"
1103:   #         memory: 256M
1104:   #   labels:
1105:   #     - "autoheal=true"
1106: 
1107:   # FlareSolverr - Cloudflare bypass for Prowlarr (DISABLED)
1108:   # flaresolverr:
1109:   #   image: ghcr.io/flaresolverr/flaresolverr:${FLARESOLVERR_TAG:-latest}
1110:   #   container_name: flaresolverr
1111:   #   logging: *default-logging
1112:   #   network_mode: "service:gluetun"
1113:   #   environment:
1114:   #     <<: *common-env
1115:   #     LOG_LEVEL: info
1116:   #     LOG_HTML: "false"
1117:   #     CAPTCHA_SOLVER: none
1118:   #   depends_on:
1119:   #     gluetun:
1120:   #       condition: service_healthy
1121:   #       restart: true
1122:   #   restart: unless-stopped
1123:   #   healthcheck:
1124:   #     test: ["CMD-SHELL", "curl -f http://127.0.0.1:8191/health || exit 1"]
1125:   #     interval: 60s
1126:   #     timeout: 10s
1127:   #     retries: 3
1128:   #     start_period: 30s
1129:   #   deploy:
1130:   #     resources:
1131:   #       limits:
1132:   #         cpus: "0.5"
1133:   #         memory: 512M
1134:   #       reservations:
1135:   #         cpus: "0.1"
1136:   #         memory: 256M
1137:   #   labels:
1138:   #     - "autoheal=true"
1139: 
1140:   # Sonarr - TV show management (DISABLED)
1141:   # sonarr:
1142:   #   image: lscr.io/linuxserver/sonarr:${SONARR_TAG:-latest}
1143:   #   container_name: sonarr
1144:   #   logging: *default-logging
1145:   #   network_mode: "service:gluetun"
1146:   #   environment:
1147:   #     <<: *common-env
1148:   #   volumes:
1149:   #     - sonarr-config:/config
1150:   #     - /mnt/storage2/media/tv:/tv
1151:   #     - /mnt/storage2/downloads:/downloads
1152:   #     - /mnt/storage2/downloads/incomplete/sonarr:/downloads/incomplete
1153:   #   depends_on:
1154:   #     gluetun:
1155:   #       condition: service_healthy
1156:   #       restart: true
1157:   #     storage-init:
1158:   #       condition: service_completed_successfully
1159:   #   restart: unless-stopped
1160:   #   healthcheck:
1161:   #     test: ["CMD-SHELL", "curl -sf http://127.0.0.1:8989/ping || exit 1"]
1162:   #     interval: 60s
1163:   #     timeout: 10s
1164:   #     retries: 3
1165:   #     start_period: 30s
1166:   #   deploy:
1167:   #     resources:
1168:   #       limits:
1169:   #         cpus: "1.0"
1170:   #         memory: 256M
1171:   #       reservations:
1172:   #         cpus: "0.25"
1173:   #         memory: 128M
1174:   #   labels:
1175:   #     - "autoheal=true"
1176: 
1177:   # Radarr - Movie management (DISABLED)
1178:   # radarr:
1179:   #   image: lscr.io/linuxserver/radarr:${RADARR_TAG:-latest}
1180:   #   container_name: radarr
1181:   #   logging: *default-logging
1182:   #   network_mode: "service:gluetun"
1183:   #   environment:
1184:   #     <<: *common-env
1185:   #   volumes:
1186:   #     - radarr-config:/config
1187:   #     - /mnt/storage2/media/movies:/movies
1188:   #     - /mnt/storage2/downloads:/downloads
1189:   #     - /mnt/storage2/downloads/incomplete/radarr:/downloads/incomplete
1190:   #   depends_on:
1191:   #     gluetun:
1192:   #       condition: service_healthy
1193:   #       restart: true
1194:   #     storage-init:
1195:   #       condition: service_completed_successfully
1196:   #   restart: unless-stopped
1197:   #   healthcheck:
1198:   #     test: ["CMD-SHELL", "curl -f http://127.0.0.1:7878/ || exit 1"]
1199:   #     interval: 60s
1200:   #     timeout: 10s
1201:   #     retries: 3
1202:   #     start_period: 30s
1203:   #   deploy:
1204:   #     resources:
1205:   #       limits:
1206:   #         cpus: "1.0"
1207:   #         memory: 256M
1208:   #       reservations:
1209:   #         cpus: "0.25"
1210:   #         memory: 128M
1211:   #   labels:
1212:   #     - "autoheal=true"
1213: 
1214:   # Lidarr - Music management (DISABLED)
1215:   # lidarr:
1216:   #   image: lscr.io/linuxserver/lidarr:${LIDARR_TAG:-latest}
1217:   #   container_name: lidarr
1218:   #   logging: *default-logging
1219:   #   network_mode: "service:gluetun"
1220:   #   environment:
1221:   #     <<: *common-env
1222:   #   volumes:
1223:   #     - lidarr-config:/config
1224:   #     - /mnt/storage/media/music:/music
1225:   #     - /mnt/storage2/downloads:/downloads
1226:   #     - /mnt/storage2/downloads/incomplete/lidarr:/downloads/incomplete
1227:   #   depends_on:
1228:   #     gluetun:
1229:   #       condition: service_healthy
1230:   #       restart: true
1231:   #     storage-init:
1232:   #       condition: service_completed_successfully
1233:   #   restart: unless-stopped
1234:   #   healthcheck:
1235:   #     test: ["CMD-SHELL", "curl -f http://127.0.0.1:8686/ || exit 1"]
1236:   #     interval: 60s
1237:   #     timeout: 10s
1238:   #     retries: 3
1239:   #     start_period: 30s
1240:   #   deploy:
1241:   #     resources:
1242:   #       limits:
1243:   #         cpus: "1.0"
1244:   #         memory: 256M
1245:   #       reservations:
1246:   #         cpus: "0.25"
1247:   #         memory: 128M
1248:   #   labels:
1249:   #     - "autoheal=true"
1250: 
1251:   # Bazarr - Subtitle management (DISABLED)
1252:   # bazarr:
1253:   #   image: lscr.io/linuxserver/bazarr:${BAZARR_TAG:-latest}
1254:   #   container_name: bazarr
1255:   #   logging: *default-logging
1256:   #   network_mode: "service:gluetun"
1257:   #   environment:
1258:   #     <<: *common-env
1259:   #   volumes:
1260:   #     - bazarr-config:/config
1261:   #     - /mnt/storage2/media/movies:/movies
1262:   #     - /mnt/storage2/media/tv:/tv
1263:   #   tmpfs:
1264:   #     - /tmp:exec,mode=1777,size=500m
1265:   #   depends_on:
1266:   #     gluetun:
1267:   #       condition: service_healthy
1268:   #       restart: true
1269:   #     storage-init:
1270:   #       condition: service_completed_successfully
1271:   #   restart: unless-stopped
1272:   #   healthcheck:
1273:   #     test: ["CMD-SHELL", "curl -f http://127.0.0.1:6767/ || exit 1"]
1274:   #     interval: 60s
1275:   #     timeout: 10s
1276:   #     retries: 3
1277:   #     start_period: 30s
1278:   #   deploy:
1279:   #     resources:
1280:   #       limits:
1281:   #         cpus: "0.25"
1282:   #         memory: 384M
1283:   #       reservations:
1284:   #         cpus: "0.1"
1285:   #         memory: 192M
1286:   #   labels:
1287:   #     - "autoheal=true"
1288: 
1289:   # Maintainerr - Rules-based media library cleanup (DISABLED)
1290:   # maintainerr:
1291:   #   image: jorenn92/maintainerr:${MAINTAINERR_TAG:-latest}
1292:   #   container_name: maintainerr
1293:   #   logging: *default-logging
1294:   #   ports:
1295:   #     - "${HOST_BIND:-127.0.0.1}:6246:6246"
1296:   #   environment:
1297:   #     <<: *common-env
1298:   #   volumes:
1299:   #     - maintainerr-data:/opt/data
1300:   #   networks:
1301:   #     - potatostack
1302:   #   depends_on:
1303:   #     - sonarr
1304:   #     - radarr
1305:   #   restart: unless-stopped
1306:   #   healthcheck:
1307:   #     test: ["CMD-SHELL", "wget -q --spider http://127.0.0.1:6246/api/health || exit 1"]
1308:   #     interval: 60s
1309:   #     timeout: 10s
1310:   #     retries: 3
1311:   #     start_period: 30s
1312:   #   deploy:
1313:   #     resources:
1314:   #       limits:
1315:   #         cpus: "0.25"
1316:   #         memory: 256M
1317:   #       reservations:
1318:   #         cpus: "0.1"
1319:   #         memory: 128M
1320: 
1321:   # Recyclarr - TRaSH Guides quality profile sync (DISABLED)
1322:   # recyclarr:
1323:   #   image: ghcr.io/recyclarr/recyclarr:${RECYCLARR_TAG:-latest}
1324:   #   container_name: recyclarr
1325:   #   logging: *default-logging
1326:   #   user: 1000:1000
1327:   #   environment:
1328:   #     <<: *common-env
1329:   #     RECYCLARR_CREATE_CONFIG: "true"
1330:   #   volumes:
1331:   #     - /mnt/ssd/docker-data/recyclarr:/config
1332:   #   networks:
1333:   #     - potatostack
1334:   #   depends_on:
1335:   #     - sonarr
1336:   #     - radarr
1337:   #   restart: unless-stopped
1338:   #   deploy:
1339:   #     resources:
1340:   #       limits:
1341:   #         cpus: "0.25"
1342:   #         memory: 128M
1343:   #       reservations:
1344:   #         cpus: "0.05"
1345:   #         memory: 64M
1346:   ################################################################################
1347:   # Unpackerr - Extracts archived downloads (standalone mode - no arr dependency)
1348:   ################################################################################
1349:   unpackerr:
1350:     image: ghcr.io/hotio/unpackerr:${UNPACKERR_TAG:-latest}
1351:     container_name: unpackerr
1352:     logging: *default-logging
1353:     entrypoint: ["/entrypoint.sh"]
1354:     environment:
1355:       <<: *common-env
1356: 
1357:       # Global Settings - Lightweight, low I/O, crash-safe, immediate startup
1358:       UMASK: "022"
1359: 
1360:       UN_INTERVAL: "1m"
1361:       UN_START_DELAY: "0m"
1362:       UN_RETRY_DELAY: "2m"
1363:       UN_MAX_RETRIES: "5"
1364:       UN_PARALLEL: "2"
1365:       UN_FILE_MODE: "0755"
1366:       UN_DIR_MODE: "0755"
1367:       UN_LOG_LEVEL: "INFO"
1368: 
1369:       # Web UI - No password (LAN access only)
1370:       UN_WEBSERVER_METRICS: "false"
1371:       UN_WEBSERVER_ENABLED: "true"
1372:       UN_WEBSERVER_LISTEN_ADDR: "0.0.0.0:5656"
1373: 
1374:       # Folder Watchers - All download folders (incomplete folders excluded)
1375:       # qBittorrent - Extract but DON'T delete (preserve seeding)
1376:       # MOVE_BACK=false: extracted content goes to <name>_unpackerred/ subdir,
1377:       # which persists as a marker so restarts don't re-extract the same archives.
1378:       UN_FOLDER_0_PATH: "/downloads/torrents"
1379:       UN_FOLDER_0_DELETE_ORIGINAL: "false"
1380:       UN_FOLDER_0_DELETE_AFTER: "0s"
1381:       UN_FOLDER_0_MOVE_BACK: "false"
1382:       UN_FOLDER_0_DELETE_FILES: "false"
1383:       UN_FOLDER_0_EXTRACT_ISOS: "false"
1384: 
1385:       # qBittorrent (adult) - Extract + delete (manually copied archives)
1386:       UN_FOLDER_1_PATH: "/media/adult"
1387:       UN_FOLDER_1_DELETE_ORIGINAL: "true"
1388:       UN_FOLDER_1_DELETE_AFTER: "0s"
1389:       UN_FOLDER_1_MOVE_BACK: "true"
1390:       UN_FOLDER_1_DELETE_FILES: "false"
1391:       UN_FOLDER_1_EXTRACT_ISOS: "false"
1392: 
1393:       # Aria2 - Extract + delete (normal cleanup)
1394:       UN_FOLDER_2_PATH: "/downloads/aria2"
1395:       UN_FOLDER_2_DELETE_ORIGINAL: "true"
1396:       UN_FOLDER_2_DELETE_AFTER: "0s"
1397:       UN_FOLDER_2_MOVE_BACK: "true"
1398:       UN_FOLDER_2_DELETE_FILES: "false"
1399:       UN_FOLDER_2_EXTRACT_ISOS: "false"
1400: 
1401:       # Slskd - Extract + delete
1402:       UN_FOLDER_3_PATH: "/downloads/slskd"
1403:       UN_FOLDER_3_DELETE_ORIGINAL: "true"
1404:       UN_FOLDER_3_DELETE_AFTER: "0s"
1405:       UN_FOLDER_3_MOVE_BACK: "true"
1406:       UN_FOLDER_3_DELETE_FILES: "false"
1407:       UN_FOLDER_3_EXTRACT_ISOS: "false"
1408: 
1409:       # Pyload - Extract + delete
1410:       UN_FOLDER_4_PATH: "/downloads/pyload"
1411:       UN_FOLDER_4_DELETE_ORIGINAL: "true"
1412:       UN_FOLDER_4_DELETE_AFTER: "0s"
1413:       UN_FOLDER_4_MOVE_BACK: "true"
1414:       UN_FOLDER_4_DELETE_FILES: "false"
1415:       UN_FOLDER_4_EXTRACT_ISOS: "false"
1416: 
1417:       # TDL (Telegram) - Extract + delete
1418:       UN_FOLDER_5_PATH: "/downloads/telegram"
1419:       UN_FOLDER_5_DELETE_ORIGINAL: "true"
1420:       UN_FOLDER_5_DELETE_AFTER: "0s"
1421:       UN_FOLDER_5_MOVE_BACK: "true"
1422:       UN_FOLDER_5_DELETE_FILES: "false"
1423:       UN_FOLDER_5_EXTRACT_ISOS: "false"
1424: 
1425:       # storage2 (SSD) - movies - Extract + delete
1426:       UN_FOLDER_6_PATH: "/media/movies"
1427:       UN_FOLDER_6_DELETE_ORIGINAL: "true"
1428:       UN_FOLDER_6_DELETE_AFTER: "0s"
1429:       UN_FOLDER_6_MOVE_BACK: "true"
1430:       UN_FOLDER_6_DELETE_FILES: "false"
1431:       UN_FOLDER_6_EXTRACT_ISOS: "false"
1432: 
1433:       # storage2 (SSD) - tv - Extract + delete
1434:       UN_FOLDER_12_PATH: "/media/tv"
1435:       UN_FOLDER_12_DELETE_ORIGINAL: "true"
1436:       UN_FOLDER_12_DELETE_AFTER: "0s"
1437:       UN_FOLDER_12_MOVE_BACK: "true"
1438:       UN_FOLDER_12_DELETE_FILES: "false"
1439:       UN_FOLDER_12_EXTRACT_ISOS: "false"
1440: 
1441:       # storage (HDD) - torrent folder - Extract but DON'T delete (preserve seeding)
1442:       # MOVE_BACK=false: extracted content goes to <name>_unpackerred/ subdir,
1443:       # which persists as a marker so restarts don't re-extract the same archives.
1444:       UN_FOLDER_7_PATH: "/storage/downloads/torrent"
1445:       UN_FOLDER_7_DELETE_ORIGINAL: "false"
1446:       UN_FOLDER_7_DELETE_AFTER: "0s"
1447:       UN_FOLDER_7_MOVE_BACK: "false"
1448:       UN_FOLDER_7_DELETE_FILES: "false"
1449:       UN_FOLDER_7_EXTRACT_ISOS: "false"
1450: 
1451:       # storage (HDD) - rdt-client - Extract + delete (debrid synced, no seeding needed)
1452:       UN_FOLDER_8_PATH: "/storage/downloads/rdt-client"
1453:       UN_FOLDER_8_DELETE_ORIGINAL: "true"
1454:       UN_FOLDER_8_DELETE_AFTER: "0s"
1455:       UN_FOLDER_8_MOVE_BACK: "true"
1456:       UN_FOLDER_8_DELETE_FILES: "false"
1457:       UN_FOLDER_8_EXTRACT_ISOS: "false"
1458: 
1459:       # storage (HDD) - pyload - Extract + delete
1460:       UN_FOLDER_9_PATH: "/storage/downloads/pyload"
1461:       UN_FOLDER_9_DELETE_ORIGINAL: "true"
1462:       UN_FOLDER_9_DELETE_AFTER: "0s"
1463:       UN_FOLDER_9_MOVE_BACK: "true"
1464:       UN_FOLDER_9_DELETE_FILES: "false"
1465:       UN_FOLDER_9_EXTRACT_ISOS: "false"
1466: 
1467:       # storage (HDD) - slskd - Extract + delete
1468:       UN_FOLDER_10_PATH: "/storage/downloads/slskd"
1469:       UN_FOLDER_10_DELETE_ORIGINAL: "true"
1470:       UN_FOLDER_10_DELETE_AFTER: "0s"
1471:       UN_FOLDER_10_MOVE_BACK: "true"
1472:       UN_FOLDER_10_DELETE_FILES: "false"
1473:       UN_FOLDER_10_EXTRACT_ISOS: "false"
1474: 
1475:       # storage (HDD) - media - Extract + delete (books, music, audiobooks, etc.)
1476:       UN_FOLDER_11_PATH: "/storage/media"
1477:       UN_FOLDER_11_DELETE_ORIGINAL: "true"
1478:       UN_FOLDER_11_DELETE_AFTER: "0s"
1479:       UN_FOLDER_11_MOVE_BACK: "true"
1480:       UN_FOLDER_11_DELETE_FILES: "false"
1481:       UN_FOLDER_11_EXTRACT_ISOS: "false"
1482: 
1483:       # NTFY notification hook
1484:       NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
1485:       NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
1486:       NTFY_TOKEN: ${NTFY_TOKEN:-}
1487: 
1488:       # Command Hook for NTFY notifications + ownership fix
1489:       UN_CMDHOOK_0_NAME: "ntfy-ownership"
1490:       UN_CMDHOOK_0_COMMAND: "/hooks/post-extract.sh"
1491:       UN_CMDHOOK_0_SHELL: "true"
1492:       UN_CMDHOOK_0_SILENT: "false"
1493:       UN_CMDHOOK_0_EVENTS_0: "2"
1494:       UN_CMDHOOK_0_EVENTS_1: "3"
1495:       UN_CMDHOOK_0_EVENTS_2: "4"
1496:       UN_CMDHOOK_0_EVENTS_3: "5"
1497:       UN_CMDHOOK_0_EVENTS_4: "6"
1498:       UN_CMDHOOK_0_EVENTS_5: "7"
1499:       UN_CMDHOOK_0_TIMEOUT: "30s"
1500: 
1501:     volumes:
1502:       - unpackerr-config:/config
1503:       - /mnt/storage2/downloads:/downloads
1504:       - /mnt/storage2/media:/media
1505:       - /mnt/storage/downloads:/storage/downloads
1506:       - /mnt/storage/media:/storage/media
1507:       - /home/daniel/potatostack/scripts/hooks/unpackerr-post-extract.sh:/hooks/post-extract.sh:ro
1508:       - /home/daniel/potatostack/scripts/init/unpackerr-entrypoint.sh:/entrypoint.sh:ro
1509:       - /home/daniel/potatostack/scripts/init/unpackerr-cleanup.sh:/cleanup.sh:ro
1510:       - /home/daniel/potatostack/scripts/init/unpackerr-startup-scan.sh:/startup-scan.sh:ro
1511:     networks:
1512:       - potatostack
1513:     depends_on:
1514:       - storage-init
1515:     restart: unless-stopped
1516:     healthcheck:
1517:       test: ["CMD-SHELL", "pgrep -f unpackerr || exit 1"]
1518:       interval: 60s
1519:       timeout: 10s
1520:       retries: 3
1521:       start_period: 30s
1522:     deploy:
1523:       resources:
1524:         limits:
1525:           cpus: "1.0"
1526:           memory: 128M
1527:         reservations:
1528:           cpus: "0.25"
1529:           memory: 64M
1530:     labels:
1531:       - "wud.trigger.docker.update=true"
1532:   ################################################################################
1533:   # Notifiarr - Unified notifications for *arr stack (DISABLED)
1534:   ################################################################################
1535:   # notifiarr:
1536:   #   image: golift/notifiarr:${NOTIFIARR_TAG:-latest}
1537:   #   container_name: notifiarr
1538:   #   logging: *default-logging
1539:   #   hostname: potatostack
1540:   #   ports:
1541:   #     - "${HOST_BIND:-127.0.0.1}:5454:5454"
1542:   #   environment:
1543:   #     <<: *common-env
1544:   #     DN_API_KEY: ${NOTIFIARR_API_KEY:-}
1545:   #     DN_UI_PASSWORD: ${NOTIFIARR_UI_PASSWORD:-}
1546:   #     # Sonarr
1547:   #     DN_SONARR_0_URL: http://gluetun:8989
1548:   #     DN_SONARR_0_API_KEY: ${SONARR_API_KEY:-}
1549:   #     # Radarr
1550:   #     DN_RADARR_0_URL: http://gluetun:7878
1551:   #     DN_RADARR_0_API_KEY: ${RADARR_API_KEY:-}
1552:   #     # Lidarr
1553:   #     DN_LIDARR_0_URL: http://gluetun:8686
1554:   #     DN_LIDARR_0_API_KEY: ${LIDARR_API_KEY:-}
1555:   #     # Prowlarr
1556:   #     DN_PROWLARR_0_URL: http://gluetun:9696
1557:   #     DN_PROWLARR_0_API_KEY: ${PROWLARR_API_KEY:-}
1558:   #   volumes:
1559:   #     - /mnt/ssd/docker-data/notifiarr:/config
1560:   #     - /var/run/utmp:/var/run/utmp:ro
1561:   #   networks:
1562:   #     - potatostack
1563:   #   depends_on:
1564:   #     - sonarr
1565:   #     - radarr
1566:   #   restart: unless-stopped
1567:   #   healthcheck:
1568:   #     test: ["CMD", "wget", "-q", "--spider", "http://127.0.0.1:5454/"]
1569:   #     interval: 60s
1570:   #     timeout: 10s
1571:   #     retries: 3
1572:   #     start_period: 30s
1573:   #   deploy:
1574:   #     resources:
1575:   #       limits:
1576:   #         cpus: "0.25"
1577:   #         memory: 192M
1578:   #       reservations:
1579:   #         cpus: "0.05"
1580:   #         memory: 64M
1581: 
1582:   # Exportarr - Prometheus metrics exporter for *arr stack (DISABLED)
1583:   # exportarr:

(Output capped at 50 KB. Showing lines 1-1583. Use offset=1584 to continue.)
</content>
```

**Tool: read**

**Input:**
```json
{
  "filePath": "/home/daniel/potatostack/config/homer/config.yml"
}
```

**Output:**
```
<path>/home/daniel/potatostack/config/homer/config.yml</path>
<type>file</type>
<content>1: ---
2: # PotatoStack Homer Dashboard
3: 
4: title: "PotatoStack"
5: subtitle: "Self-Hosted Infrastructure"
6: documentTitle: "PotatoStack Dashboard"
7: logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/homer.png"
8: icon: "fas fa-server"
9: 
10: header: true
11: footer: '<p>PotatoStack <span class="has-text-grey">|</span> <a href="https://github.com/bastienwirtz/homer" target="_blank">Homer</a></p>'
12: 
13: columns: "4"
14: connectivityCheck: false
15: 
16: stylesheet:
17:   - "assets/custom.css"
18: 
19: defaults:
20:   layout: columns
21:   colorTheme: dark
22: 
23: colors:
24:   light:
25:     highlight-primary: "#1a1a1a"
26:     highlight-secondary: "#2d2d2d"
27:     highlight-hover: "#3d3d3d"
28:     background: "#0d0d0d"
29:     card-background: "#1a1a1a"
30:     text: "#f5f5f5"
31:     text-header: "#ffffff"
32:     text-title: "#ffffff"
33:     text-subtitle: "#b0b0b0"
34:     card-shadow: rgba(0, 0, 0, 0.5)
35:     link: "#ffffff"
36:     link-hover: "#4a9eff"
37:   dark:
38:     highlight-primary: "#0f0f0f"
39:     highlight-secondary: "#1a1a1a"
40:     highlight-hover: "#2a2a2a"
41:     background: "#000000"
42:     card-background: "#0f0f0f"
43:     text: "#f0f0f0"
44:     text-header: "#ffffff"
45:     text-title: "#ffffff"
46:     text-subtitle: "#909090"
47:     card-shadow: rgba(0, 0, 0, 0.8)
48:     link: "#ffffff"
49:     link-hover: "#4a9eff"
50: 
51: links:
52:   - name: "GitHub"
53:     icon: "fab fa-github"
54:     url: "https://github.com"
55:     target: "_blank"
56: 
57: services:
58:   - name: "Media"
59:     icon: "fas fa-play-circle"
60:     items:
61:       - name: "Jellyfin"
62:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/jellyfin.png"
63:         subtitle: "Movies & TV"
64:         url: "https://potatostack.tale-iwato.ts.net:8096"
65:         target: "_blank"
66: 
67: 
68:       - name: "Audiobookshelf"
69:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/audiobookshelf.png"
70:         subtitle: "Audiobooks & Podcasts"
71:         url: "https://potatostack.tale-iwato.ts.net:13378"
72:         target: "_blank"
73: 
74:       - name: "Navidrome"
75:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/navidrome.png"
76:         subtitle: "Music"
77:         url: "https://potatostack.tale-iwato.ts.net:4533"
78:         target: "_blank"
79: 
80:       - name: "Stash"
81:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/stash.png"
82:         subtitle: "Adult Media"
83:         url: "https://potatostack.tale-iwato.ts.net:9900"
84:         target: "_blank"
85: 
86:   - name: "Downloads"
87:     icon: "fas fa-download"
88:     items:
89:       - name: "qBittorrent"
90:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/qbittorrent.png"
91:         subtitle: "Torrents"
92:         url: "https://potatostack.tale-iwato.ts.net:8282"
93:         target: "_blank"
94: 
95:       - name: "slskd"
96:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/slskd.png"
97:         subtitle: "Soulseek"
98:         url: "https://potatostack.tale-iwato.ts.net:2234"
99:         target: "_blank"
100: 
101:       - name: "SpotiFLAC"
102:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/spotify.png"
103:         subtitle: "Spotify Ripper"
104:         url: "https://potatostack.tale-iwato.ts.net:8097"
105:         target: "_blank"
106: 
107:       - name: "pyLoad"
108:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/pyload.png"
109:         subtitle: "Direct Downloads"
110:         url: "https://potatostack.tale-iwato.ts.net:8076"
111:         target: "_blank"
112: 
113: 
114:   - name: "Photos & Files"
115:     icon: "fas fa-images"
116:     items:
117:       - name: "Immich"
118:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/immich.png"
119:         subtitle: "Photos"
120:         url: "https://potatostack.tale-iwato.ts.net:2283"
121:         target: "_blank"
122: 
123:       - name: "Filebrowser"
124:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/filebrowser.png"
125:         subtitle: "File Manager"
126:         url: "https://potatostack.tale-iwato.ts.net:8086"
127:         target: "_blank"
128: 
129:       - name: "Filestash"
130:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/filestash.png"
131:         subtitle: "Advanced File Manager"
132:         url: "https://potatostack.tale-iwato.ts.net:8095"
133:         target: "_blank"
134: 
135:       - name: "Filestash Admin"
136:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/filestash.png"
137:         subtitle: "Filestash Admin Panel"
138:         url: "https://potatostack.tale-iwato.ts.net:8095/admin"
139:         target: "_blank"
140: 
141:       - name: "FileBrowser (Public)"
142:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/filebrowser.png"
143:         subtitle: "File Sharing via Cloudflare"
144:         url: "https://holes-gratis-chrome-examinations.trycloudflare.com"
145:         target: "_blank"
146: 
147:       - name: "Syncthing"
148:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/syncthing.png"
149:         subtitle: "File Sync"
150:         url: "https://potatostack.tale-iwato.ts.net:8384"
151:         target: "_blank"
152: 
153:       - name: "Backrest"
154:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/restic.png"
155:         subtitle: "Backups"
156:         url: "https://potatostack.tale-iwato.ts.net:9898"
157:         target: "_blank"
158: 
159:       - name: "Velld"
160:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/database.png"
161:         subtitle: "Database Backups"
162:         url: "https://potatostack.tale-iwato.ts.net:3010"
163:         target: "_blank"
164: 
165:   - name: "Productivity"
166:     icon: "fas fa-tasks"
167:     items:
168:       - name: "Miniflux"
169:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/miniflux.png"
170:         subtitle: "RSS Reader"
171:         url: "https://potatostack.tale-iwato.ts.net:8093"
172:         target: "_blank"
173: 
174:       - name: "Obsidian Sync"
175:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/obsidian.png"
176:         subtitle: "Notes"
177:         url: "https://potatostack.tale-iwato.ts.net:5984"
178:         target: "_blank"
179: 
180:       - name: "Baïkal"
181:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/baikal.png"
182:         subtitle: "Calendar & Contacts"
183:         url: "https://potatostack.tale-iwato.ts.net:8085"
184:         target: "_blank"
185: 
186:       - name: "SearXNG"
187:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/searxng.png"
188:         subtitle: "Search Engine"
189:         url: "https://potatostack.tale-iwato.ts.net:8180"
190:         target: "_blank"
191: 
192:   - name: "Finance"
193:     icon: "fas fa-chart-pie"
194:     items:
195:       - name: "Actual Budget"
196:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/actual.png"
197:         subtitle: "Budgeting"
198:         url: "https://potatostack.tale-iwato.ts.net:5006"
199:         target: "_blank"
200: 
201:       - name: "Ghostfolio"
202:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/ghostfolio.png"
203:         subtitle: "Portfolio Tracker"
204:         url: "https://potatostack.tale-iwato.ts.net:3335"
205:         target: "_blank"
206: 
207:       - name: "Freqtrade"
208:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/freqtrade.png"
209:         subtitle: "Algo Trading"
210:         url: "https://potatostack.tale-iwato.ts.net:8181"
211:         target: "_blank"
212: 
213:   - name: "Home & Infra"
214:     icon: "fas fa-home"
215:     items:
216:       - name: "Gluetun"
217:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/gluetun.png"
218:         subtitle: "VPN Gateway"
219:         url: "https://potatostack.tale-iwato.ts.net:8008"
220:         target: "_blank"
221: 
222:       - name: "ntfy"
223:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/ntfy.png"
224:         subtitle: "Notifications"
225:         url: "https://potatostack.tale-iwato.ts.net:8060"
226:         target: "_blank"
227: 
228:       - name: "Atuin"
229:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/atuin.png"
230:         subtitle: "Shell History"
231:         url: "https://potatostack.tale-iwato.ts.net:8889"
232:         target: "_blank"
233: 
234:   - name: "Monitoring"
235:     icon: "fas fa-chart-line"
236:     items:
237:       - name: "Grafana"
238:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/grafana.png"
239:         subtitle: "Dashboards"
240:         url: "https://potatostack.tale-iwato.ts.net:3002"
241:         target: "_blank"
242: 
243:       - name: "Prometheus"
244:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/prometheus.png"
245:         subtitle: "Metrics"
246:         url: "https://potatostack.tale-iwato.ts.net:9090"
247:         target: "_blank"
248: 
249:       - name: "Healthchecks"
250:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/healthchecks.png"
251:         subtitle: "Cron Monitoring"
252:         url: "https://potatostack.tale-iwato.ts.net:8001"
253:         target: "_blank"
254: 
255:       - name: "Loki"
256:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/loki.png"
257:         subtitle: "Logs"
258:         url: "https://potatostack.tale-iwato.ts.net:3100"
259:         target: "_blank"
260: 
261:       - name: "Alertmanager"
262:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/alertmanager.png"
263:         subtitle: "Alerts"
264:         url: "https://potatostack.tale-iwato.ts.net:9093"
265:         target: "_blank"
266: 
267:       - name: "Scrutiny"
268:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/scrutiny.png"
269:         subtitle: "Disk Health"
270:         url: "https://potatostack.tale-iwato.ts.net:8087"
271:         target: "_blank"
272: 
273:   - name: "Security & Dev"
274:     icon: "fas fa-shield-alt"
275:     items:
276:       - name: "Vaultwarden"
277:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/bitwarden.png"
278:         subtitle: "Passwords"
279:         url: "https://potatostack.tale-iwato.ts.net:8888"
280:         target: "_blank"
281: 
282:       - name: "Gitea"
283:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/gitea.png"
284:         subtitle: "Git Hosting"
285:         url: "https://potatostack.tale-iwato.ts.net:3004"
286:         target: "_blank"
287: 
288:   - name: "Utilities"
289:     icon: "fas fa-tools"
290:     items:
291:       - name: "Rustypaste"
292:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/rustypaste.png"
293:         subtitle: "Pastebin"
294:         url: "https://potatostack.tale-iwato.ts.net:8788"
295:         target: "_blank"
296: 
297:       - name: "PairDrop"
298:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/pairdrop.png"
299:         subtitle: "P2P File Sharing"
300:         url: "https://potatostack.tale-iwato.ts.net:3013"
301:         target: "_blank"
302: 
303:       - name: "WUD"
304:         logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/whats-up-docker.png"
305:         subtitle: "Docker Updates"
306:         url: "https://potatostack.tale-iwato.ts.net:3000"
307:         target: "_blank"

(End of file - total 307 lines)
</content>
```

**Tool: read**

**Input:**
```json
{
  "filePath": "/home/daniel/potatostack/.env.example"
}
```

**Output:**
```
<path>/home/daniel/potatostack/.env.example</path>
<type>file</type>
<content>1: ################################################################################
2: # PotatoStack Main Environment Configuration
3: # Target: Mini PC with 16GB RAM, 4+ core CPU, 1GB ethernet
4: # Copy this file to .env and fill in your actual values
5: # NEVER commit the .env file to git - it contains sensitive passwords!
6: ################################################################################
7: 
8: ################################################################################
9: # NETWORK CONFIGURATION
10: ################################################################################
11: # Network interface/IP to bind service ports
12: # Use 0.0.0.0 to bind on all interfaces (required for Tailscale access)
13: # Use specific IP (e.g., 192.168.178.40) to bind only to LAN
14: HOST_BIND=0.0.0.0
15: # Local network subnet for VPN firewall
16: LAN_NETWORK=192.168.178.0/24
17: # Domain for services (can be local or public domain)
18: HOST_DOMAIN=local.domain
19: # Email for Let's Encrypt SSL certificates
20: ACME_EMAIL=admin@example.com
21: # Cloudflare DNS-01 challenge credentials (for HTTPS behind firewall)
22: # Create API token at: https://dash.cloudflare.com/profile/api-tokens
23: # Token needs: Zone:DNS:Edit permissions for your domain
24: CF_API_EMAIL=admin@example.com
25: CF_DNS_API_TOKEN=your_cloudflare_dns_api_token_here
26: 
27: ################################################################################
28: # FILE SHARING (Samba)
29: ################################################################################
30: # Username for Samba access
31: SAMBA_USER=potato
32: # Password for Samba access (change this!)
33: SAMBA_PASSWORD=changeme
34: 
35: ################################################################################
36: # Gitea
37: ################################################################################
38: # Gitea SSH port (host side)
39: GITEA_SSH_PORT=2223
40: 
41: ################################################################################
42: # SSH (OpenSSH Server)
43: ################################################################################
44: # SSH username (non-root user created in container)
45: OPENSSH_USER=sshuser
46: # SSH password (required if PASSWORD_ACCESS is true)
47: OPENSSH_PASSWORD=change_this_ssh_password
48: # Enable password login (set to false when using keys only)
49: OPENSSH_PASSWORD_ACCESS=true
50: # Allow sudo for the SSH user (set to true if needed)
51: OPENSSH_SUDO_ACCESS=false
52: # SSH server port (host side)
53: OPENSSH_PORT=2222
54: # Optional public key sources (recommended over passwords)
55: OPENSSH_PUBLIC_KEY=
56: OPENSSH_PUBLIC_KEY_URL=
57: OPENSSH_PUBLIC_KEY_FILE=
58: OPENSSH_PUBLIC_KEY_DIR=
59: # Log SSH output to container stdout (useful for Docker logs)
60: OPENSSH_LOG_STDOUT=true
61: 
62: ################################################################################
63: # CORE DATABASES
64: ################################################################################
65: # PostgreSQL superuser password
66: POSTGRES_SUPER_PASSWORD=change_this_postgres_super_password
67: # Comma-separated list of databases to create
68: # NOTE: The init script creates these on postgres startup
69: POSTGRES_DATABASES=gitea,woodpecker,immich,calibre,karakeep,healthchecks,atuin,homarr,miniflux,grafana,infisical,baikal
70: 
71: # Database for Grafana (uses superuser access)
72: # Grafana will create its own tables on startup
73: 
74: # MongoDB
75: MONGO_ROOT_PASSWORD=change_this_mongo_root_password
76: 
77: # Redis (shared instance) - no password needed for internal use
78: 
79: ################################################################################
80: # VPN CONFIGURATION (Gluetun + Tailscale + WireGuard)
81: ################################################################################
82: # Gluetun VPN - For torrent/download clients
83: VPN_PROVIDER=surfshark
84: VPN_TYPE=wireguard
85: WIREGUARD_PRIVATE_KEY=your_wireguard_private_key
86: WIREGUARD_ADDRESSES=10.64.222.21/16
87: VPN_COUNTRY=Germany
88: VPN_LOG_LEVEL=info
89: VPN_INPUT_PORTS=51413,50000,6888
90: VPN_DNS=1.1.1.1
91: 
92: # Gluetun Monitor - VPN connection monitoring interval (in seconds)
93: # Checks VPN status and auto-restarts dependent containers on reconnect
94: GLUETUN_CHECK_INTERVAL=10
95: GLUETUN_RESTART_ON_STOP=true
96: GLUETUN_RESTART_ON_FAILURE=true
97: GLUETUN_RESTART_COOLDOWN=120
98: 
99: # Disk space monitor
100: DISK_MONITOR_PATHS=/mnt/storage /mnt/ssd /mnt/cachehdd
101: DISK_MONITOR_INTERVAL=300
102: DISK_MONITOR_WARN=80
103: DISK_MONITOR_CRIT=90
104: 
105: # Backup monitor
106: BACKUP_MONITOR_PATHS=/mnt/storage/kopia/stack-snapshot.log /mnt/storage/velld/backups
107: BACKUP_MAX_AGE_HOURS=48
108: BACKUP_MONITOR_INTERVAL=3600
109: 
110: # DB health monitor
111: DB_MONITOR_INTERVAL=30
112: DB_FAIL_THRESHOLD=3
113: DB_RESTART_COOLDOWN=180
114: DB_RESTART_ON_FAILURE=true
115: DB_CHECK_POSTGRES=true
116: DB_CHECK_REDIS=true
117: DB_CHECK_MONGO=false
118: 
119: # Tailscale connectivity monitor
120: TAILSCALE_PING_TARGET=
121: TAILSCALE_PING_INTERVAL=60
122: TAILSCALE_PING_FAIL_THRESHOLD=3
123: TAILSCALE_RESTART_COOLDOWN=300
124: 
125: # Internet connectivity monitor
126: INTERNET_CHECK_INTERVAL=30
127: INTERNET_FAIL_THRESHOLD=3
128: INTERNET_CHECK_TIMEOUT=5
129: INTERNET_CHECK_URLS=https://1.1.1.1 https://www.google.com/generate_204 https://cloudflare.com/cdn-cgi/trace
130: 
131: # Tailscale - Mesh VPN for remote access (RECOMMENDED - easiest)
132: # Get key from: https://login.tailscale.com/admin/settings/keys
133: TAILSCALE_AUTHKEY=tskey-auth-your_tailscale_auth_key_here
134: # Tailscale HTTPS (tailscale serve) - wrap HTTP ports with TLS on your tailnet domain
135: # Comma-separated ports to expose as https://<tailscale-host>:PORT
136: TAILSCALE_SERVE_PORTS=7575,8088,3001,3002,8089,8096,5055,8989,7878,8686,9696,6767,8787,13378,8945,8282,6880,6800,2234,8097,8000,2283,8090,8095,8080,8384,3004,3006,9000,9091,8093,5006,9090,3100,9093,10903,10902,8094,8087,6060,8091,8001,8788,8888,8889,8081,3010,8085,8060,8288,5656,5984,8180,8181,3335
137: TAILSCALE_SERVE_INTERVAL=300
138: 
139: ################################################################################
140: # NOTIFICATIONS (ntfy)
141: ################################################################################
142: NTFY_AUTH_DEFAULT_ACCESS=read-write
143: NTFY_ENABLE_LOGIN=false
144: NTFY_ENABLE_METRICS=true
145: NTFY_TOPIC=potatostack
146: NTFY_TOPIC_CRITICAL=potatostack-critical
147: NTFY_TOPIC_WARNING=potatostack-warning
148: NTFY_TOPIC_INFO=potatostack-info
149: NTFY_INTERNAL_URL=http://ntfy:80
150: NTFY_TOKEN=
151: NTFY_DEFAULT_TAGS=potatostack,monitor
152: NTFY_DEFAULT_PRIORITY=default
153: NTFY_RETRY_COUNT=3
154: NTFY_RETRY_DELAY=5
155: NTFY_TIMEOUT=10
156: 
157: # ntfy webhook bridges
158: JELLYFIN_NTFY_PORT=8081
159: JELLYSEERR_NTFY_PORT=8082
160: MINIFLUX_NTFY_PORT=8083
161: 
162: ################################################################################
163: # SECRETS MANAGEMENT (Infisical - DISABLED)
164: # INFISICAL_ENCRYPTION_KEY=change_this_infisical_encryption_key
165: # Base64 secret. Generate: openssl rand -base64 32
166: # INFISICAL_AUTH_SECRET=change_this_infisical_auth_secret
167: 
168: ################################################################################
169: # IMMICH LOG MONITOR
170: ################################################################################
171: IMMICH_LOG_CHECK_INTERVAL=60
172: IMMICH_RESTART_COOLDOWN=300
173: IMMICH_REACHABILITY_TIMEOUT=120
174: IMMICH_REACHABILITY_RETRIES=6
175: IMMICH_LOG_PATTERNS=redis|Redis|ECONNREFUSED|Connection refused|connect ECONNREFUSED|socket hang up
176: IMMICH_NOTIFY_COOLDOWN=300
177: 
178: # WireGuard Server - For VPS gateway or advanced users
179: # SERVERURL: Your public IP or DynDNS (e.g., fritz.duckdns.org)
180: # Leave as 'auto' to auto-detect public IP
181: WIREGUARD_SERVERURL=auto
182: WIREGUARD_SERVERPORT=51820
183: # PEERS: Comma-separated list of peer names (each gets a config)
184: WIREGUARD_PEERS=vps,android,laptop,tablet,raspberry
185: WIREGUARD_PEERDNS=auto
186: WIREGUARD_INTERNAL_SUBNET=10.13.13.0/24
187: WIREGUARD_ALLOWEDIPS=0.0.0.0/0
188: WIREGUARD_LOG_CONFS=true
189: 
190: ################################################################################
191: # REVERSE PROXY & SSL
192: ################################################################################
193: # Traefik is main proxy on ports 80/443
194: # Nginx Proxy Manager is alternative on ports 8081/4443/81
195: 
196: ################################################################################
197: # AUTHENTICATION & SECURITY
198: ################################################################################
199: # Authentik SSO - Generate with: openssl rand -base64 32
200: # NOTE: Authentik 2025.10+ no longer requires Redis
201: AUTHENTIK_DB_PASSWORD=change_this_authentik_db_password
202: AUTHENTIK_SECRET_KEY=change_this_authentik_secret_key_64_chars_min
203: 
204: # Vaultwarden Password Manager - Generate with: openssl rand -base64 32
205: VAULTWARDEN_ADMIN_TOKEN=change_this_to_a_secure_random_token
206: VAULTWARDEN_SIGNUPS_ALLOWED=false
207: VAULTWARDEN_INVITATIONS_ALLOWED=true
208: 
209: # OAuth2-Proxy (OIDC / Authentik integration)
210: # NOTE: COOKIE_SECRET must be 16 or 32 bytes, base64-encoded
211: OAUTH2_PROXY_OIDC_ISSUER_URL=https://auth.local.domain/application/o/your-provider/
212: OAUTH2_PROXY_CLIENT_ID=change_this_oauth2_proxy_client_id
213: OAUTH2_PROXY_CLIENT_SECRET=change_this_oauth2_proxy_client_secret
214: OAUTH2_PROXY_COOKIE_SECRET=change_this_oauth2_proxy_cookie_secret
215: OAUTH2_PROXY_REDIRECT_URL=https://auth.local.domain/oauth2/callback
216: OAUTH2_PROXY_EMAIL_DOMAINS=*
217: OAUTH2_PROXY_COOKIE_DOMAINS=.local.domain
218: OAUTH2_PROXY_WHITELIST_DOMAINS=.local.domain
219: 
220: ################################################################################
221: # DASHBOARD & MANAGEMENT
222: ################################################################################
223: # Homarr - Generate with: openssl rand -hex 32
224: HOMARR_SECRET_KEY=change_this_homarr_secret_key_64_chars
225: HOMARR_LOG_LEVEL=info
226: HOMARR_DOCKER_HOSTNAMES=socket-proxy
227: HOMARR_DOCKER_PORTS=2375
228: SOCKET_PROXY_TAG=latest
229: 
230: ################################################################################
231: # CLOUD STORAGE & FILE SYNC
232: ################################################################################
233: # Nextcloud All-in-One (SOTA 2025)
234: # Includes Collabora Office, Talk, Whiteboard, and more
235: # No separate database needed - managed by AIO
236: # Access AIO interface at http://HOST_BIND:8080
237: # Nextcloud at http://HOST_BIND:8443
238: 
239: ################################################################################
240: # KNOWLEDGE MANAGEMENT
241: ################################################################################
242: # CouchDB - For Obsidian LiveSync (SOTA 2025)
243: COUCHDB_USER=obsidian
244: COUCHDB_PASSWORD=change_this_couchdb_password
245: COUCHDB_DATABASE=obsidian-vault
246: # After setup, configure Obsidian plugin:
247: # Server: https://obsidian.HOST_DOMAIN (via Traefik)
248: # Database: obsidian-vault
249: # Username: obsidian
250: # Password: [COUCHDB_PASSWORD]
251: 
252: ################################################################################
253: # FINANCE
254: ################################################################################
255: # Firefly III - Generate APP_KEY with: php artisan key:generate
256: FIREFLY_DB_PASSWORD=change_this_firefly_db_password
257: FIREFLY_APP_KEY=base64:your_firefly_app_key_here
258: FIREFLY_ACCESS_TOKEN=get_from_firefly_after_setup
259: 
260: # Optional: GoCardless / Nordigen for bank sync
261: NORDIGEN_ID=
262: NORDIGEN_KEY=
263: 
264: # Optional: Spectre for bank sync
265: SPECTRE_APP_ID=
266: SPECTRE_SECRET=
267: 
268: # Freqtrade - Adaptive Regime Strategy
269: FREQTRADE_STRATEGY_PATH=./.related_deps/freqtrader-strategy
270: FREQTRADE_KRAKEN_API_KEY=
271: FREQTRADE_KRAKEN_API_SECRET=
272: FREQTRADE_FRED_API_KEY=
273: FREQTRADE_COINMETRICS_API_KEY=
274: FREQTRADE_SOSOVALUE_API_KEY=
275: FREQTRADE_COINGLASS_API_KEY=
276: FREQTRADE_NTFY_TOPIC=freqtrade-regime
277: FREQTRADE_REGIME_UPDATE_INTERVAL_SEC=300
278: FREQTRADE_REGIME_LAZINESS_WINDOW=5
279: 
280: # Ghostfolio - Wealth Management & Portfolio Tracker
281: # Generate secrets: openssl rand -hex 32
282: GHOSTFOLIO_ACCESS_TOKEN_SALT=change_this_ghostfolio_salt
283: GHOSTFOLIO_JWT_SECRET_KEY=change_this_ghostfolio_jwt_secret
284: 
285: ################################################################################
286: # MEDIA - ARR STACK
287: ################################################################################
288: # All *arr apps use the same structure, no passwords needed for internal use
289: 
290: ################################################################################
291: # DOWNLOAD CLIENTS (behind VPN)
292: ################################################################################
293: # qBittorrent Torrent Client
294: # Automatically downloads to /mnt/cachehdd/qbittorrent-incomplete
295: # and moves completed files to /mnt/storage/downloads
296: QBITTORRENT_USER=admin
297: QBITTORRENT_PASSWORD=change_this_qbittorrent_password
298: 
299: # pyLoad-ng - HTTP/FTP download manager + debrid support
300: # Real-Debrid: auto-configured as account plugin (get key: https://real-debrid.com/apitoken)
301: # Torbox: pending pyload plugin (https://github.com/pyload/pyload/issues/4578)
302: PYLOAD_USER=pyload
303: PYLOAD_PASSWORD=change_this_pyload_password
304: PYLOAD_ENABLE_NTFY_HOOKS=true
305: REALDEBRID_API_KEY=your_real_debrid_api_key
306: TORBOX_API_KEY=your_torbox_api_key
307: 
308: ################################################################################
309: # Aria2 Download Manager
310: # Advanced download manager with BT/HTTP/FTP/Metalink support
311: # Downloads to /mnt/storage/aria2-downloads
312: # Incomplete files cached at /mnt/cachehdd/aria2-incomplete
313: # WebUI: http://HOST_BIND:6880 (AriaNg)
314: # Configure RPC secret in AriaNg settings
315: ARIA2_RPC_SECRET=change_this_aria2_rpc_secret
316: 
317: # slskd - Soulseek P2P client
318: # Web UI credentials
319: SLSKD_USER=admin
320: SLSKD_PASSWORD=change_this_slskd_password
321: # Soulseek network credentials (your actual Soulseek account)
322: SLSKD_SOULSEEK_USERNAME=your_soulseek_username
323: SLSKD_SOULSEEK_PASSWORD=your_soulseek_password
324: # Get API key from slskd WebUI: Settings > Application > API Key
325: # Leave empty on first run, then get from UI after setup
326: SLSKD_API_KEY=
327: SLSKD_UPLOAD_SLOTS=4
328: SLSKD_UPLOAD_SPEED_LIMIT=25
329: SLSKD_DOWNLOAD_SLOTS=500
330: SLSKD_DOWNLOAD_SPEED_LIMIT=1000
331: SLSKD_QUEUE_FILES=500
332: SLSKD_QUEUE_MEGABYTES=5000
333: SLSKD_QUEUE_CHECK_INTERVAL=60
334: SLSKD_QUEUE_WARN_PERCENT=80
335: SLSKD_NOTIFY_LIMIT=5
336: SLSKD_GROUP_UPLOAD_SLOTS=4
337: SLSKD_GROUP_UPLOAD_SPEED_LIMIT=25
338: SLSKD_GROUP_QUEUE_FILES=150
339: SLSKD_GROUP_QUEUE_MEGABYTES=1500
340: SLSKD_LOGGER_DISK=true
341: SLSKD_LOGGER_NO_COLOR=true
342: SLSKD_LOGGER_LOKI=null
343: SLSKD_METRICS_ENABLED=true
344: SLSKD_METRICS_URL=/metrics
345: SLSKD_METRICS_AUTH_DISABLED=true
346: SLSKD_METRICS_USERNAME=slskd
347: SLSKD_METRICS_PASSWORD=change_this_slskd_metrics_password
348: 
349: ################################################################################
350: # MONITORING & OBSERVABILITY
351: ################################################################################
352: # Grafana
353: GRAFANA_USER=admin
354: GRAFANA_PASSWORD=change_this_grafana_password
355: GRAFANA_PLUGINS=
356: 
357: # Prometheus retention
358: PROMETHEUS_RETENTION_DAYS=30d
359: 
360: # Parseable - Log analytics (Loki alternative)
361: PARSEABLE_USERNAME=admin
362: PARSEABLE_PASSWORD=change_this_parseable_password
363: PARSEABLE_ADDR=0.0.0.0:8000
364: PARSEABLE_FS_DIR=/data
365: 
366: # Scrutiny - HDD SMART monitoring
367: # Add more devices in docker-compose.yml if needed
368: SCRUTINY_DEVICE_1=/dev/sda
369: 
370: # Fritzbox Exporter (optional)
371: FRITZ_USERNAME=
372: FRITZ_PASSWORD=your_fritzbox_admin_password
373: FRITZ_HOSTNAME=fritz.box
374: 
375: ################################################################################
376: # AUTOMATION & WORKFLOWS
377: ################################################################################
378: # News Pipeline - Keyword alerts from Miniflux
379: # Override default keyword regex (optional)
380: # KEYWORD_PATTERN=(cs2|counter-strike|hltv|bielefeld|owl|paderborn|\bnw\b|westfalen)
381: 
382: # Huginn - Generate with: openssl rand -hex 32
383: HUGINN_SECRET_TOKEN=change_this_huginn_secret_token
384: HUGINN_INVITATION_CODE=change_this_invitation_code
385: 
386: # Healthchecks
387: HEALTHCHECKS_ADMIN_EMAIL=admin@example.com
388: HEALTHCHECKS_ADMIN_PASSWORD=change_this_healthchecks_password
389: HEALTHCHECKS_SECRET_KEY=change_this_healthchecks_secret_key
390: 
391: ################################################################################
392: # DOCUMENT MANAGEMENT
393: ################################################################################
394: # Paperless-ngx - Generate with: openssl rand -base64 32
395: PAPERLESS_SECRET_KEY=change_this_paperless_secret_key
396: PAPERLESS_ADMIN_PASSWORD=change_this_paperless_admin_password
397: 
398: ################################################################################
399: # UTILITIES & TOOLS
400: ################################################################################
401: # Karakeep - AI-powered bookmark manager (replaces Linkding)
402: # Generate secrets with: openssl rand -base64 32
403: KARAKEEP_NEXTAUTH_SECRET=change_this_karakeep_nextauth_secret
404: KARAKEEP_MEILI_MASTER_KEY=change_this_karakeep_meili_master_key
405: # Optional: OpenAI API key for AI tagging (leave empty to disable)
406: KARAKEEP_OPENAI_API_KEY=
407: # Optional: Ollama base URL for local AI tagging (e.g., http://host.docker.internal:11434)
408: KARAKEEP_OLLAMA_BASE_URL=
409: # Set to true to disable new user signups after initial setup
410: KARAKEEP_DISABLE_SIGNUPS=false
411: 
412: # Cal.com - Generate with: openssl rand -base64 32
413: CALCOM_NEXTAUTH_SECRET=change_this_calcom_nextauth_secret
414: CALCOM_ENCRYPTION_KEY=change_this_calcom_encryption_key
415: 
416: # Atuin - Shell history sync (uses shared postgres)
417: ATUIN_HOST=0.0.0.0
418: ATUIN_PORT=8888
419: 
420: # Filebrowser - Web-based file manager
421: FILEBROWSER_USER=admin
422: FILEBROWSER_PASSWORD=change_this_filebrowser_password
423: 
424: # Filestash - Advanced web file manager with multi-protocol support
425: # Create OAuth2 application in Authentik:
426: # - Name: Filestash
427: # - Provider: OAuth2/OIDC
428: # - Redirect URI: https://filestash.HOST_DOMAIN/login
429: # - Scopes: openid, profile, email
430: FILESTASH_OIDC_CLIENT_ID=get_from_authentik_after_setup
431: FILESTASH_OIDC_CLIENT_SECRET=get_from_authentik_after_setup
432: 
433: # Code Server
434: CODE_SERVER_PASSWORD=change_this_code_server_password
435: CODE_SERVER_SUDO_PASSWORD=change_this_code_server_sudo_password
436: 
437: ################################################################################
438: # DEVELOPMENT & GIT
439: ################################################################################
440: # Gitea
441: GITEA_TAG=latest
442: GITEA_RUNNER_TOKEN=get_from_gitea_after_setup
443: 
444: # Woodpecker CI (Gitea integration)
445: WOODPECKER_GITEA_CLIENT=get_from_gitea_oauth_app
446: WOODPECKER_GITEA_SECRET=get_from_gitea_oauth_app
447: WOODPECKER_AGENT_SECRET=change_this_woodpecker_agent_secret
448: WOODPECKER_ADMIN=your_gitea_username
449: WOODPECKER_MAX_WORKFLOWS=2
450: 
451: # Calibre-web - Ebook management (optional, uses shared postgres)
452: # Access: http://HOST_BIND:8083
453: # Note: Calibre-web uses SQLite by default, configure for PostgreSQL if needed
454: CALIBRE_DB_PASSWORD=change_this_calibre_db_password
455: 
456: # Drone CI
457: DRONE_GITEA_CLIENT_ID=get_from_gitea_oauth_app
458: DRONE_GITEA_CLIENT_SECRET=get_from_gitea_oauth_app
459: DRONE_RPC_SECRET=change_this_drone_rpc_secret
460: DRONE_ADMIN_USER=your_gitea_username
461: 
462: # Sentry
463: SENTRY_DB_PASSWORD=change_this_sentry_db_password
464: SENTRY_SECRET_KEY=change_this_sentry_secret_key
465: 
466: ################################################################################
467: # AI & SPECIAL APPLICATIONS
468: ################################################################################
469: # Open WebUI - For local LLM interaction
470: OLLAMA_BASE_URL=http://host.docker.internal:11434
471: OPEN_WEBUI_SECRET_KEY=change_this_open_webui_secret_key
472: 
473: # OctoBot - Crypto trading bot (configure in web UI after first start)
474: 
475: ################################################################################
476: # ELASTICSEARCH STACK
477: ################################################################################
478: ELASTIC_PASSWORD=change_this_elastic_password
479: 
480: ################################################################################
481: # BACKUPS
482: ################################################################################
483: # Kopia - Fast and secure backups (see docs/KOPIA-BACKUP-GUIDE.md)
484: # CRITICAL: Store KOPIA_PASSWORD safely - if lost, backups are unrecoverable!
485: KOPIA_PASSWORD=change_this_kopia_password
486: KOPIA_SERVER_USER=admin
487: KOPIA_SERVER_PASSWORD=change_this_kopia_server_password
488: KOPIA_HOSTNAME=potatostack
489: 
490: # Kopia snapshot scheduler (cron)
491: SNAPSHOT_CRON_SCHEDULE=0 3 * * *
492: SNAPSHOT_PATHS=/data
493: SNAPSHOT_LOG_FILE=/mnt/storage/kopia/stack-snapshot.log
494: 
495: # Velld - Database backup scheduler
496: # NOTE: ENCRYPTION_KEY must be a 64-character hex string (openssl rand -hex 32)
497: VELLD_API_URL=http://192.168.178.40:8085
498: VELLD_JWT_SECRET=change_this_velld_jwt_secret
499: VELLD_ENCRYPTION_KEY=change_this_velld_encryption_key
500: VELLD_ADMIN_USERNAME=admin
501: VELLD_ADMIN_PASSWORD=change_this_velld_admin_password
502: VELLD_ALLOW_REGISTER=false
503: 
504: ################################################################################
505: # FILE SYNC & SHARING
506: ################################################################################
507: # Syncthing - P2P file synchronization
508: # Hostname for Syncthing instance (visible to other devices)
509: SYNCTHING_HOSTNAME=potatostack-sync
510: # Get API key from Syncthing WebUI: Settings > General > API Key
511: # Leave empty on first run, then get from UI after setup
512: SYNCTHING_API_KEY=
513: 
514: # Portainer - Container management
515: # Get API key from Portainer: User Settings > Access tokens
516: # Leave empty on first run, then get from UI after setup
517: PORTAINER_API_KEY=
518: 
519: ################################################################################
520: # ATUIN (Shell history sync)
521: ################################################################################
522: # Allow open registration (set true for initial setup, then false after registering)
523: ATUIN_OPEN_REGISTRATION=true
524: 
525: ################################################################################
526: # SYSTEM UTILITIES
527: ################################################################################
528: # Diun - Docker Image Update Notifier (SOTA 2025 best practice)
529: # Monitors for updates but doesn't auto-update (safer than Watchtower)
530: # Notification options (optional - choose one or more):
531: 
532: # Gotify
533: DIUN_GOTIFY_ENDPOINT=
534: DIUN_GOTIFY_TOKEN=
535: 
536: # Discord
537: DIUN_DISCORD_WEBHOOK=
538: 
539: # Telegram
540: DIUN_TELEGRAM_TOKEN=
541: DIUN_TELEGRAM_CHATIDS=
542: 
543: ################################################################################
544: # BAIKAL - Calendar & Contacts (CalDAV/CardDAV)
545: ################################################################################
546: BAIKAL_TAG=latest
547: 
548: ################################################################################
549: # IMAGE TAGS
550: # Databases pinned for data compatibility, apps set to latest
551: ################################################################################
552: # Core (databases pinned)
553: ALPINE_TAG=latest
554: POSTGRES_TAG=16-alpine
555: PGBOUNCER_TAG=latest
556: MONGO_TAG=8
557: REDIS_TAG=alpine
558: ADMINER_TAG=latest
559: 
560: # Authentication
561: AUTHENTIK_TAG=latest
562: VAULTWARDEN_TAG=latest
563: OAUTH2_PROXY_TAG=latest
564: 
565: # Security
566: CROWDSEC_TAG=latest
567: FAIL2BAN_TAG=latest
568: TRIVY_TAG=latest
569: 
570: # VPN
571: GLUETUN_TAG=latest
572: TAILSCALE_TAG=latest
573: 
574: # Cloud Storage
575: # NEXTCLOUD_AIO_TAG=latest
576: SYNCTHING_TAG=latest
577: FILEBROWSER_TAG=latest
578: FILESTASH_TAG=latest
579: CLOUDFLARED_TAG=latest
580: 
581: # Knowledge Management
582: COUCHDB_TAG=latest
583: OBSIDIAN_LIVESYNC_TAG=latest
584: CURL_TAG=latest
585: 
586: # Finance
587: FIREFLY_TAG=latest
588: FIREFLY_IMPORTER_TAG=latest
589: ACTUAL_TAG=latest
590: GHOSTFOLIO_TAG=latest
591: 
592: # Media Management
593: PROWLARR_TAG=latest
594: SONARR_TAG=latest
595: RADARR_TAG=latest
596: LIDARR_TAG=latest
597: READARR_TAG=develop
598: BAZARR_TAG=latest
599: MAINTAINERR_TAG=latest
600: BOOKSHELF_TAG=hardcover
601: JELLYFIN_TAG=latest
602: JELLYSEERR_TAG=latest
603: OVERSEERR_TAG=latest
604: AUDIOBOOKSHELF_TAG=latest
605: BOOKLOGR_TAG=v1.5.0
606: # BookLogr authentication (change these!)
607: BOOKLOGR_AUTH_SECRET_KEY=change_this_to_random_string_for_booklogr
608: BOOKLOGR_ALLOW_REGISTRATION=false
609: BOOKLOGR_REQUIRE_VERIFICATION=false
610: BOOKLOGR_GOOGLE_CLIENT_ID=
611: BOOKLOGR_GOOGLE_CLIENT_SECRET=
612: 
613: # Downloads
614: QBITTORRENT_TAG=latest
615: ARIA2_TAG=latest
616: ARIANG_TAG=latest
617: 
618: # Photos
619: IMMICH_TAG=release
620: 
621: # Music
622: NAVIDROME_TAG=latest
623: 
624: # Adult Media
625: STASH_TAG=latest
626: 
627: # Monitoring
628: PROMETHEUS_TAG=latest
629: GRAFANA_TAG=latest
630: LOKI_TAG=latest
631: PROMTAIL_TAG=latest
632: ALLOY_TAG=latest
633: NODE_EXPORTER_TAG=latest
634: CADVISOR_TAG=latest
635: FRITZBOX_EXPORTER_TAG=latest
636: NETDATA_TAG=latest
637: UPTIME_KUMA_TAG=latest
638: PARSEABLE_TAG=latest
639: SCRUTINY_TAG=latest
640: ALERTMANAGER_TAG=latest
641: 
642: # Exporters
643: POSTGRES_EXPORTER_TAG=latest
644: REDIS_EXPORTER_TAG=latest
645: MONGODB_EXPORTER_TAG=0.43
646: SMARTCTL_EXPORTER_TAG=latest
647: 
648: # Automation
649: HUGINN_TAG=latest
650: HEALTHCHECKS_TAG=latest
651: 
652: # Utilities
653: SAMBA_TAG=latest
654: OPENSSH_SERVER_TAG=latest
655: NTFY_TAG=latest
656: RUSTYPASTE_TAG=latest
657: PAIRDROP_TAG=latest
658: PAIRDROP_RATE_LIMIT=false
659: PAIRDROP_WS_FALLBACK=false
660: PAIRDROP_RTC_CONFIG=
661: PAIRDROP_DEBUG_MODE=false
662: STIRLING_PDF_TAG=latest
663: # Karakeep
664: KARAKEEP_TAG=release
665: KARAKEEP_CHROME_TAG=123
666: KARAKEEP_MEILISEARCH_TAG=v1.13.3
667: CALCOM_TAG=latest
668: CODE_SERVER_TAG=latest
669: DRAWIO_TAG=latest
670: EXCALIDRAW_TAG=latest
671: ATUIN_TAG=latest
672: DUCKDB_TAG=latest
673: IT_TOOLS_TAG=latest
674: 
675: # Development
676: GITEA_TAG=latest
677: GITEA_RUNNER_TAG=latest
678: WOODPECKER_TAG=latest
679: WOODPECKER_AGENT_TAG=latest
680: DRONE_TAG=latest
681: DRONE_RUNNER_TAG=latest
682: SENTRY_TAG=latest
683: 
684: # Search
685: SEARXNG_TAG=latest
686: 
687: # AI & Special
688: OPEN_WEBUI_TAG=latest
689: OCTOBOT_TAG=latest
690: PINCHFLAT_TAG=latest
691: 
692: # Elasticsearch (must match versions)
693: ELASTICSEARCH_TAG=8
694: KIBANA_TAG=8
695: LOGSTASH_TAG=8
696: 
697: # Dashboard
698: GLANCE_TAG=latest
699: HOMARR_TAG=latest
700: 
701: # System
702: WUD_TAG=latest
703: AUTOHEAL_TAG=latest
704: PORTAINER_TAG=latest
705: DOCKER_CLI_TAG=latest
706: DOCKER_TAG=cli
707: 
708: # Secrets
709: # INFISICAL_TAG=latest
710: 
711: # VPN Tags
712: WIREGUARD_TAG=latest
713: 
714: # Downloads & P2P
715: SLSKD_TAG=latest
716: UNPACKERR_TAG=latest
717: 
718: # Backups
719: KOPIA_TAG=latest
720: VELLD_API_TAG=latest
721: VELLD_WEB_TAG=latest
722: 
723: ################################################################################
724: # Notes:
725: # - Copy this file to .env and fill in real values
726: # - Generate strong passwords/secrets for all services
727: # - Update HOST_BIND to your server's IP
728: # - Configure VPN credentials for services behind Gluetun
729: # - Set up Tailscale for secure remote access
730: # - Configure domain and SSL for production use
731: #
732: # Quick generation commands:
733: # - Password: openssl rand -base64 32
734: # - Secret key: openssl rand -hex 64
735: # - API key: openssl rand -hex 32
736: #
737: # Service URLs (after setup):
738: # - Grafana: http://grafana.HOST_DOMAIN
739: # - Prometheus: http://HOST_BIND:9090
740: # - Jellyfin: http://jellyfin.HOST_DOMAIN
741: # - Immich: http://immich.HOST_DOMAIN
742: # - Nextcloud AIO: http://HOST_BIND:8443
743: # - Gitea: http://git.HOST_DOMAIN
744: # - Cal.com: http://cal.HOST_DOMAIN
745: # - Authentik: http://authentik.HOST_DOMAIN:9000
746: ################################################################################
747: 
748: # RSS & News Aggregation
749: MINIFLUX_TAG=latest
750: MINIFLUX_API_KEY=change_this_miniflux_api_key
751: MINIFLUX_ADMIN_USER=admin
752: MINIFLUX_ADMIN_PASSWORD=changeme
753: 
754: # Search
755: SEARXNG_SECRET_KEY=change_this_searxng_secret_key
756: 
757: # Monitoring & Observability (tags defined above in IMAGE TAGS section)
758: GRAFANA_ADMIN_USER=admin
759: GRAFANA_ADMIN_PASSWORD=changeme
760: NETDATA_CLAIM_TOKEN=
761: NETDATA_CLAIM_ROOMS=
762: # Thanos Long-term Storage
763: THANOS_TAG=latest
764: 
765: 
766: # Gluetun Monitor Service Type
767: # host: Runs as systemd host service (reliable, handles auto-restart)
768: # container: Runs inside Docker container (DinD network issues)
769: GLUETUN_MONITOR_TYPE=host

(End of file - total 769 lines)
</content>
```

---

## Assistant (Plan · glm-5-free · 11.7s)

_Thinking:_

Now I have a comprehensive view of the stack. Let me also check the navidrome and slskd sections more specifically in the docker-compose, and check the tailscale-serve-https script to understand the port configuration better. Let me also search for any existing music-related configuration.

Let me check the navidrome and slskd service definitions more closely, and see the tailscale-serve ports:

**Tool: read**

**Input:**
```json
{
  "filePath": "/home/daniel/potatostack/docker-compose.yml",
  "offset": 1584,
  "limit": 1500
}
```

**Output:**
```
<path>/home/daniel/potatostack/docker-compose.yml</path>
<type>file</type>
<content>1584:   #   image: ghcr.io/onedr0p/exportarr:${EXPORTARR_TAG:-latest}
1585:   #   container_name: exportarr-sonarr
1586:   #   logging: *default-logging
1587:   #   command: ["sonarr"]
1588:   #   environment:
1589:   #     PORT: "9707"
1590:   #     URL: http://gluetun:8989
1591:   #     APIKEY: ${SONARR_API_KEY:-}
1592:   #     ENABLE_ADDITIONAL_METRICS: "true"
1593:   #     ENABLE_UNKNOWN_QUEUE_ITEMS: "true"
1594:   #   ports:
1595:   #     - "${HOST_BIND:-127.0.0.1}:9707:9707"
1596:   #   networks:
1597:   #     - potatostack
1598:   #   depends_on:
1599:   #     - sonarr
1600:   #   restart: unless-stopped
1601:   #   deploy:
1602:   #     resources:
1603:   #       limits:
1604:   #         cpus: "0.1"
1605:   #         memory: 64M
1606: 
1607:   # exportarr-radarr:
1608:   #   image: ghcr.io/onedr0p/exportarr:${EXPORTARR_TAG:-latest}
1609:   #   container_name: exportarr-radarr
1610:   #   logging: *default-logging
1611:   #   command: ["radarr"]
1612:   #   environment:
1613:   #     PORT: "9708"
1614:   #     URL: http://gluetun:7878
1615:   #     APIKEY: ${RADARR_API_KEY:-}
1616:   #     ENABLE_ADDITIONAL_METRICS: "true"
1617:   #     ENABLE_UNKNOWN_QUEUE_ITEMS: "true"
1618:   #   ports:
1619:   #     - "${HOST_BIND:-127.0.0.1}:9708:9708"
1620:   #   networks:
1621:   #     - potatostack
1622:   #   depends_on:
1623:   #     - radarr
1624:   #   restart: unless-stopped
1625:   #   deploy:
1626:   #     resources:
1627:   #       limits:
1628:   #         cpus: "0.1"
1629:   #         memory: 64M
1630: 
1631:   # exportarr-lidarr:
1632:   #   image: ghcr.io/onedr0p/exportarr:${EXPORTARR_TAG:-latest}
1633:   #   container_name: exportarr-lidarr
1634:   #   logging: *default-logging
1635:   #   command: ["lidarr"]
1636:   #   environment:
1637:   #     PORT: "9709"
1638:   #     URL: http://gluetun:8686
1639:   #     APIKEY: ${LIDARR_API_KEY:-}
1640:   #     ENABLE_ADDITIONAL_METRICS: "true"
1641:   #     ENABLE_UNKNOWN_QUEUE_ITEMS: "true"
1642:   #   ports:
1643:   #     - "${HOST_BIND:-127.0.0.1}:9709:9709"
1644:   #   networks:
1645:   #     - potatostack
1646:   #   depends_on:
1647:   #     - lidarr
1648:   #   restart: unless-stopped
1649:   #   deploy:
1650:   #     resources:
1651:   #       limits:
1652:   #         cpus: "0.1"
1653:   #         memory: 64M
1654: 
1655:   # exportarr-prowlarr:
1656:   #   image: ghcr.io/onedr0p/exportarr:${EXPORTARR_TAG:-latest}
1657:   #   container_name: exportarr-prowlarr
1658:   #   logging: *default-logging
1659:   #   command: ["prowlarr"]
1660:   #   environment:
1661:   #     PORT: "9710"
1662:   #     URL: http://gluetun:9696
1663:   #     APIKEY: ${PROWLARR_API_KEY:-}
1664:   #     ENABLE_ADDITIONAL_METRICS: "true"
1665:   #   ports:
1666:   #     - "${HOST_BIND:-127.0.0.1}:9710:9710"
1667:   #   networks:
1668:   #     - potatostack
1669:   #   depends_on:
1670:   #     - prowlarr
1671:   #   restart: unless-stopped
1672:   #   deploy:
1673:   #     resources:
1674:   #       limits:
1675:   #         cpus: "0.1"
1676:   #         memory: 64M
1677:   ################################################################################
1678:   # Notifiarr - Unified notifications for *arr stack (DISABLED)
1679:   ################################################################################
1680:   # notifiarr:
1681:   #   image: golift/notifiarr:${NOTIFIARR_TAG:-latest}
1682:   #   container_name: notifiarr
1683:   #   logging: *default-logging
1684:   #   hostname: potatostack
1685:   #   ports:
1686:   #     - "${HOST_BIND:-127.0.0.1}:5454:5454"
1687:   #   environment:
1688:   #     <<: *common-env
1689:   #     DN_API_KEY: ${NOTIFIARR_API_KEY:-}
1690:   #     DN_UI_PASSWORD: ${NOTIFIARR_UI_PASSWORD:-}
1691:   #     # Sonarr
1692:   #     DN_SONARR_0_URL: http://gluetun:8989
1693:   #     DN_SONARR_0_API_KEY: ${SONARR_API_KEY:-}
1694:   #     # Radarr
1695:   #     DN_RADARR_0_URL: http://gluetun:7878
1696:   #     DN_RADARR_0_API_KEY: ${RADARR_API_KEY:-}
1697:   #     # Lidarr
1698:   #     DN_LIDARR_0_URL: http://gluetun:8686
1699:   #     DN_LIDARR_0_API_KEY: ${LIDARR_API_KEY:-}
1700:   #     # Prowlarr
1701:   #     DN_PROWLARR_0_URL: http://gluetun:9696
1702:   #     DN_PROWLARR_0_API_KEY: ${PROWLARR_API_KEY:-}
1703:   #   volumes:
1704:   #     - /mnt/ssd/docker-data/notifiarr:/config
1705:   #     - /var/run/utmp:/var/run/utmp:ro
1706:   #   networks:
1707:   #     - potatostack
1708:   #   depends_on:
1709:   #     - sonarr
1710:   #     - radarr
1711:   #   restart: unless-stopped
1712:   #   healthcheck:
1713:   #     test: ["CMD", "wget", "-q", "--spider", "http://127.0.0.1:5454/"]
1714:   #     interval: 60s
1715:   #     timeout: 10s
1716:   #     retries: 3
1717:   #     start_period: 30s
1718:   #   deploy:
1719:   #     resources:
1720:   #       limits:
1721:   #         cpus: "0.25"
1722:   #         memory: 192M
1723:   #       reservations:
1724:   #         cpus: "0.05"
1725:   #         memory: 64M
1726: 
1727:   # Exportarr - Prometheus metrics exporter for *arr stack (DISABLED)
1728:   # exportarr:
1729:   #   image: ghcr.io/onedr0p/exportarr:${EXPORTARR_TAG:-latest}
1730:   #   container_name: exportarr-sonarr
1731:   #   logging: *default-logging
1732:   #   command: ["sonarr"]
1733:   #   environment:
1734:   #     PORT: "9707"
1735:   #     URL: http://gluetun:8989
1736:   #     APIKEY: ${SONARR_API_KEY:-}
1737:   #     ENABLE_ADDITIONAL_METRICS: "true"
1738:   #     ENABLE_UNKNOWN_QUEUE_ITEMS: "true"
1739:   #   ports:
1740:   #     - "${HOST_BIND:-127.0.0.1}:9707:9707"
1741:   #   networks:
1742:   #     - potatostack
1743:   #   depends_on:
1744:   #     - sonarr
1745:   #   restart: unless-stopped
1746:   #   deploy:
1747:   #     resources:
1748:   #       limits:
1749:   #         cpus: "0.1"
1750:   #         memory: 64M
1751: 
1752:   # exportarr-radarr:
1753:   #   image: ghcr.io/onedr0p/exportarr:${EXPORTARR_TAG:-latest}
1754:   #   container_name: exportarr-radarr
1755:   #   logging: *default-logging
1756:   #   command: ["radarr"]
1757:   #   environment:
1758:   #     PORT: "9708"
1759:   #     URL: http://gluetun:7878
1760:   #     APIKEY: ${RADARR_API_KEY:-}
1761:   #     ENABLE_ADDITIONAL_METRICS: "true"
1762:   #     ENABLE_UNKNOWN_QUEUE_ITEMS: "true"
1763:   #   ports:
1764:   #     - "${HOST_BIND:-127.0.0.1}:9708:9708"
1765:   #   networks:
1766:   #     - potatostack
1767:   #   depends_on:
1768:   #     - radarr
1769:   #   restart: unless-stopped
1770:   #   deploy:
1771:   #     resources:
1772:   #       limits:
1773:   #         cpus: "0.1"
1774:   #         memory: 64M
1775: 
1776:   # exportarr-lidarr:
1777:   #   image: ghcr.io/onedr0p/exportarr:${EXPORTARR_TAG:-latest}
1778:   #   container_name: exportarr-lidarr
1779:   #   logging: *default-logging
1780:   #   command: ["lidarr"]
1781:   #   environment:
1782:   #     PORT: "9709"
1783:   #     URL: http://gluetun:8686
1784:   #     APIKEY: ${LIDARR_API_KEY:-}
1785:   #     ENABLE_ADDITIONAL_METRICS: "true"
1786:   #     ENABLE_UNKNOWN_QUEUE_ITEMS: "true"
1787:   #   ports:
1788:   #     - "${HOST_BIND:-127.0.0.1}:9709:9709"
1789:   #   networks:
1790:   #     - potatostack
1791:   #   depends_on:
1792:   #     - lidarr
1793:   #   restart: unless-stopped
1794:   #   deploy:
1795:   #     resources:
1796:   #       limits:
1797:   #         cpus: "0.1"
1798:   #         memory: 64M
1799: 
1800:   # exportarr-prowlarr:
1801:   #   image: ghcr.io/onedr0p/exportarr:${EXPORTARR_TAG:-latest}
1802:   #   container_name: exportarr-prowlarr
1803:   #   logging: *default-logging
1804:   #   command: ["prowlarr"]
1805:   #   environment:
1806:   #     PORT: "9710"
1807:   #     URL: http://gluetun:9696
1808:   #     APIKEY: ${PROWLARR_API_KEY:-}
1809:   #     ENABLE_ADDITIONAL_METRICS: "true"
1810:   #   ports:
1811:   #     - "${HOST_BIND:-127.0.0.1}:9710:9710"
1812:   #   networks:
1813:   #     - potatostack
1814:   #   depends_on:
1815:   #     - prowlarr
1816:   #   restart: unless-stopped
1817:   #   deploy:
1818:   #     resources:
1819:   #       limits:
1820:   #         cpus: "0.1"
1821:   #         memory: 64M
1822: 
1823:   ################################################################################
1824:   # MEDIA SERVERS & REQUEST MANAGEMENT
1825:   ################################################################################
1826: 
1827:   # Jellyfin - Media server (SOTA 2025 with HW acceleration support)
1828:   jellyfin:
1829:     image: lscr.io/linuxserver/jellyfin:${JELLYFIN_TAG:-latest}
1830:     container_name: jellyfin
1831:     logging: *default-logging
1832:     ports:
1833:       - "127.0.0.1:8096:8096"          # Tailscale serve
1834:       - "192.168.178.158:8096:8096"    # LAN (Android TV)
1835:       - "127.0.0.1:8920:8920"          # Tailscale HTTPS serve
1836:       - "192.168.178.158:8920:8920"    # LAN HTTPS
1837:       - "192.168.178.158:7359:7359/udp"  # Service discovery (Android TV)
1838:       - "192.168.178.158:1900:1900/udp"  # SSDP/DLNA discovery
1839:     environment:
1840:       <<: *common-env
1841:     devices:
1842:       - /dev/dri/renderD128:/dev/dri/renderD128  # Intel QSV/VAAPI
1843:       - /dev/dri/card0:/dev/dri/card0
1844:     group_add:
1845:       - "109"  # render group (getent group render | cut -d: -f3)
1846:     volumes:
1847:       - jellyfin-config:/config
1848:       - /mnt/storage2/media/tv:/data/tvshows
1849:       - /mnt/storage2/media/movies:/data/movies
1850:       - /mnt/storage/media/tv:/data/tvshows2
1851:       - /mnt/storage/media/movies:/data/movies2
1852:       - /mnt/storage/media/music:/data/music
1853:       - /mnt/storage/media/audiobooks:/data/audiobooks
1854:       - /mnt/storage/cache/jellyfin:/cache
1855:     tmpfs:
1856:       - /tmp:exec,mode=1777,size=512m
1857:       - /transcode:exec,mode=1777,size=4g
1858:     networks:
1859:       - potatostack
1860:     depends_on:
1861:       - storage-init
1862:     restart: unless-stopped
1863:     healthcheck:
1864:       test: ["CMD-SHELL", "curl -f http://127.0.0.1:8096/health || exit 1"]
1865:       interval: 60s
1866:       timeout: 10s
1867:       retries: 3
1868:       start_period: 60s
1869:     deploy:
1870:       resources:
1871:         limits:
1872:           cpus: "3.0"   # 3/4 cores; QSV offloads transcode so CPU is rarely saturated
1873:           memory: 4G    # library cache + image cache + concurrent streams; tmpfs adds ~4.5GB on top
1874:         reservations:
1875:           cpus: "0.5"
1876:           memory: 512M
1877:     labels:
1878:       - "wud.trigger.docker.update=true"
1879: 
1880:   # Jellyfin webhook to ntfy bridge
1881:   jellyfin-webhook:
1882:     image: python:3.12-alpine
1883:     container_name: jellyfin-webhook
1884:     logging: *default-logging
1885:     command: ["python", "/app/jellyfin-webhook.py"]
1886:     environment:
1887:       <<: *common-env
1888:       JELLYFIN_NTFY_PORT: 8081
1889:       NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
1890:       NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
1891:       NTFY_TOKEN: ${NTFY_TOKEN:-}
1892:     volumes:
1893:       - ./scripts/webhooks/jellyfin-webhook.py:/app/jellyfin-webhook.py:ro
1894:     networks:
1895:       - potatostack
1896:     depends_on:
1897:       - ntfy
1898:     restart: unless-stopped
1899:     deploy:
1900:       resources:
1901:         limits:
1902:           cpus: "0.1"
1903:           memory: 64M
1904:         reservations:
1905:           cpus: "0.05"
1906:           memory: 32M
1907: 
1908:   # Audiobookshelf - Audiobook and podcast server
1909:   audiobookshelf:
1910:     image: ghcr.io/advplyr/audiobookshelf:${AUDIOBOOKSHELF_TAG:-latest}
1911:     container_name: audiobookshelf
1912:     logging: *default-logging
1913:     ports:
1914:       - "${HOST_BIND:-127.0.0.1}:13378:80"
1915:     environment:
1916:       <<: *common-env
1917:     volumes:
1918:       - audiobookshelf-config:/config
1919:       - /mnt/storage/cache/audiobookshelf/metadata:/metadata
1920:       - /mnt/storage/media/audiobooks:/audiobooks
1921:       - /mnt/storage/media/podcasts:/podcasts
1922:     networks:
1923:       - potatostack
1924:     depends_on:
1925:       - storage-init
1926:     labels:
1927:       - "wud.trigger.docker.update=true"
1928:     restart: unless-stopped
1929:     deploy:
1930:       resources:
1931:         limits:
1932:           cpus: "1.0"
1933:           memory: 256M
1934: 
1935:   # Navidrome - Music streaming server (Subsonic/Airsonic compatible)
1936:   navidrome:
1937:     image: deluan/navidrome:${NAVIDROME_TAG:-latest}
1938:     container_name: navidrome
1939:     logging: *default-logging
1940:     ports:
1941:       - "${HOST_BIND:-127.0.0.1}:4533:4533"
1942:     environment:
1943:       <<: *common-env
1944:       ND_SCANSCHEDULE: 1h
1945:       ND_LOGLEVEL: info
1946:       ND_SESSIONTIMEOUT: 24h
1947:       ND_BASEURL: ""
1948:       ND_ENABLETRANSCODINGCONFIG: "true"
1949:       ND_ENABLESHARING: "true"
1950:     volumes:
1951:       - /mnt/ssd/docker-data/navidrome:/data
1952:       - /mnt/storage/media/music:/music:ro
1953:     networks:
1954:       - potatostack
1955:     depends_on:
1956:       - storage-init
1957:     labels:
1958:       - "wud.trigger.docker.update=true"
1959:     restart: unless-stopped
1960:     healthcheck:
1961:       test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:4533/ping || exit 1"]
1962:       interval: 30s
1963:       timeout: 10s
1964:       retries: 3
1965:     deploy:
1966:       resources:
1967:         limits:
1968:           cpus: "1.0"
1969:           memory: 128M
1970:         reservations:
1971:           cpus: "0.25"
1972:           memory: 64M
1973: 
1974:   # SpotiFLAC - Spotify to FLAC downloader (Tidal/Qobuz/Amazon sources)
1975:   spotiflac:
1976:     image: neiht/spotiflac:latest@sha256:65de2009536579cda0311b2d9606faa70bfc5bc9962f96f8ad150e5e82b3da23
1977:     container_name: spotiflac
1978:     logging: *default-logging
1979:     network_mode: "service:gluetun"
1980:     environment:
1981:       <<: *common-env
1982:       SERVER_PORT: 8080
1983:       SERVER_HOST: 0.0.0.0
1984:       DOWNLOAD_DIR: /downloads
1985:       STATIC_DIR: /app/dist
1986:     volumes:
1987:       - /mnt/storage2/downloads:/downloads
1988:     depends_on:
1989:       gluetun:
1990:         condition: service_healthy
1991:         restart: true
1992:       storage-init:
1993:         condition: service_completed_successfully
1994:     restart: unless-stopped
1995:     healthcheck:
1996:       test: ["CMD-SHELL", "pgrep -f spotiflac || exit 1"]
1997:       interval: 60s
1998:       timeout: 10s
1999:       retries: 3
2000:       start_period: 30s
2001:     deploy:
2002:       resources:
2003:         limits:
2004:           cpus: "0.25"
2005:           memory: 128M
2006:         reservations:
2007:           cpus: "0.1"
2008:           memory: 128M
2009:     labels:
2010:       - "potatostack.alerts=critical"
2011:       - "autoheal=true"
2012:       - "wud.trigger.docker.update=true"
2013: 
2014:   ################################################################################
2015:   # DOWNLOAD CLIENTS (behind VPN)
2016:   ################################################################################
2017: 
2018:   # qBittorrent - Torrent client
2019:   qbittorrent:
2020:     image: lscr.io/linuxserver/qbittorrent:${QBITTORRENT_TAG:-latest}
2021:     container_name: qbittorrent
2022:     logging: *default-logging
2023:     network_mode: "service:gluetun"
2024:     environment:
2025:       <<: *common-env
2026:       WEBUI_PORT: 8282
2027:       QBITTORRENT_USER: ${QBITTORRENT_USER:-daniel}
2028:       QBITTORRENT_PASSWORD: ${QBITTORRENT_PASSWORD:-}
2029:       NTFY_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
2030:       NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
2031:       NTFY_TOKEN: ${NTFY_TOKEN:-}
2032:     volumes:
2033:       - qbittorrent-config:/config
2034:       - /mnt/storage2/downloads/torrents:/downloads
2035:       - /mnt/storage2/downloads/incomplete/qbittorrent:/incomplete
2036:       - /mnt/storage2/media:/media
2037:       - /home/daniel/potatostack/scripts/init/qbittorrent-init.sh:/custom-cont-init.d/99-qb-init.sh:ro
2038:       - /home/daniel/potatostack/scripts/hooks/qbittorrent-post-torrent.sh:/hooks/post-torrent.sh:ro
2039:     depends_on:
2040:       gluetun:
2041:         condition: service_healthy
2042:         restart: true
2043:       storage-init:
2044:         condition: service_completed_successfully
2045:     restart: unless-stopped
2046:     healthcheck:
2047:       test: ["CMD-SHELL", "curl -f http://127.0.0.1:8282/ || exit 1"]
2048:       interval: 60s
2049:       timeout: 10s
2050:       retries: 3
2051:       start_period: 60s
2052:     deploy:
2053:       resources:
2054:         limits:
2055:           cpus: "2.0"
2056:           memory: 2G
2057:         reservations:
2058:           cpus: "1.0"
2059:           memory: 1G
2060:     labels:
2061:       - "autoheal=true"
2062:       - "wud.trigger.docker.update=true"
2063: 
2064:   # Aria2 - High-performance download client (RPC on port 6800)
2065:   aria2:
2066:     image: p3terx/aria2-pro:${ARIA2_TAG:-latest}
2067:     container_name: aria2
2068:     logging: *default-logging
2069:     network_mode: "service:gluetun"
2070:     entrypoint: sh /aria2-init.sh
2071:     environment:
2072:       <<: *common-env
2073:       RPC_PORT: 6800
2074:       LISTEN_PORT: 6888
2075:       DISK_CACHE: 64M
2076:       IPV6_MODE: "false"
2077:     volumes:
2078:       - aria2-config:/config
2079:       - /mnt/storage2/downloads/aria2:/downloads
2080:       - /mnt/storage2/downloads/incomplete/aria2:/incomplete
2081:       - /home/daniel/potatostack/scripts/init/aria2-init.sh:/aria2-init.sh:ro
2082:       - shared-keys:/keys:ro
2083:     depends_on:
2084:       gluetun:
2085:         condition: service_healthy
2086:         restart: true
2087:       storage-init:
2088:         condition: service_completed_successfully
2089:     restart: unless-stopped
2090:     healthcheck:
2091:       test: ["CMD-SHELL", "pgrep -x aria2c || exit 1"]
2092:       interval: 60s
2093:       timeout: 10s
2094:       retries: 3
2095:       start_period: 60s
2096:     deploy:
2097:       resources:
2098:         limits:
2099:           cpus: "0.25"
2100:           memory: 128M
2101:         reservations:
2102:           cpus: "0.1"
2103:           memory: 128M
2104:     labels:
2105:       - "autoheal=true"
2106: 
2107:   slskd:
2108:     image: ghcr.io/slskd/slskd:${SLSKD_TAG:-latest}
2109:     container_name: slskd
2110:     logging: *default-logging
2111:     network_mode: "service:gluetun"
2112:     entrypoint: sh /init.sh
2113:     environment:
2114:       <<: *common-env
2115:       SLSKD_HTTP_PORT: 2234
2116:       SLSKD_SLSK_LISTEN_PORT: 50000
2117:       SLSKD_USERNAME: ${SLSKD_USER:-admin}
2118:       SLSKD_PASSWORD: ${SLSKD_PASSWORD}
2119:       SLSKD_SOULSEEK_USERNAME: ${SLSKD_SOULSEEK_USERNAME}
2120:       SLSKD_SOULSEEK_PASSWORD: ${SLSKD_SOULSEEK_PASSWORD}
2121:       SLSKD_METRICS: ${SLSKD_METRICS_ENABLED:-true}
2122:       SLSKD_API_KEY: ${SLSKD_API_KEY}
2123: 
2124:       SLSKD_UPLOAD_SLOTS: ${SLSKD_UPLOAD_SLOTS:-4}
2125:       SLSKD_UPLOAD_SPEED_LIMIT: ${SLSKD_UPLOAD_SPEED_LIMIT:-25}
2126:       SLSKD_DOWNLOAD_SLOTS: ${SLSKD_DOWNLOAD_SLOTS:-500}
2127:       SLSKD_DOWNLOAD_SPEED_LIMIT: ${SLSKD_DOWNLOAD_SPEED_LIMIT:-1000}
2128:       SLSKD_QUEUE_FILES: ${SLSKD_QUEUE_FILES:-500}
2129:       SLSKD_QUEUE_MEGABYTES: ${SLSKD_QUEUE_MEGABYTES:-5000}
2130:       SLSKD_GROUP_UPLOAD_SLOTS: ${SLSKD_GROUP_UPLOAD_SLOTS:-4}
2131:       SLSKD_GROUP_UPLOAD_SPEED_LIMIT: ${SLSKD_GROUP_UPLOAD_SPEED_LIMIT:-25}
2132:       SLSKD_GROUP_QUEUE_FILES: ${SLSKD_GROUP_QUEUE_FILES:-150}
2133:       SLSKD_GROUP_QUEUE_MEGABYTES: ${SLSKD_GROUP_QUEUE_MEGABYTES:-1500}
2134:       SLSKD_LOGGER_DISK: ${SLSKD_LOGGER_DISK:-true}
2135:       SLSKD_LOGGER_NO_COLOR: ${SLSKD_LOGGER_NO_COLOR:-true}
2136:       SLSKD_LOGGER_LOKI: ${SLSKD_LOGGER_LOKI:-null}
2137:       SLSKD_METRICS_ENABLED: ${SLSKD_METRICS_ENABLED:-true}
2138:       SLSKD_METRICS_URL: ${SLSKD_METRICS_URL:-/metrics}
2139:       SLSKD_METRICS_AUTH_DISABLED: ${SLSKD_METRICS_AUTH_DISABLED:-true}
2140:       SLSKD_METRICS_USERNAME: ${SLSKD_METRICS_USERNAME:-slskd}
2141:       SLSKD_METRICS_PASSWORD: ${SLSKD_METRICS_PASSWORD:-slskd}
2142:     volumes:
2143:       - slskd-config:/app
2144:       - /mnt/ssd/docker-data/slskd/logs:/app/logs
2145:       - /mnt/storage2/downloads/slskd:/var/slskd/downloads
2146:       - /mnt/storage2/downloads/incomplete/slskd:/var/slskd/incomplete
2147:       - /mnt/storage/slskd-shared:/var/slskd/shared
2148:       - shared-keys:/keys:ro
2149:       - /home/daniel/potatostack/scripts/init/slskd-init.sh:/init.sh:ro
2150:       # Share music and audiobook libraries
2151:       - /mnt/storage/media/music:/music:ro
2152:       - /mnt/storage/media/audiobooks:/audiobooks:ro
2153:     tmpfs:
2154:       - /tmp:exec,mode=1777,size=256m
2155:     depends_on:
2156:       gluetun:
2157:         condition: service_healthy
2158:         restart: true
2159:       storage-init:
2160:         condition: service_completed_successfully
2161:     restart: unless-stopped
2162:     healthcheck:
2163:       test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://127.0.0.1:2234/ || exit 1"]
2164:       interval: 60s
2165:       timeout: 10s
2166:       retries: 3
2167:       start_period: 60s
2168:     deploy:
2169:       resources:
2170:         limits:
2171:           cpus: "1.5"
2172:           memory: 512M
2173:         reservations:
2174:           cpus: "0.5"
2175:           memory: 256M
2176:     labels:
2177:       - "autoheal=true"
2178:       - "potatostack.alerts=critical"
2179:       - "wud.trigger.docker.update=true"
2180: 
2181:   # Soularr - Bridge between Lidarr and Soulseek (slskd) (DISABLED)
2182:   # soularr:
2183:   #   image: mrusse08/soularr:${SOULARR_TAG:-latest}
2184:   #   container_name: soularr
2185:   #   logging: *default-logging
2186:   #   environment:
2187:   #     <<: *common-env
2188:   #     SCRIPT_INTERVAL: 300
2189:   #   volumes:
2190:   #     - /mnt/ssd/docker-data/soularr:/data
2191:   #     - /mnt/storage/downloads/slskd:/downloads/slskd
2192:   #   networks:
2193:   #     - potatostack
2194:   #   depends_on:
2195:   #     - lidarr
2196:   #     - slskd
2197:   #   restart: unless-stopped
2198:   #   deploy:
2199:   #     resources:
2200:   #       limits:
2201:   #         cpus: "0.5"
2202:   #         memory: 256M
2203:   #       reservations:
2204:   #         cpus: "0.25"
2205:   #         memory: 128M
2206: 
2207:   # pyLoad-ng - Web-based download manager (HTTP/FTP/etc)
2208:   # WebUI: 8076 (external) -> 8000 (internal), Click'n'Load: 9666
2209:   pyload:
2210:     image: lscr.io/linuxserver/pyload-ng:latest
2211:     container_name: pyload
2212:     logging: *default-logging
2213:     network_mode: "service:gluetun"
2214:     entrypoint: /pyload-init.sh
2215:     environment:
2216:       <<: *common-env
2217:       PYLOAD_USER: ${PYLOAD_USER:-pyload}
2218:       PYLOAD_PASSWORD: ${PYLOAD_PASSWORD:-}
2219:       PYLOAD_ENABLE_NTFY_HOOKS: ${PYLOAD_ENABLE_NTFY_HOOKS:-true}
2220:       REALDEBRID_API_KEY: ${REALDEBRID_API_KEY:-}
2221:       TORBOX_API_KEY: ${TORBOX_API_KEY:-}
2222:       NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
2223:       NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
2224:       NTFY_TOKEN: ${NTFY_TOKEN:-}
2225:     volumes:
2226:       - pyload-config:/config
2227:       - /mnt/storage2/downloads/pyload:/downloads
2228:       - /mnt/storage2/downloads/incomplete/pyload:/incomplete
2229:       - /home/daniel/potatostack/scripts/init/pyload-init.sh:/pyload-init.sh:ro
2230:     depends_on:
2231:       gluetun:
2232:         condition: service_healthy
2233:         restart: true
2234:       storage-init:
2235:         condition: service_completed_successfully
2236:     restart: unless-stopped
2237:     healthcheck:
2238:       test: ["CMD-SHELL", "curl -f http://127.0.0.1:8000/ || exit 1"]
2239:       interval: 60s
2240:       timeout: 10s
2241:       retries: 3
2242:       start_period: 60s
2243:     deploy:
2244:       resources:
2245:         limits:
2246:           cpus: "0.5"
2247:           memory: 256M
2248:         reservations:
2249:           cpus: "0.1"
2250:           memory: 128M
2251:     labels:
2252:       - "autoheal=true"
2253:       - "wud.trigger.docker.update=true"
2254: 
2255:   # tdl - Telegram downloader toolkit
2256:   tdl:
2257:     image: iyear/tdl:${TDL_TAG:-latest}
2258:     container_name: tdl
2259:     logging: *default-logging
2260:     network_mode: "service:gluetun"
2261:     entrypoint:
2262:       [
2263:         "/bin/sh",
2264:         "-c",
2265:         "crond -b && sh /etc/tdl/download-saved.sh >> /downloads/tdl-download.log 2>&1 & tail -f /dev/null",
2266:       ]
2267:     environment:
2268:       <<: *common-env
2269:     volumes:
2270:       - tdl-data:/root/.tdl
2271:       - /mnt/storage2/downloads/telegram:/downloads
2272:       - /mnt/storage2/media/adult:/adult:ro
2273:       - /mnt/storage2/media/adult/telegram:/adult-telegram
2274:       - /home/daniel/potatostack/config/tdl/crontab:/etc/crontabs/root:ro
2275:       - /home/daniel/potatostack/config/tdl/download-saved.sh:/etc/tdl/download-saved.sh:ro
2276:       - /home/daniel/potatostack/config/tdl/blocklist.txt:/etc/tdl/blocklist.txt:ro
2277:     depends_on:
2278:       gluetun:
2279:         condition: service_healthy
2280:         restart: true
2281:     restart: unless-stopped
2282:     healthcheck:
2283:       test: ["CMD-SHELL", "pgrep crond || exit 1"]
2284:       interval: 60s
2285:       timeout: 10s
2286:       retries: 3
2287:       start_period: 60s
2288:     deploy:
2289:       resources:
2290:         limits:
2291:           cpus: "0.1"
2292:           memory: 64M
2293:         reservations:
2294:           cpus: "0.05"
2295:           memory: 32M
2296:     labels:
2297:       - "wud.trigger.docker.update=true"
2298: 
2299:   ################################################################################
2300:   # PHOTO MANAGEMENT
2301:   ################################################################################
2302: 
2303:   # Immich - Photo management and AI tagging
2304:   immich-server:
2305:     image: ghcr.io/immich-app/immich-server:${IMMICH_TAG:-release}
2306:     container_name: immich-server
2307:     logging: *default-logging
2308:     ports:
2309:       - "${HOST_BIND:-127.0.0.1}:2283:2283"
2310:     environment:
2311:       <<: *common-env
2312:       DB_HOSTNAME: postgres
2313:       DB_USERNAME: postgres
2314:       DB_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
2315:       DB_DATABASE_NAME: immich
2316:       REDIS_HOSTNAME: redis-cache
2317:       REDIS_DBINDEX: 0
2318:       UPLOAD_LOCATION: /usr/src/app/upload
2319:       IMMICH_FFMPEG_ACCEL: qsv
2320:       IMMICH_FFMPEG_PREFERRED_HW_DEVICE: /dev/dri/renderD128
2321:     devices:
2322:       - /dev/dri/renderD128:/dev/dri/renderD128
2323:     volumes:
2324:       - /mnt/storage2/photos:/usr/src/app/upload
2325:       - /etc/localtime:/etc/localtime:ro
2326:     networks:
2327:       - potatostack
2328:     depends_on:
2329:       - postgres
2330:       - redis-cache
2331:       - storage-init
2332:     restart: unless-stopped
2333:     healthcheck:
2334:       test: ["CMD-SHELL", "curl -f http://127.0.0.1:2283/api/server/ping || exit 1"]
2335:       interval: 60s
2336:       timeout: 10s
2337:       retries: 3
2338:       start_period: 60s
2339:     deploy:
2340:       resources:
2341:         limits:
2342:           cpus: "2.0"
2343:           memory: 2G
2344:         reservations:
2345:           cpus: "0.5"
2346:           memory: 512M
2347:     labels:
2348:       - "wud.trigger.docker.update=false"
2349: 
2350:   immich-machine-learning:
2351:     image: ghcr.io/immich-app/immich-machine-learning:${IMMICH_TAG:-release}
2352:     container_name: immich-ml
2353:     logging: *default-logging
2354:     environment:
2355:       <<: *common-env
2356:     volumes:
2357:       - /mnt/storage/cache/immich-ml:/cache
2358:     networks:
2359:       - potatostack
2360:     depends_on:
2361:       immich-server:
2362:         condition: service_healthy
2363:     tmpfs:
2364:       - /tmp:exec,mode=1777,size=500m
2365:     restart: unless-stopped
2366:     healthcheck:
2367:       test: ["CMD-SHELL", 'python3 -c ''import requests; requests.get("http://127.0.0.1:3003/ping")''']
2368:       interval: 60s
2369:       timeout: 10s
2370:       retries: 3
2371:       start_period: 120s
2372:     deploy:
2373:       resources:
2374:         limits:
2375:           cpus: "2.0"
2376:           memory: 2G
2377:         reservations:
2378:           cpus: "0.5"
2379:           memory: 1G
2380:     labels:
2381:       - "wud.trigger.docker.update=false"
2382: 
2383:   # Immich Log Monitor - Restart on error patterns
2384:   immich-log-monitor:
2385:     image: docker:${DOCKER_TAG:-cli}
2386:     container_name: immich-log-monitor
2387:     logging: *default-logging
2388:     entrypoint: sh /immich-log-monitor.sh
2389:     environment:
2390:       IMMICH_CONTAINER: immich-server
2391:       IMMICH_ML_CONTAINER: immich-ml
2392:       CHECK_INTERVAL: ${IMMICH_LOG_CHECK_INTERVAL:-60}
2393:       RESTART_COOLDOWN: ${IMMICH_RESTART_COOLDOWN:-300}
2394:       REACHABILITY_TIMEOUT: ${IMMICH_REACHABILITY_TIMEOUT:-120}
2395:       REACHABILITY_RETRIES: ${IMMICH_REACHABILITY_RETRIES:-6}
2396:       IMMICH_LOG_PATTERNS: ${IMMICH_LOG_PATTERNS:-redis|Redis|ECONNREFUSED|Connection refused|connect ECONNREFUSED|socket hang up}
2397:       IMMICH_NOTIFY_COOLDOWN: ${IMMICH_NOTIFY_COOLDOWN:-300}
2398:       NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
2399:       NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
2400:       NTFY_TOKEN: ${NTFY_TOKEN:-}
2401:       NTFY_DEFAULT_TAGS: ${NTFY_DEFAULT_TAGS:-potatostack,monitor}
2402:       NTFY_DEFAULT_PRIORITY: ${NTFY_DEFAULT_PRIORITY:-default}
2403:     volumes:
2404:       - /var/run/docker.sock:/var/run/docker.sock
2405:       - ./scripts/monitor/immich-log-monitor.sh:/immich-log-monitor.sh:ro
2406:       - ./scripts/monitor/notify.sh:/notify.sh:ro
2407:     depends_on:
2408:       - immich-server
2409:     networks:
2410:       - potatostack
2411:     restart: unless-stopped
2412:     deploy:
2413:       resources:
2414:         limits:
2415:           cpus: "0.1"
2416:           memory: 64M
2417:         reservations:
2418:           cpus: "0.05"
2419:           memory: 32M
2420: 
2421:   ################################################################################
2422:   # MONITORING & OBSERVABILITY
2423:   ################################################################################
2424: 
2425:   # Prometheus - Metrics collection and time-series database (SOTA 2025)
2426:   prometheus:
2427:     image: prom/prometheus:${PROMETHEUS_TAG:-latest}
2428:     container_name: prometheus
2429:     logging: *default-logging
2430:     command:
2431:       - "--config.file=/etc/prometheus/prometheus.yml"
2432:       - "--storage.tsdb.path=/prometheus"
2433:       - "--storage.tsdb.retention.time=7d"
2434:       - "--storage.tsdb.retention.size=5GB"
2435:       - "--storage.tsdb.min-block-duration=2h"
2436:       - "--storage.tsdb.max-block-duration=2h"
2437:       - "--web.console.libraries=/usr/share/prometheus/console_libraries"
2438:       - "--web.console.templates=/usr/share/prometheus/consoles"
2439:       - "--web.enable-lifecycle"
2440:       - "--web.enable-admin-api"
2441:     ports:
2442:       - "${HOST_BIND:-127.0.0.1}:9090:9090"
2443:     environment:
2444:       <<: *common-env
2445:     volumes:
2446:       - ./config/prometheus:/etc/prometheus:ro
2447:       - /mnt/ssd/docker-data/prometheus:/prometheus
2448:     networks:
2449:       - potatostack
2450:     restart: unless-stopped
2451:     healthcheck:
2452:       test: ["CMD", "wget", "--spider", "-q", "http://127.0.0.1:9090/-/healthy"]
2453:       interval: 30s
2454:       timeout: 10s
2455:       retries: 3
2456:     deploy:
2457:       resources:
2458:         limits:
2459:           cpus: "0.5"
2460:           memory: 256M
2461:         reservations:
2462:           cpus: "0.25"
2463:           memory: 128M
2464:     labels:
2465:       - "potatostack.alerts=critical"
2466:       - "wud.trigger.docker.update=true"
2467: 
2468:   # Grafana - Metrics visualization and dashboards (SOTA 2025)
2469:   grafana:
2470:     image: grafana/grafana:${GRAFANA_TAG:-latest}
2471:     container_name: grafana
2472:     logging: *default-logging
2473:     user: "472:472"
2474:     ports:
2475:       - "${HOST_BIND:-127.0.0.1}:3002:3000"
2476:     environment:
2477:       <<: *common-env
2478:       GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
2479:       GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
2480:       GF_PLUGINS_PREINSTALL: grafana-clock-panel
2481:       GF_SERVER_ROOT_URL: https://grafana.${HOST_DOMAIN:-local.domain}
2482:       GF_DATABASE_TYPE: postgres
2483:       GF_DATABASE_HOST: postgres:5432
2484:       GF_DATABASE_NAME: grafana
2485:       GF_DATABASE_USER: postgres
2486:       GF_DATABASE_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
2487:       GF_DATABASE_SSL_MODE: disable
2488:     volumes:
2489:       - /mnt/ssd/docker-data/grafana:/var/lib/grafana
2490:       - ./config/grafana/provisioning:/etc/grafana/provisioning:ro
2491:       - ./config/grafana/dashboards:/var/lib/grafana/dashboards:ro
2492:     networks:
2493:       - potatostack
2494:     depends_on:
2495:       - postgres
2496:       - pgbouncer
2497:       - prometheus
2498:       - loki
2499:     restart: unless-stopped
2500:     healthcheck:
2501:       test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:3000/api/health || exit 1"]
2502:       interval: 60s
2503:       timeout: 10s
2504:       retries: 3
2505:     deploy:
2506:       resources:
2507:         limits:
2508:           cpus: "0.5"
2509:           memory: 256M
2510:         reservations:
2511:           cpus: "0.25"
2512:           memory: 128M
2513:     labels:
2514:       - "wud.trigger.docker.update=true"
2515: 
2516:   # cAdvisor - Docker container metrics (low-resource config)
2517:   # Requires containerd socket for cgroup v2 container enumeration.
2518:   # Key savings vs default: housekeeping_interval=30s (default=1s = 30x less CPU),
2519:   # docker_only skips host cgroups, enable_metrics keeps only cpu/memory/network
2520:   cadvisor:
2521:     image: gcr.io/cadvisor/cadvisor:${CADVISOR_TAG:-latest}
2522:     container_name: cadvisor
2523:     logging: *default-logging
2524:     privileged: true
2525:     devices:
2526:       - /dev/kmsg:/dev/kmsg
2527:     command:
2528:       - "--housekeeping_interval=30s"
2529:       - "--global_housekeeping_interval=30s"
2530:       - "--max_housekeeping_interval=35s"
2531:       - "--allow_dynamic_housekeeping=true"
2532:       - "--docker_only=true"
2533:       - "--store_container_labels=false"
2534:       - "--containerd=/run/containerd/containerd.sock"
2535:       - "--enable_metrics=cpu,memory,network,diskIO"
2536:     volumes:
2537:       - /:/rootfs:ro
2538:       - /var/run/docker.sock:/var/run/docker.sock:ro
2539:       - /run/containerd/containerd.sock:/run/containerd/containerd.sock:ro
2540:       - /sys:/sys:ro
2541:       - /var/lib/docker:/var/lib/docker:ro
2542:     networks:
2543:       - potatostack
2544:     restart: unless-stopped
2545:     deploy:
2546:       resources:
2547:         limits:
2548:           cpus: "0.15"
2549:           memory: 128M
2550:         reservations:
2551:           cpus: "0.05"
2552:           memory: 64M
2553: 
2554:   # Node Exporter - Host metrics for Prometheus (SOTA 2025)
2555:   node-exporter:
2556:     image: prom/node-exporter:${NODE_EXPORTER_TAG:-latest}
2557:     container_name: node-exporter
2558:     logging: *default-logging
2559:     command:
2560:       - "--path.rootfs=/host"
2561:       - "--path.procfs=/host/proc"
2562:       - "--path.sysfs=/host/sys"
2563:       - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|run|var/lib/docker/.+)($|/)"
2564:       - "--collector.systemd"
2565:       - "--collector.processes"
2566:       - "--collector.tcpstat"
2567:       - "--collector.interrupts"
2568:     volumes:
2569:       - /:/host:ro,rslave
2570:       - /run/dbus/system_bus_socket:/var/run/dbus/system_bus_socket:ro
2571:     networks:
2572:       - potatostack
2573:     restart: unless-stopped
2574:     deploy:
2575:       resources:
2576:         limits:
2577:           cpus: "0.25"
2578:           memory: 128M
2579:         reservations:
2580:           cpus: "0.1"
2581:           memory: 64M
2582: 
2583:   # Postgres Exporter - PostgreSQL metrics
2584:   postgres-exporter:
2585:     image: quay.io/prometheuscommunity/postgres-exporter:${POSTGRES_EXPORTER_TAG:-latest}
2586:     container_name: postgres-exporter
2587:     logging: *default-logging
2588:     environment:
2589:       DATA_SOURCE_NAME: postgresql://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/postgres?sslmode=disable
2590:     networks:
2591:       - potatostack
2592:     depends_on:
2593:       - postgres
2594:     restart: unless-stopped
2595:     deploy:
2596:       resources:
2597:         limits:
2598:           cpus: "0.1"
2599:           memory: 64M
2600:         reservations:
2601:           cpus: "0.05"
2602:           memory: 32M
2603: 
2604:   # Redis Exporter - Redis metrics
2605:   redis-exporter:
2606:     image: oliver006/redis_exporter:${REDIS_EXPORTER_TAG:-latest}
2607:     container_name: redis-exporter
2608:     logging: *default-logging
2609:     environment:
2610:       REDIS_ADDR: redis-cache:6379
2611:     networks:
2612:       - potatostack
2613:     depends_on:
2614:       - redis-cache
2615:     restart: unless-stopped
2616:     deploy:
2617:       resources:
2618:         limits:
2619:           cpus: "0.1"
2620:           memory: 64M
2621:         reservations:
2622:           cpus: "0.05"
2623:           memory: 32M
2624: 
2625:   # MongoDB Exporter - MongoDB metrics
2626:   mongodb-exporter:
2627:     image: percona/mongodb_exporter:${MONGODB_EXPORTER_TAG:-0.43}
2628:     container_name: mongodb-exporter
2629:     logging: *default-logging
2630:     environment:
2631:       MONGODB_URI: mongodb://root:${MONGO_ROOT_PASSWORD}@mongo:27017/admin
2632:     networks:
2633:       - potatostack
2634:     depends_on:
2635:       - mongo
2636:     restart: unless-stopped
2637:     deploy:
2638:       resources:
2639:         limits:
2640:           cpus: "0.1"
2641:           memory: 64M
2642:         reservations:
2643:           cpus: "0.05"
2644:           memory: 32M
2645: 
2646:   # Smartctl Exporter - SMART metrics for disk health alerting
2647:   smartctl-exporter:
2648:     image: prometheuscommunity/smartctl-exporter:${SMARTCTL_EXPORTER_TAG:-latest}
2649:     container_name: smartctl-exporter
2650:     logging: *default-logging
2651:     privileged: true
2652:     group_add:
2653:       - disk
2654:     devices:
2655:       - /dev/sda
2656:       - /dev/sda1
2657:       - /dev/sda2
2658:       - /dev/sda3
2659:       - /dev/sdb
2660:       - /dev/sdb1
2661:       - /dev/sdc
2662:       - /dev/sdc1
2663:     command:
2664:       - "--smartctl.path=/usr/sbin/smartctl"
2665:       - "--smartctl.device=/dev/sda"
2666:       - "--smartctl.device=/dev/sdb"
2667:       - "--smartctl.device=/dev/sdc"
2668:     volumes:
2669:       - /dev:/dev
2670:       - /run/udev:/run/udev:ro
2671:     networks:
2672:       - potatostack
2673:     restart: unless-stopped
2674:     deploy:
2675:       resources:
2676:         limits:
2677:           cpus: "0.1"
2678:           memory: 64M
2679:         reservations:
2680:           cpus: "0.05"
2681:           memory: 32M
2682: 
2683:   # Netdata - DISABLED: Use Prometheus + Grafana instead for monitoring
2684:   # netdata:
2685:   #   image: netdata/netdata:${NETDATA_TAG:-latest}
2686:   #   container_name: netdata
2687:   #   logging: *default-logging
2688:   #   hostname: potatostack-main
2689:   #   ports:
2690:   #     - "${HOST_BIND:-127.0.0.1}:19999:19999"
2691:   #   cap_add:
2692:   #     - SYS_PTRACE
2693:   #     - SYS_ADMIN
2694:   #   security_opt:
2695:   #     - apparmor:unconfined
2696:   #   environment:
2697:   #     <<: *common-env
2698:   #     NETDATA_CLAIM_TOKEN: ${NETDATA_CLAIM_TOKEN:-}
2699:   #     NETDATA_CLAIM_ROOMS: ${NETDATA_CLAIM_ROOMS:-}
2700:   #     NETDATA_CLAIM_URL: https://app.netdata.cloud
2701:   #     DOCKER_HOST: /var/run/docker.sock
2702:   #   volumes:
2703:   #     - netdata-config:/etc/netdata
2704:   #     - netdata-lib:/var/lib/netdata
2705:   #     - netdata-cache:/var/cache/netdata
2706:   #     - /etc/passwd:/host/etc/passwd:ro
2707:   #     - /etc/group:/host/etc/group:ro
2708:   #     - /proc:/host/proc:ro
2709:   #     - /sys:/host/sys:ro
2710:   #     - /etc/os-release:/host/etc/os-release:ro
2711:   #     - /var/run/docker.sock:/var/run/docker.sock:ro
2712:   #   networks:
2713:   #     - potatostack
2714:   #   restart: unless-stopped
2715:   #   healthcheck:
2716:   #     test:
2717:   #       ["CMD-SHELL", "curl -f http://127.0.0.1:19999/api/v1/info || exit 1"]
2718:   #     interval: 60s
2719:   #     timeout: 10s
2720:   #     retries: 3
2721:   #   deploy:
2722:   #     resources:
2723:   #       limits:
2724:   #         cpus: "0.5"
2725:   #         memory: 384M
2726:   #       reservations:
2727:   #         cpus: "0.1"
2728:   #         memory: 64M
2729:   #   labels:
2730: 
2731:   # Loki - Log aggregation
2732:   loki:
2733:     image: grafana/loki:${LOKI_TAG:-latest}
2734:     container_name: loki
2735:     logging: *default-logging
2736:     ports:
2737:       - "${HOST_BIND:-127.0.0.1}:3100:3100"
2738:     command: -config.file=/etc/loki/loki.yml -target=all
2739:     volumes:
2740:       - ./config/loki:/etc/loki
2741:       - /mnt/ssd/docker-data/loki:/loki
2742:     networks:
2743:       - potatostack
2744:     restart: unless-stopped
2745:     deploy:
2746:       resources:
2747:         limits:
2748:           cpus: "1.0"
2749:           memory: 384M
2750:     labels:
2751:       - "wud.trigger.docker.update=true"
2752: 
2753:   # Grafana Alloy - Unified collector (Promtail replacement)
2754:   alloy:
2755:     image: grafana/alloy:${ALLOY_TAG:-latest}
2756:     container_name: alloy
2757:     logging: *default-logging
2758:     command: run /etc/alloy/config.alloy --storage.path=/var/lib/alloy
2759:     volumes:
2760:       - ./config/alloy:/etc/alloy
2761:       - /var/log:/var/log:ro
2762:       - /mnt/ssd/docker-data/slskd/logs:/slskd-logs:ro
2763:       - /var/run/docker.sock:/var/run/docker.sock:ro
2764:       - alloy-data:/var/lib/alloy
2765:     networks:
2766:       - potatostack
2767:     depends_on:
2768:       - loki
2769:     labels:
2770:       - "wud.trigger.docker.update=true"
2771:     restart: unless-stopped
2772:     deploy:
2773:       resources:
2774:         limits:
2775:           cpus: "0.25"
2776:           memory: 384M
2777: 
2778:   # Parseable - REMOVED (using Loki instead)
2779:   # parseable:
2780:   #   image: parseable/parseable:${PARSEABLE_TAG:-latest}
2781:   #   container_name: parseable
2782:   #   logging: *default-logging
2783:   #   command: ["parseable", "local-store"]
2784:   #   ports:
2785:   #     - "${HOST_BIND:-127.0.0.1}:8094:8000"
2786:   #   environment:
2787:   #     <<: *common-env
2788:   #     P_USERNAME: ${PARSEABLE_USERNAME:-admin}
2789:   #     P_PASSWORD: ${PARSEABLE_PASSWORD:-admin}
2790:   #     P_ADDR: ${PARSEABLE_ADDR:-0.0.0.0:8000}
2791:   #     P_FS_DIR: ${PARSEABLE_FS_DIR:-/data}
2792:   #   volumes:
2793:   #     - /mnt/ssd/docker-data/parseable:/data
2794:   #   networks:
2795:   #     - potatostack
2796:   #   depends_on:
2797:   #     - storage-init
2798:   #   restart: unless-stopped
2799:   #   deploy:
2800:   #     resources:
2801:   #       limits:
2802:   #         cpus: "0.5"
2803:   #         memory: 256M
2804:   #   labels:
2805: 
2806:   # Scrutiny - HDD SMART monitoring
2807:   scrutiny:
2808:     image: ghcr.io/analogj/scrutiny:${SCRUTINY_TAG:-master-omnibus}
2809:     container_name: scrutiny
2810:     logging: *default-logging
2811:     cap_add:
2812:       - SYS_RAWIO
2813:       - SYS_ADMIN
2814:     ports:
2815:       - "${HOST_BIND:-127.0.0.1}:8087:8080"
2816:     volumes:
2817:       - /run/udev:/run/udev:ro
2818:       - /mnt/ssd/docker-data/scrutiny/config:/opt/scrutiny/config
2819:       - /mnt/ssd/docker-data/scrutiny/influxdb:/opt/scrutiny/influxdb
2820:     devices:
2821:       - "${SCRUTINY_DEVICE_1:-/dev/sda}"
2822:     networks:
2823:       - potatostack
2824:     depends_on:
2825:       - storage-init
2826:     labels:
2827:       - "wud.trigger.docker.update=true"
2828:     restart: unless-stopped
2829:     deploy:
2830:       resources:
2831:         limits:
2832:           cpus: "0.25"
2833:           memory: 192M
2834: 
2835:   # Uptime Kuma - Uptime monitoring with Docker container support (DISABLED)
2836:   # uptime-kuma:
2837:   #   image: louislam/uptime-kuma:${UPTIME_KUMA_TAG:-latest}
2838:   #   container_name: uptime-kuma
2839:   #   logging: *default-logging
2840:   #   ports:
2841:   #     - "${HOST_BIND:-127.0.0.1}:3001:3001"
2842:   #   environment:
2843:   #     <<: *common-env
2844:   #     # Docker socket access via socket-proxy for container monitoring
2845:   #     # NOTE: When creating Docker monitors in UI, set Docker Host to: tcp://socket-proxy:2375
2846:   #     DOCKER_HOST: tcp://socket-proxy:2375
2847:   #   volumes:
2848:   #     - /mnt/ssd/docker-data/uptime-kuma:/app/data
2849:   #     # Mount docker.sock for Docker monitors (socket-proxy env var may not work for all features)
2850:   #     - /var/run/docker.sock:/var/run/docker.sock:ro
2851:   #   group_add:
2852:   #     - "987"  # docker group for socket access
2853:   #   networks:
2854:   #     - potatostack
2855:   #   depends_on:
2856:   #     - socket-proxy
2857:   #   restart: unless-stopped
2858:   #   healthcheck:
2859:   #     test: ["CMD-SHELL", "curl -f http://127.0.0.1:3001 || exit 1"]
2860:   #     interval: 60s
2861:   #     timeout: 10s
2862:   #     retries: 3
2863:   #     start_period: 30s
2864:   #   deploy:
2865:   #     resources:
2866:   #       limits:
2867:   #         cpus: "0.25"
2868:   #         memory: 192M
2869: 
2870:   ################################################################################
2871:   # AUTOMATION & WORKFLOWS
2872:   ################################################################################
2873: 
2874:   # News Pipeline - Fetches RSS + crawls NW.de, extracts full text, serves combined RSS feed
2875:   news-pipeline:
2876:     image: python:3.12-alpine
2877:     container_name: news-pipeline
2878:     logging: *default-logging
2879:     command: ["sh", "-c", "pip install -q pg8000 && python /app/news-pipeline.py"]
2880:     environment:
2881:       <<: *common-env
2882:       INTERVAL_SECONDS: ${INTERVAL_SECONDS:-900}
2883:       MINIFLUX_DB_URL: postgres://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/miniflux
2884:       PURGE_READ_DAYS: "7"
2885:       PURGE_UNREAD_DAYS: "30"
2886:       PURGE_INTERVAL_HOURS: "6"
2887:     volumes:
2888:       - ./scripts/webhooks/news-pipeline.py:/app/news-pipeline.py:ro
2889:     networks:
2890:       - potatostack
2891:     depends_on:
2892:       - article-extractor
2893:     restart: unless-stopped
2894:     deploy:
2895:       resources:
2896:         limits:
2897:           cpus: "0.50"
2898:           memory: 128M
2899: 
2900:   # Article Extractor - Headless Chromium + Bypass Paywalls Clean + trafilatura
2901:   article-extractor:
2902:     image: python:3.12-alpine
2903:     container_name: article-extractor
2904:     logging: *default-logging
2905:     entrypoint: ["/bin/sh", "/app/init.sh"]
2906:     environment:
2907:       <<: *common-env
2908:     volumes:
2909:       - ./scripts/webhooks/article-extractor.py:/app/article-extractor.py:ro
2910:       - ./scripts/init/article-extractor-init.sh:/app/init.sh:ro
2911:     networks:
2912:       - potatostack
2913:     restart: unless-stopped
2914:     deploy:
2915:       resources:
2916:         limits:
2917:           cpus: "1.0"
2918:           memory: 1G
2919: 
2920:   # Healthchecks - Cron monitoring
2921:   healthchecks:
2922:     image: lscr.io/linuxserver/healthchecks:${HEALTHCHECKS_TAG:-latest}
2923:     container_name: healthchecks
2924:     logging: *default-logging
2925:     ports:
2926:       - "${HOST_BIND:-127.0.0.1}:8001:8000"
2927:     environment:
2928:       <<: *common-env
2929:       SITE_ROOT: https://healthchecks.${HOST_DOMAIN:-local.domain}
2930:       SITE_NAME: PotatoStack Healthchecks
2931:       SUPERUSER_EMAIL: ${HEALTHCHECKS_ADMIN_EMAIL}
2932:       SUPERUSER_PASSWORD: ${HEALTHCHECKS_ADMIN_PASSWORD}
2933:       SECRET_KEY: ${HEALTHCHECKS_SECRET_KEY}
2934:       DB: postgres
2935:       DB_HOST: postgres
2936:       DB_PORT: 5432
2937:       DB_NAME: healthchecks
2938:       DB_USER: postgres
2939:       DB_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
2940:       INTEGRATIONS_ENABLED: "True"
2941:     volumes:
2942:       - /mnt/ssd/docker-data/healthchecks:/config
2943:     networks:
2944:       - potatostack
2945:     depends_on:
2946:       - postgres
2947:     labels:
2948:       - "wud.trigger.docker.update=true"
2949:     restart: unless-stopped
2950:     deploy:
2951:       resources:
2952:         limits:
2953:           cpus: "0.5"
2954:           memory: 256M
2955:         reservations:
2956:           cpus: "0.1"
2957:           memory: 128M
2958: 
2959:   ################################################################################
2960:   # UTILITIES & TOOLS
2961:   ################################################################################
2962: 
2963:   # Rustypaste config init - injects env vars (tokens) into config and landing page at startup
2964:   rustypaste-config-init:
2965:     image: alpine:latest
2966:     container_name: rustypaste-config-init
2967:     command:
2968:       - sh
2969:       - -c
2970:       - |
2971:         sed -e "s/__AUTH_TOKEN__/$$RUSTYPASTE_AUTH_TOKEN/g" \
2972:             -e "s/__DELETE_TOKEN__/$$RUSTYPASTE_DELETE_TOKEN/g" \
2973:             /templates/config.toml > /config/config.toml
2974:         sed -e "s/__AUTH_TOKEN__/$$RUSTYPASTE_AUTH_TOKEN/g" \
2975:             -e "s/__DELETE_TOKEN__/$$RUSTYPASTE_DELETE_TOKEN/g" \
2976:             /templates/landing_page.html > /config/landing_page.html
2977:         echo "Rustypaste config generated"
2978:     environment:
2979:       RUSTYPASTE_AUTH_TOKEN: ${RUSTYPASTE_AUTH_TOKEN}
2980:       RUSTYPASTE_DELETE_TOKEN: ${RUSTYPASTE_DELETE_TOKEN}
2981:     volumes:
2982:       - ./config/rustypaste:/templates:ro
2983:       - rustypaste-config:/config
2984:     networks:
2985:       - potatostack
2986:     restart: "no"
2987: 
2988:   # Rustypaste - Pastebin
2989:   rustypaste:
2990:     image: orhunp/rustypaste:${RUSTYPASTE_TAG:-latest}
2991:     container_name: rustypaste
2992:     logging: *default-logging
2993:     ports:
2994:       - "${HOST_BIND:-127.0.0.1}:${RUSTYPASTE_PORT:-8788}:${RUSTYPASTE_SERVER_PORT:-8000}"
2995:     environment:
2996:       <<: *common-env
2997:       CONFIG: /config/config.toml
2998:       RUST_LOG: ${RUSTYPASTE_LOG_LEVEL:-info}
2999:     volumes:
3000:       - /mnt/storage/rustypaste:/data
3001:       - rustypaste-config:/config:ro
3002:     networks:
3003:       - potatostack
3004:     depends_on:
3005:       storage-init:
3006:         condition: service_completed_successfully
3007:       rustypaste-config-init:
3008:         condition: service_completed_successfully
3009:     restart: unless-stopped
3010:     healthcheck:
3011:       disable: true
3012:     deploy:
3013:       resources:
3014:         limits:
3015:           cpus: "${RUSTYPASTE_MAX_CPUS:-0.25}"
3016:           memory: "${RUSTYPASTE_MAX_MEMORY:-64M}"
3017:         reservations:
3018:           cpus: "${RUSTYPASTE_MIN_CPUS:-0.1}"
3019:           memory: "${RUSTYPASTE_MIN_MEMORY:-32M}"
3020:     labels:
3021:       - "autoheal=true"
3022:       - "wud.trigger.docker.update=true"
3023: 
3024:   # PairDrop - AirDrop alternative (P2P file sharing)
3025:   pairdrop:
3026:     image: lscr.io/linuxserver/pairdrop:${PAIRDROP_TAG:-latest}
3027:     container_name: pairdrop
3028:     logging: *default-logging
3029:     ports:
3030:       - "${HOST_BIND:-127.0.0.1}:3013:3000"
3031:     environment:
3032:       <<: *common-env
3033:       RATE_LIMIT: ${PAIRDROP_RATE_LIMIT:-false}
3034:       WS_FALLBACK: ${PAIRDROP_WS_FALLBACK:-false}
3035:       RTC_CONFIG: ${PAIRDROP_RTC_CONFIG:-}
3036:       DEBUG_MODE: ${PAIRDROP_DEBUG_MODE:-false}
3037:     volumes:
3038:       - /mnt/ssd/docker-data/pairdrop:/config
3039:       - /mnt/storage/pairdrop:/data
3040:     networks:
3041:       - potatostack
3042:     depends_on:
3043:       - storage-init
3044:     restart: unless-stopped
3045:     healthcheck:
3046:       disable: true
3047:     deploy:
3048:       resources:
3049:         limits:
3050:           cpus: "0.5"
3051:           memory: 256M
3052:         reservations:
3053:           cpus: "0.1"
3054:           memory: 64M
3055:     labels:
3056:       - "autoheal=true"
3057:       - "wud.trigger.docker.update=true"
3058: 
3059:   # Stirling-PDF - PDF tools
3060:   # stirling-pdf:
3061:   #   image: frooodle/s-pdf:${STIRLING_PDF_TAG:-latest}
3062:   #   container_name: stirling-pdf
3063:   #   logging: *default-logging
3064:   #   ports:
3065:   #     - "${HOST_BIND:-127.0.0.1}:8086:8080"
3066:   #   environment:
3067:   #     <<: *common-env
3068:   #     DOCKER_ENABLE_SECURITY: "false"
3069:   #     INSTALL_BOOK_AND_ADVANCED_HTML_OPS: "true"
3070:   #     LANGS: de_DE,en_US
3071:   #   volumes:
3072:   #     - stirling-pdf-data:/usr/share/tessdata
3073:   #     - stirling-pdf-configs:/configs
3074:   #   networks:
3075:   #     - potatostack
3076:   #   restart: unless-stopped
3077:   #   deploy:
3078:   #     resources:
3079:   #       limits:
3080:   #         cpus: "0.75"
3081:   #         memory: 512M
3082:   #       reservations:
3083:   #         cpus: "0.25"

(Showing lines 1584-3083 of 4689. Use offset=3084 to continue.)
</content>
```

**Tool: read**

**Input:**
```json
{
  "filePath": "/home/daniel/potatostack/scripts/init/tailscale-serve-https.sh"
}
```

**Output:**
```
<path>/home/daniel/potatostack/scripts/init/tailscale-serve-https.sh</path>
<type>file</type>
<content>1: #!/bin/sh
2: ################################################################################
3: # Tailscale HTTPS Setup - Wrap local HTTP ports with Tailscale TLS
4: # Uses "tailscale serve --https=<port> http://127.0.0.1:<port>" for each port
5: #
6: # Features:
7: # - Automatically re-applies rules after crash/restart
8: # - Checks if backend ports are listening before mapping
9: # - Supports special port mappings (e.g., 443->11000 for Nextcloud)
10: # - Runs in loop mode for continuous monitoring
11: ################################################################################
12: 
13: export PATH="/usr/bin:$PATH"
14: 
15: # Install Docker CLI if needed
16: if ! command -v docker >/dev/null 2>&1; then
17: 	echo "Installing Docker CLI..."
18: 	apk add --no-cache docker-cli curl >/dev/null 2>&1
19: 	echo "✓ Docker CLI installed"
20: fi
21: 
22: TAILSCALE_CONTAINER="${TAILSCALE_CONTAINER:-tailscale}"
23: PORTS="${TAILSCALE_SERVE_PORTS:-}"
24: TAILSCALE_SERVE_LOOP="${TAILSCALE_SERVE_LOOP:-false}"
25: TAILSCALE_SERVE_INTERVAL="${TAILSCALE_SERVE_INTERVAL:-300}"
26: TAILSCALE_MARKER_FILE="${TAILSCALE_MARKER_FILE:-/https-marker/setup-complete}"
27: SPECIAL_MAPPINGS="${TAILSCALE_SERVE_SPECIAL:-}"
28: 
29: # Ports that serve HTTPS on the backend (need https+insecure://)
30: HTTPS_BACKEND_PORTS="9443 8080"
31: 
32: create_marker() {
33: 	if [ -n "$TAILSCALE_MARKER_FILE" ]; then
34: 		mkdir -p "$(dirname "$TAILSCALE_MARKER_FILE")" 2>/dev/null || true
35: 		date >"$TAILSCALE_MARKER_FILE" 2>/dev/null || true
36: 	fi
37: }
38: 
39: wait_for_tailscale() {
40: 	echo "Waiting for Tailscale to be ready..."
41: 	for i in $(seq 1 30); do
42: 		if docker exec "$TAILSCALE_CONTAINER" tailscale status >/dev/null 2>&1; then
43: 			echo "✓ Tailscale is ready"
44: 			return 0
45: 		fi
46: 		sleep 2
47: 	done
48: 	echo "⚠ Tailscale not ready after 60s"
49: 	return 1
50: }
51: 
52: # Check if a port is listening on localhost
53: port_is_listening() {
54: 	local port="$1"
55: 	# Try multiple methods to check port
56: 	if command -v ss >/dev/null 2>&1; then
57: 		ss -tlnH "sport = :$port" 2>/dev/null | grep -q "$port"
58: 	elif command -v netstat >/dev/null 2>&1; then
59: 		netstat -tlnp 2>/dev/null | grep -q ":$port "
60: 	else
61: 		# Fallback: try to connect
62: 		(echo >/dev/tcp/127.0.0.1/$port) 2>/dev/null
63: 	fi
64: }
65: 
66: # Get the backend URL for a port
67: get_backend_url() {
68: 	local port="$1"
69: 	local backend_url="http://127.0.0.1:$port"
70: 	for https_port in $HTTPS_BACKEND_PORTS; do
71: 		if [ "$port" = "$https_port" ]; then
72: 			backend_url="https+insecure://127.0.0.1:$port"
73: 			break
74: 		fi
75: 	done
76: 	echo "$backend_url"
77: }
78: 
79: # Apply serve rule for a single port mapping
80: apply_serve_rule() {
81: 	local ext_port="$1"
82: 	local int_port="$2"
83: 	local backend_url="$3"
84: 
85: 	# Check if backend is listening (skip check for special mappings where AIO manages the port)
86: 	if [ "$ext_port" != "443" ] && ! port_is_listening "$int_port"; then
87: 		echo "  ⏳ Port $int_port not listening yet, skipping"
88: 		return 1
89: 	fi
90: 
91: 	# Apply the serve rule
92: 	if docker exec "$TAILSCALE_CONTAINER" \
93: 		tailscale serve --bg --https="$ext_port" "$backend_url" 2>&1 | grep -v "already"; then
94: 		echo "  ✓ Port $ext_port -> $backend_url"
95: 		return 0
96: 	else
97: 		# Rule might already exist, which is fine
98: 		return 0
99: 	fi
100: }
101: 
102: apply_rules() {
103: 	echo "$(date '+%Y-%m-%d %H:%M:%S') - Configuring Tailscale HTTPS..."
104: 
105: 	# Apply standard port mappings (same port on both sides)
106: 	if [ -n "$PORTS" ]; then
107: 		echo "Standard ports: $PORTS"
108: 		for port in $(echo "$PORTS" | tr ',' ' '); do
109: 			if [ -z "$port" ]; then
110: 				continue
111: 			fi
112: 			backend_url=$(get_backend_url "$port")
113: 			apply_serve_rule "$port" "$port" "$backend_url"
114: 		done
115: 	fi
116: 
117: 	# Apply special port mappings (external_port:internal_port)
118: 	if [ -n "$SPECIAL_MAPPINGS" ]; then
119: 		echo "Special mappings: $SPECIAL_MAPPINGS"
120: 		for mapping in $(echo "$SPECIAL_MAPPINGS" | tr ',' ' '); do
121: 			if [ -z "$mapping" ]; then
122: 				continue
123: 			fi
124: 			ext_port=$(echo "$mapping" | cut -d: -f1)
125: 			int_port=$(echo "$mapping" | cut -d: -f2)
126: 			# Special mappings always use http (reverse proxy handles TLS)
127: 			apply_serve_rule "$ext_port" "$int_port" "http://127.0.0.1:$int_port"
128: 		done
129: 	fi
130: 
131: 	echo "✓ Tailscale HTTPS configuration complete"
132: }
133: 
134: # Reset serve rules for our ports (useful for clean restart)
135: reset_rules() {
136: 	echo "Resetting Tailscale serve rules..."
137: 
138: 	# Reset standard ports
139: 	for port in $(echo "$PORTS" | tr ',' ' '); do
140: 		if [ -n "$port" ]; then
141: 			docker exec "$TAILSCALE_CONTAINER" tailscale serve --https="$port" off 2>/dev/null || true
142: 		fi
143: 	done
144: 
145: 	# Reset special mappings
146: 	for mapping in $(echo "$SPECIAL_MAPPINGS" | tr ',' ' '); do
147: 		if [ -n "$mapping" ]; then
148: 			ext_port=$(echo "$mapping" | cut -d: -f1)
149: 			docker exec "$TAILSCALE_CONTAINER" tailscale serve --https="$ext_port" off 2>/dev/null || true
150: 		fi
151: 	done
152: 
153: 	echo "✓ Serve rules reset"
154: }
155: 
156: ################################################################################
157: # Main
158: ################################################################################
159: 
160: if [ -z "$PORTS" ] && [ -z "$SPECIAL_MAPPINGS" ]; then
161: 	echo "No TAILSCALE_SERVE_PORTS or TAILSCALE_SERVE_SPECIAL set; nothing to configure."
162: 	exit 0
163: fi
164: 
165: # Wait for tailscale first
166: if ! wait_for_tailscale; then
167: 	echo "Continuing anyway..."
168: fi
169: 
170: if [ "$TAILSCALE_SERVE_LOOP" = "true" ]; then
171: 	echo "Loop mode enabled (interval: ${TAILSCALE_SERVE_INTERVAL}s)"
172: 	echo "This ensures rules are re-applied after crashes or service restarts"
173: 	create_marker
174: 
175: 	# Initial reset and apply
176: 	reset_rules
177: 	sleep 2
178: 	apply_rules
179: 
180: 	while true; do
181: 		sleep "$TAILSCALE_SERVE_INTERVAL"
182: 		apply_rules
183: 		create_marker
184: 	done
185: else
186: 	# One-shot mode: reset and apply
187: 	reset_rules
188: 	sleep 2
189: 	apply_rules
190: 	create_marker
191: 	echo "✓ Tailscale HTTPS setup complete (one-shot mode)"
192: fi

(End of file - total 192 lines)
</content>
```

---

## Assistant (Plan · glm-5-free · 12.1s)

_Thinking:_

Now I have a comprehensive understanding of the stack. Let me check the config directory structure to see where beets config should go:

**Tool: bash**

**Input:**
```json
{
  "command": "ls -la /home/daniel/potatostack/config/",
  "description": "List config directory structure"
}
```

**Output:**
```
insgesamt 64
drwxr-xr-x 16 daniel daniel 4096 17. Feb 11:50 .
drwxr-xr-x 11 daniel daniel 4096 21. Feb 21:05 ..
drwxr-xr-x  2 daniel daniel 4096 27. Jan 15:56 alertmanager
drwxrwxr-x  2 daniel daniel 4096 21. Jan 19:07 alloy
drwxrwxr-x  3 daniel daniel 4096 17. Feb 11:50 backrest
drwxrwxr-x  2 daniel daniel 4096 30. Jan 17:46 gluetun
drwxrwxr-x  4 daniel daniel 4096 17. Jan 16:59 grafana
drwxrwxr-x  3 daniel daniel 4096 21. Feb 20:27 homer
drwxrwxr-x  2 daniel daniel 4096 18. Jan 12:31 loki
drwxr-xr-x  3 daniel daniel 4096 21. Feb 20:27 prometheus
drwxrwxr-x  2 daniel daniel 4096 17. Jan 17:28 promtail
drwxrwxr-x  2 daniel daniel 4096 21. Feb 19:35 rustypaste
drwxrwxr-x  2    977    977 4096 17. Feb 20:31 searxng
drwxrwxr-x  2 daniel daniel 4096 14. Feb 23:01 systemd
drwxrwxr-x  2 daniel daniel 4096  8. Feb 16:27 tdl
drwxrwxr-x  2 daniel daniel 4096 17. Jan 16:59 thanos

```

**Tool: bash**

**Input:**
```json
{
  "command": "grep -n \"8337\\|beets\" /home/daniel/potatostack/docker-compose.yml 2>/dev/null | head -20",
  "description": "Check for existing beets/port 8337"
}
```

---

## Assistant (Plan · glm-5-free · 172.9s)

_Thinking:_

Good - port 8337 is not used and beets is not in the compose file. Now I have all the information needed to create a comprehensive plan. Let me also check if there's a storage init script to see the directory creation pattern:

Now I have all the information needed. Here's the **comprehensive integration plan**:

---

## Beets Integration Plan for PotatoStack

### Overview

Add **beets** (music library manager) to:
1. Organize/tag music from downloads → music library
2. Integrate with existing **Navidrome** (streaming)
3. Integrate with existing **slskd** (Soulseek sharing)
4. Add to **Homer** dashboard
5. Add to **Tailscale** HTTPS serve

---

### 1. Docker Compose Service

**Location**: Add after `navidrome` service (around line 1973)

```yaml
  # Beets - Music library manager and tagger
  beets:
    image: lscr.io/linuxserver/beets:${BEETS_TAG:-latest}
    container_name: beets
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:8337:8337"
    environment:
      <<: *common-env
    volumes:
      - /mnt/ssd/docker-data/beets:/config
      - /mnt/storage/media/music:/music        # Library destination (RW)
      - /mnt/storage2/downloads/slskd:/downloads/slskd:ro
      - /mnt/storage2/downloads/torrents:/downloads/torrents:ro
      - /mnt/storage2/downloads/aria2:/downloads/aria2:ro
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f beets || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 128M
    labels:
      - "wud.trigger.docker.update=true"
```

**Key decisions**:
- **Port 8337** (beets default WebUI port - not used in your stack)
- **RW mount** for `/music` (beets needs to write organized files)
- **RO mounts** for download sources (safer, beets only reads)
- **Regular network** (not behind gluetun - no VPN needed for tagging)

---

### 2. Beets Configuration (`config/beets/config.yaml`)

```yaml
directory: /music
library: /config/library.db

import:
  copy: yes
  write: yes
  move: no
  timid: no
  autotag: yes
  quiet: no
  log: /config/beets.log

musicbrainz:
  searchlimit: 5

paths:
  default: $albumartist/$year - $album/$track - $title
  singleton: Singles/$artist - $title
  comp: Compilations/$album/$track - $title

item_fields:
  albumartist_multivalued: albumartist

plugins:
  - fetchart
  - embedart
  - lyrics
  - lastgenre
  - mbsync
  - web
  - duplicates

fetchart:
  auto: yes
  sources: coverart musicbrainz amazon albumart
  cover_names: cover folder front

web:
  host: 0.0.0.0
  port: 8337

replace:
  '[\\/]': _
  '^\.': _
  '[\x00-\x1f]': _
  '[<>:"\?\*\|]': _
  '\.$': ''
  '\s+$': ''
  '^\s+': ''

per_disc_numbering: yes
```

---

### 3. Tailscale HTTPS Integration

**File**: `scripts/init/tailscale-serve-https.sh` (no changes needed)

**Update `docker-compose.yml`** in two places:

**tailscale-https-setup** (line ~546):
```yaml
TAILSCALE_SERVE_PORTS: "7575,8088,3000,3001,3002,8086,8096,8989,7878,8686,9696,6767,13378,4533,9900,8282,8076,2234,8097,8008,2283,8090,8095,8384,3004,3006,8888,8093,5006,9090,3100,9093,10903,10902,8087,6060,8001,8788,8060,8889,8081,3010,3013,8180,8181,9925,5984,9898,8123,8191,3333,3335,5656,8085,8337"
```

**tailscale-https-monitor** (line ~575):
```yaml
TAILSCALE_SERVE_PORTS: 7575,8088,3000,3001,3002,8086,8096,8989,7878,8686,9696,6767,13378,4533,9900,8282,8076,2234,8097,8008,2283,8090,8095,8384,3004,3006,8888,8093,5006,9090,3100,9093,10903,10902,8087,6060,8001,8788,8060,8889,8081,3010,3013,8180,8181,9925,5984,9898,8123,8191,3333,3335,5656,8085,8337
```

**URL**: `https://potatostack.tale-iwato.ts.net:8337`

---

### 4. Homer Dashboard

**File**: `config/homer/config.yml`

Add to the "Media" section (after Navidrome):
```yaml
      - name: "Beets"
        logo: "https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/beets.png"
        subtitle: "Music Library Manager"
        url: "https://potatostack.tale-iwato.ts.net:8337"
        target: "_blank"
```

---

### 5. Storage Init Script

**File**: `scripts/init/init-storage.sh` - add beets data directory creation

Add to the SSD docker-data section:
```bash
mkdir -p /mnt/ssd/docker-data/beets
```

---

### 6. Environment Variables

**File**: `.env.example` - add tag:
```bash
# Music Management
BEETS_TAG=latest
```

---

### 7. Workflow Integration

```
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│   slskd         │────▶│   downloads/    │────▶│     beets       │
│   (Soulseek)    │     │   slskd/        │     │   (tag/organize)│
└─────────────────┘     └─────────────────┘     └────────┬────────┘
                                                         │
┌─────────────────┐     ┌─────────────────┐              │
│   unpackerr     │────▶│   extracted     │──────────────┘
│   (archives)    │     │   files         │
└─────────────────┘     └─────────────────┘
                                                         │
                        ┌─────────────────┐              ▼
                        │ /mnt/storage/   │     ┌─────────────────┐
                        │ media/music/    │◀────│ organized FLAC  │
                        │ (library)       │     │ Artist/Album    │
                        └────────┬────────┘     └─────────────────┘
                                 │
              ┌──────────────────┼──────────────────┐
              │                  │                  │
              ▼                  ▼                  ▼
     ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐
     │   Navidrome     │ │     slskd       │ │    Jellyfin     │
     │   (streaming)   │ │   (share:ro)    │ │   (streaming)   │
     └─────────────────┘ └─────────────────┘ └─────────────────┘
```

**Key integration points**:
1. **beets** imports from `/mnt/storage2/downloads/slskd` (slskd downloads)
2. **beets** organizes into `/mnt/storage/media/music` (library)
3. **Navidrome** reads `/mnt/storage/media/music:ro` (already configured)
4. **slskd** shares `/mnt/storage/media/music:ro` (already configured)
5. **Jellyfin** already mounts music library

---

### 8. Files to Create/Modify

| File | Action |
|------|--------|
| `docker-compose.yml` | Add beets service |
| `docker-compose.yml` | Add 8337 to TAILSCALE_SERVE_PORTS (2 places) |
| `config/beets/config.yaml` | **CREATE** - beets configuration |
| `config/homer/config.yml` | Add beets entry to Media section |
| `scripts/init/init-storage.sh` | Add beets directory creation |
| `.env.example` | Add BEETS_TAG=latest |

---

### Questions Before Implementation

**Tool: question**

**Input:**
```json
{
  "questions": [
    {
      "question": "Should beets automatically import from download folders, or will you trigger imports manually via CLI/WebUI?",
      "header": "Import Mode",
      "options": [
        {
          "label": "Manual import only",
          "description": "You run 'beet import /downloads/...' when needed - safest approach"
        },
        {
          "label": "Watch folder auto-import",
          "description": "Beets monitors downloads and auto-imports - requires additional setup"
        }
      ],
      "multiple": false
    },
    {
      "question": "What should happen to files after beets processes them?",
      "header": "File Handling",
      "options": [
        {
          "label": "Copy to library (Recommended)",
          "description": "Keep originals in downloads, copy organized files to library - safer for seeding"
        },
        {
          "label": "Move to library",
          "description": "Move files from downloads to library - saves space but breaks seeding"
        }
      ],
      "multiple": false
    }
  ]
}
```

**Output:**
```
User has answered your questions: "Should beets automatically import from download folders, or will you trigger imports manually via CLI/WebUI?"="Manual import only", "What should happen to files after beets processes them?"="Copy to library (Recommended)". You can now continue with the user's answers in mind.
```

---

## Assistant (Plan · glm-5-free · 24.6s)

_Thinking:_

The user wants:
1. Manual import only - they'll run `beet import` commands when needed
2. Copy to library - keeps originals for seeding

Now I have all the information needed. Let me also check the AGENTS.md to see if there are any specific conventions I need to follow:

Good choices. The plan is finalized with:
- **Manual import** via `docker exec -it beets beet import /downloads/slskd`
- **Copy mode** (preserves originals for seeding, configured via `import.copy: yes`)

---

## Final Implementation Plan

### Files to Modify/Create:

| # | File | Change |
|---|------|--------|
| 1 | `docker-compose.yml` | Add beets service (~line 1973, after navidrome) |
| 2 | `docker-compose.yml` | Add port 8337 to tailscale-https-setup (line ~546) |
| 3 | `docker-compose.yml` | Add port 8337 to tailscale-https-monitor (line ~575) |
| 4 | `config/beets/config.yaml` | **CREATE** - beets configuration file |
| 5 | `config/homer/config.yml` | Add beets to Media section |
| 6 | `scripts/init/init-storage.sh` | Add `/mnt/ssd/docker-data/beets` directory |
| 7 | `.env.example` | Add `BEETS_TAG=latest` |

### Usage After Deployment:

```bash
# Import music from slskd downloads
docker exec -it -u abc beets beet import /downloads/slskd

# Import with auto-accept (no prompts)
docker exec -it -u abc beets beet import -A /downloads/slskd

# View library stats
docker exec -it -u abc beets beet stats

# Access WebUI
https://potatostack.tale-iwato.ts.net:8337
```

Ready to execute when you give the go-ahead.

---

i only upload check if this plan is SOTA and make it bette rmore plugins while keeping it lightwheight
