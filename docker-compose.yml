################################################################################
# PotatoStack Main - Optimized for Mini PC (16GB RAM)
# Full-featured self-hosted stack with monitoring, automation, and media
# Target: Mini PC with 16GB RAM, 4+ core CPU, 1GB ethernet
################################################################################

x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"
    compress: "true"

x-common-env: &common-env
  TZ: Europe/Berlin
  PUID: 1000
  PGID: 1000

services:
  ################################################################################
  # Storage Init - Creates required directories on startup
  ################################################################################
  storage-init:
    image: alpine:${ALPINE_TAG:-latest}
    container_name: storage-init
    command: sh /init-storage.sh
    environment:
      <<: *common-env
    volumes:
      - /mnt/storage:/mnt/storage
      - /mnt/cachehdd:/mnt/cachehdd
      - ./init-storage.sh:/init-storage.sh:ro
    network_mode: none
    restart: "no"

  ################################################################################
  # CORE DATABASES
  ################################################################################

  # PostgreSQL - Primary database for multiple services (SOTA 2025)
  postgres:
    image: postgres:${POSTGRES_TAG:-16-alpine}
    container_name: postgres
    logging: *default-logging
    shm_size: 1gb
    environment:
      <<: *common-env
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
      POSTGRES_MULTIPLE_DATABASES: ${POSTGRES_DATABASES:-nextcloud,authentik,gitea,immich,calibre,linkding,n8n,healthchecks,stirlingpdf,calcom,atuin,homarr,paperless,pingvin}
    command:
      - "postgres"
      - "-c"
      - "shared_buffers=1GB"
      - "-c"
      - "effective_cache_size=3GB"
      - "-c"
      - "work_mem=16MB"
      - "-c"
      - "maintenance_work_mem=256MB"
      - "-c"
      - "max_connections=100"
      - "-c"
      - "checkpoint_completion_target=0.9"
      - "-c"
      - "wal_buffers=16MB"
      - "-c"
      - "default_statistics_target=100"
      - "-c"
      - "random_page_cost=1.1"
      - "-c"
      - "effective_io_concurrency=200"
      - "-c"
      - "max_worker_processes=8"
      - "-c"
      - "max_parallel_workers=4"
      - "-c"
      - "max_parallel_workers_per_gather=2"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./scripts/init-postgres-multiple-dbs.sh:/docker-entrypoint-initdb.d/init-postgres-multiple-dbs.sh:ro
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M

  # MongoDB - Document database (SOTA 2025)
  mongo:
    image: mongo:${MONGO_TAG:-7}
    container_name: mongo
    logging: *default-logging
    command:
      - "mongod"
      - "--wiredTigerCacheSizeGB=1.5"
      - "--wiredTigerJournalCompressor=snappy"
      - "--bind_ip_all"
    environment:
      <<: *common-env
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_ROOT_PASSWORD}
    volumes:
      - mongo-data:/data/db
      - mongo-config:/data/configdb
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M

  # Adminer - Database management UI
  adminer:
    image: adminer:${ADMINER_TAG:-latest}
    container_name: adminer
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8090:8080"
    environment:
      <<: *common-env
      ADMINER_DEFAULT_SERVER: postgres
      ADMINER_DESIGN: nette
    networks:
      - potatostack
    restart: unless-stopped
    depends_on:
      - postgres
      - mongo
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M

  # Redis Shared Cache - SOTA 2025 consolidated (N8n, Gitea, Immich, Paperless, Sentry)
  redis-cache:
    image: redis:${REDIS_TAG:-7-alpine}
    container_name: redis-cache
    logging: *default-logging
    command: redis-server --maxmemory 1gb --maxmemory-policy allkeys-lfu --databases 16 --activedefrag yes --lazyfree-lazy-eviction yes --lazyfree-lazy-expire yes --lazyfree-lazy-server-del yes --save 60 1000 --appendonly yes --appendfsync everysec
    volumes:
      - redis-cache-data:/data
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 512M

  ################################################################################
  # SECURITY & INTRUSION PREVENTION
  ################################################################################

  # CrowdSec - Modern IPS/IDS with community threat intelligence (SOTA 2025)
  crowdsec:
    image: crowdsecurity/crowdsec:${CROWDSEC_TAG:-latest}
    container_name: crowdsec
    logging: *default-logging
    environment:
      <<: *common-env
      COLLECTIONS: "crowdsecurity/traefik crowdsecurity/http-cve crowdsecurity/whitelist-good-actors crowdsecurity/nginx crowdsecurity/linux"
      GID: "1000"
    volumes:
      - crowdsec-db:/var/lib/crowdsec/data/
      - crowdsec-config:/etc/crowdsec/
      - /var/log:/var/log:ro
      - traefik-logs:/var/log/traefik:ro
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "cscli", "version"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M

  # CrowdSec Traefik Bouncer - Blocks malicious IPs at reverse proxy level
  crowdsec-traefik-bouncer:
    image: fbonalair/traefik-crowdsec-bouncer:${CROWDSEC_BOUNCER_TAG:-latest}
    container_name: crowdsec-traefik-bouncer
    logging: *default-logging
    environment:
      CROWDSEC_BOUNCER_API_KEY: ${CROWDSEC_BOUNCER_KEY}
      CROWDSEC_AGENT_HOST: crowdsec:8080
      GIN_MODE: release
    networks:
      - potatostack
    depends_on:
      - crowdsec
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.1'
          memory: 64M

  ################################################################################
  # REVERSE PROXY & SSL
  ################################################################################

  # Traefik - Modern reverse proxy with automatic SSL (SOTA 2025)
  traefik:
    image: traefik:${TRAEFIK_TAG:-latest}
    container_name: traefik
    logging: *default-logging
    security_opt:
      - no-new-privileges:true
    ports:
      - "80:80"
      - "443:443"
      - "${HOST_BIND:-192.168.178.40}:8080:8080"  # Dashboard
    environment:
      <<: *common-env
    command:
      # EntryPoints
      - "--entrypoints.web.address=:80"
      - "--entrypoints.web.http.redirections.entrypoint.to=websecure"
      - "--entrypoints.web.http.redirections.entrypoint.scheme=https"
      - "--entrypoints.websecure.address=:443"
      - "--entrypoints.websecure.http.tls=true"
      # Providers
      - "--providers.docker=true"
      - "--providers.docker.exposedbydefault=false"
      - "--providers.docker.network=potatostack"
      # API & Dashboard
      - "--api.dashboard=true"
      - "--api.insecure=false"
      # SSL/TLS
      - "--certificatesresolvers.letsencrypt.acme.email=${ACME_EMAIL}"
      - "--certificatesresolvers.letsencrypt.acme.storage=/letsencrypt/acme.json"
      - "--certificatesresolvers.letsencrypt.acme.httpchallenge.entrypoint=web"
      # Observability
      - "--log.level=INFO"
      - "--accesslog=true"
      - "--metrics.prometheus=true"
      - "--metrics.prometheus.addServicesLabels=true"
      - "--metrics.prometheus.addEntryPointsLabels=true"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - traefik-certs:/letsencrypt
    networks:
      - potatostack
    restart: unless-stopped
    labels:
      - "traefik.enable=true"
      # Dashboard router
      - "traefik.http.routers.traefik.rule=Host(`traefik.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.traefik.entrypoints=websecure"
      - "traefik.http.routers.traefik.service=api@internal"
      - "traefik.http.routers.traefik.tls=true"
      # Security headers middleware
      - "traefik.http.middlewares.security-headers.headers.customResponseHeaders.X-Robots-Tag=noindex,nofollow,nosnippet,noarchive,notranslate,noimageindex"
      - "traefik.http.middlewares.security-headers.headers.sslRedirect=true"
      - "traefik.http.middlewares.security-headers.headers.stsSeconds=315360000"
      - "traefik.http.middlewares.security-headers.headers.stsIncludeSubdomains=true"
      - "traefik.http.middlewares.security-headers.headers.stsPreload=true"
      - "traefik.http.middlewares.security-headers.headers.forceSTSHeader=true"
      - "traefik.http.middlewares.security-headers.headers.frameDeny=true"
      - "traefik.http.middlewares.security-headers.headers.contentTypeNosniff=true"
      - "traefik.http.middlewares.security-headers.headers.browserXssFilter=true"
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M

  ################################################################################
  # AUTHENTICATION & SECURITY
  ################################################################################

  # Authentik - SSO and 2FA provider (SOTA 2025 - no Redis needed in v2025.10+)
  authentik-postgres:
    image: postgres:${POSTGRES_TAG:-16-alpine}
    container_name: authentik-postgres
    logging: *default-logging
    shm_size: 128mb
    command:
      - "postgres"
      - "-c"
      - "shared_buffers=128MB"
      - "-c"
      - "effective_cache_size=384MB"
      - "-c"
      - "work_mem=4MB"
      - "-c"
      - "maintenance_work_mem=32MB"
      - "-c"
      - "max_connections=50"
    environment:
      POSTGRES_USER: authentik
      POSTGRES_PASSWORD: ${AUTHENTIK_DB_PASSWORD}
      POSTGRES_DB: authentik
    volumes:
      - authentik-postgres:/var/lib/postgresql/data
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U authentik"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M

  authentik-server:
    image: ghcr.io/goauthentik/server:${AUTHENTIK_TAG:-latest}
    container_name: authentik-server
    logging: *default-logging
    command: server
    ports:
      - "${HOST_BIND:-192.168.178.40}:9000:9000"
      - "${HOST_BIND:-192.168.178.40}:9443:9443"
    environment:
      <<: *common-env
      AUTHENTIK_POSTGRESQL__HOST: authentik-postgres
      AUTHENTIK_POSTGRESQL__USER: authentik
      AUTHENTIK_POSTGRESQL__NAME: authentik
      AUTHENTIK_POSTGRESQL__PASSWORD: ${AUTHENTIK_DB_PASSWORD}
      AUTHENTIK_SECRET_KEY: ${AUTHENTIK_SECRET_KEY}
      AUTHENTIK_ERROR_REPORTING__ENABLED: "false"
    volumes:
      - authentik-media:/media
      - authentik-custom-templates:/templates
    networks:
      - potatostack
    depends_on:
      - authentik-postgres
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 1G

  authentik-worker:
    image: ghcr.io/goauthentik/server:${AUTHENTIK_TAG:-latest}
    container_name: authentik-worker
    logging: *default-logging
    command: worker
    environment:
      <<: *common-env
      AUTHENTIK_POSTGRESQL__HOST: authentik-postgres
      AUTHENTIK_POSTGRESQL__USER: authentik
      AUTHENTIK_POSTGRESQL__NAME: authentik
      AUTHENTIK_POSTGRESQL__PASSWORD: ${AUTHENTIK_DB_PASSWORD}
      AUTHENTIK_SECRET_KEY: ${AUTHENTIK_SECRET_KEY}
    volumes:
      - authentik-media:/media
      - authentik-certs:/certs
    networks:
      - potatostack
    depends_on:
      - authentik-postgres
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M

  # Vaultwarden - Password manager and 2FA aggregator
  vaultwarden:
    image: vaultwarden/server:${VAULTWARDEN_TAG:-latest}
    container_name: vaultwarden
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8888:80"
      - "${HOST_BIND:-192.168.178.40}:3012:3012"
    environment:
      <<: *common-env
      DOMAIN: https://vault.${HOST_DOMAIN:-local.domain}
      ROCKET_PORT: 80
      WEBSOCKET_ENABLED: "true"
      WEBSOCKET_PORT: 3012
      SIGNUPS_ALLOWED: ${VAULTWARDEN_SIGNUPS_ALLOWED:-false}
      INVITATIONS_ALLOWED: ${VAULTWARDEN_INVITATIONS_ALLOWED:-true}
      ADMIN_TOKEN: ${VAULTWARDEN_ADMIN_TOKEN}
      DATABASE_URL: /data/db.sqlite3
      ICON_CACHE_TTL: 2592000
      LOG_LEVEL: warn
    volumes:
      - vaultwarden-data:/data
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:80/alive"]
      interval: 60s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.vaultwarden.rule=Host(`vault.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.vaultwarden.entrypoints=websecure"
      - "traefik.http.routers.vaultwarden.tls=true"
      - "traefik.http.services.vaultwarden.loadbalancer.server.port=80"
      - "traefik.http.routers.vaultwarden.middlewares=security-headers@docker"

  ################################################################################
  # DNS & AD BLOCKING
  ################################################################################

  # AdGuard Home - DNS-level ad blocking with encrypted DNS (SOTA 2025)
  adguardhome:
    image: adguard/adguardhome:${ADGUARD_TAG:-latest}
    container_name: adguardhome
    logging: *default-logging
    ports:
      - "53:53/tcp"
      - "53:53/udp"
      - "${HOST_BIND:-192.168.178.40}:3053:3000"
      - "${HOST_BIND:-192.168.178.40}:8053:80"
    environment:
      <<: *common-env
    volumes:
      - adguard-work:/opt/adguardhome/work
      - adguard-conf:/opt/adguardhome/conf
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:80/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.adguard.rule=Host(`dns.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.adguard.entrypoints=websecure"
      - "traefik.http.routers.adguard.tls=true"
      - "traefik.http.services.adguard.loadbalancer.server.port=80"

  ################################################################################
  # VPN & NETWORKING
  ################################################################################

  # Gluetun - VPN client with killswitch
  gluetun:
    image: qmcgaw/gluetun:${GLUETUN_TAG:-latest}
    container_name: gluetun
    logging: *default-logging
    cap_add:
      - NET_ADMIN
    devices:
      - /dev/net/tun:/dev/net/tun
    ports:
      - "${HOST_BIND:-192.168.178.40}:8000:8000"     # Gluetun control
      - "${HOST_BIND:-192.168.178.40}:8989:8989"     # Sonarr
      - "${HOST_BIND:-192.168.178.40}:7878:7878"     # Radarr
      - "${HOST_BIND:-192.168.178.40}:9696:9696"     # Prowlarr
      - "${HOST_BIND:-192.168.178.40}:6767:6767"     # Bazarr
      - "${HOST_BIND:-192.168.178.40}:8787:8787"     # Readarr
      - "${HOST_BIND:-192.168.178.40}:8686:8686"     # Lidarr
      - "${HOST_BIND:-192.168.178.40}:8282:8282"     # qBittorrent WebUI
      - "${HOST_BIND:-192.168.178.40}:51413:51413"   # qBittorrent peer
      - "${HOST_BIND:-192.168.178.40}:51413:51413/udp"
      - "${HOST_BIND:-192.168.178.40}:6800:6800"     # aria2 RPC
      - "${HOST_BIND:-192.168.178.40}:6880:80"       # aria2 WebUI
      - "${HOST_BIND:-192.168.178.40}:2234:2234"     # slskd WebUI
      - "${HOST_BIND:-192.168.178.40}:50000:50000"   # slskd peer
    environment:
      <<: *common-env
      VPN_SERVICE_PROVIDER: ${VPN_PROVIDER:-surfshark}
      VPN_TYPE: ${VPN_TYPE:-openvpn}
      OPENVPN_USER: ${VPN_USER}
      OPENVPN_PASSWORD: ${VPN_PASSWORD}
      SERVER_COUNTRIES: ${VPN_COUNTRY:-Germany}
      FIREWALL_OUTBOUND_SUBNETS: ${LAN_NETWORK:-192.168.178.0/24}
      FIREWALL_VPN_INPUT_PORTS: 51413,6800,50000
      HTTP_CONTROL_SERVER_ADDRESS: :8000
      LOG_LEVEL: info
      HEALTH_VPN_DURATION_INITIAL: 60s
      HEALTH_VPN_DURATION_ADDITION: 10s
    volumes:
      - gluetun-config:/gluetun
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "pgrep", "openvpn"]
      interval: 120s
      timeout: 10s
      retries: 3
      start_period: 120s
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M
    labels:
      - "autoheal=true"

  # Tailscale - Mesh VPN for remote access (SOTA 2025 - easiest option)
  tailscale:
    image: tailscale/tailscale:${TAILSCALE_TAG:-latest}
    container_name: tailscale
    logging: *default-logging
    hostname: potatostack
    environment:
      <<: *common-env
      TS_AUTHKEY: ${TAILSCALE_AUTHKEY}
      TS_STATE_DIR: /var/lib/tailscale
      TS_USERSPACE: "false"
    volumes:
      - tailscale-data:/var/lib/tailscale
      - /dev/net/tun:/dev/net/tun
    cap_add:
      - NET_ADMIN
      - SYS_MODULE
    network_mode: host
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M

  ################################################################################
  # CLOUD STORAGE & FILE SYNC
  ################################################################################

  # Nextcloud AIO - All-in-One with Collabora, Talk, Whiteboard, etc. (SOTA 2025)
  nextcloud-aio:
    image: nextcloud/all-in-one:${NEXTCLOUD_AIO_TAG:-latest}
    container_name: nextcloud-aio-mastercontainer
    logging: *default-logging
    init: true
    ports:
      - "${HOST_BIND:-192.168.178.40}:8080:8080"  # AIO interface
      - "${HOST_BIND:-192.168.178.40}:8443:8443"  # Nextcloud
    environment:
      <<: *common-env
      APACHE_PORT: 11000
      APACHE_IP_BINDING: 0.0.0.0
      NEXTCLOUD_DATADIR: /mnt/storage/nextcloud
      NEXTCLOUD_MOUNT: /mnt/storage/nextcloud
      NEXTCLOUD_UPLOAD_LIMIT: 10G
      NEXTCLOUD_MAX_TIME: 3600
      NEXTCLOUD_MEMORY_LIMIT: 512M
      SKIP_DOMAIN_VALIDATION: "true"
    volumes:
      - nextcloud-aio-mastercontainer:/mnt/docker-aio-config
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /mnt/storage/nextcloud:/mnt/storage/nextcloud:rw
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M

  # Syncthing - P2P file sync
  syncthing:
    image: lscr.io/linuxserver/syncthing:${SYNCTHING_TAG:-latest}
    container_name: syncthing
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8384:8384"
      - "${HOST_BIND:-192.168.178.40}:22000:22000/tcp"
      - "${HOST_BIND:-192.168.178.40}:22000:22000/udp"
      - "${HOST_BIND:-192.168.178.40}:21027:21027/udp"
    environment:
      <<: *common-env
    volumes:
      - syncthing-config:/config
      - /mnt/storage/syncthing:/data
      - /mnt/cachehdd/syncthing-versions:/data/.stversions
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8384/rest/noauth/health"]
      interval: 120s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M

  ################################################################################
  # KNOWLEDGE MANAGEMENT
  ################################################################################

  ################################################################################
  # FINANCE
  ################################################################################

  # Actual Budget - Modern budgeting with bank sync (SOTA 2025)
  actual-budget:
    image: actualbudget/actual-server:${ACTUAL_TAG:-latest}
    container_name: actual-budget
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:5006:5006"
    environment:
      <<: *common-env
    volumes:
      - actual-data:/data
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:5006/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.actual.rule=Host(`budget.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.actual.entrypoints=websecure"
      - "traefik.http.routers.actual.tls=true"
      - "traefik.http.services.actual.loadbalancer.server.port=5006"

  ################################################################################
  # MEDIA MANAGEMENT - *ARR STACK
  ################################################################################

  # Prowlarr - Indexer manager
  prowlarr:
    image: lscr.io/linuxserver/prowlarr:${PROWLARR_TAG:-latest}
    container_name: prowlarr
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
    volumes:
      - prowlarr-config:/config
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:9696/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M
    labels:
      - "autoheal=true"

  # Sonarr - TV show management
  sonarr:
    image: lscr.io/linuxserver/sonarr:${SONARR_TAG:-latest}
    container_name: sonarr
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
    volumes:
      - sonarr-config:/config
      - /mnt/storage/media/tv:/tv
      - /mnt/storage/downloads:/downloads
      - /mnt/cachehdd/qbittorrent-incomplete:/downloads/incomplete
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:8989/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    labels:
      - "autoheal=true"

  # Radarr - Movie management
  radarr:
    image: lscr.io/linuxserver/radarr:${RADARR_TAG:-latest}
    container_name: radarr
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
    volumes:
      - radarr-config:/config
      - /mnt/storage/media/movies:/movies
      - /mnt/storage/downloads:/downloads
      - /mnt/cachehdd/qbittorrent-incomplete:/downloads/incomplete
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:7878/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    labels:
      - "autoheal=true"

  # Lidarr - Music management
  lidarr:
    image: lscr.io/linuxserver/lidarr:${LIDARR_TAG:-latest}
    container_name: lidarr
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
    volumes:
      - lidarr-config:/config
      - /mnt/storage/media/music:/music
      - /mnt/storage/downloads:/downloads
      - /mnt/cachehdd/qbittorrent-incomplete:/downloads/incomplete
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:8686/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    labels:
      - "autoheal=true"

  # Readarr - Ebook management
  readarr:
    image: lscr.io/linuxserver/readarr:${READARR_TAG:-develop}
    container_name: readarr
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
    volumes:
      - readarr-config:/config
      - /mnt/storage/media/books:/books
      - /mnt/storage/downloads:/downloads
      - /mnt/cachehdd/qbittorrent-incomplete:/downloads/incomplete
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:8787/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    labels:
      - "autoheal=true"

  # Bazarr - Subtitle management
  bazarr:
    image: lscr.io/linuxserver/bazarr:${BAZARR_TAG:-latest}
    container_name: bazarr
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
    volumes:
      - bazarr-config:/config
      - /mnt/storage/media/movies:/movies
      - /mnt/storage/media/tv:/tv
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      sonarr:
        condition: service_started
      radarr:
        condition: service_started
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:6767/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M
    labels:
      - "autoheal=true"

  # Maintainerr - Media library cleanup
  maintainerr:
    image: jorenn92/maintainerr:${MAINTAINERR_TAG:-latest}
    container_name: maintainerr
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:6246:6246"
    environment:
      <<: *common-env
    volumes:
      - maintainerr-data:/opt/data
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M

  ################################################################################
  # MEDIA SERVERS & REQUEST MANAGEMENT
  ################################################################################

  # Jellyfin - Media server (SOTA 2025 with HW acceleration support)
  jellyfin:
    image: lscr.io/linuxserver/jellyfin:${JELLYFIN_TAG:-latest}
    container_name: jellyfin
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8096:8096"
      - "${HOST_BIND:-192.168.178.40}:8920:8920"
      - "${HOST_BIND:-192.168.178.40}:7359:7359/udp"
      - "${HOST_BIND:-192.168.178.40}:1900:1900/udp"
    environment:
      <<: *common-env
    # Uncomment group_add and devices for Intel/AMD hardware acceleration
    # Get render group ID with: getent group render | cut -d: -f3
    # group_add:
    #   - "107"  # render group - adjust to match your system
    # devices:
    #   - /dev/dri/renderD128:/dev/dri/renderD128
    devices:
      - /dev/dri/renderD128:/dev/dri/renderD128
      - /dev/dri/card0:/dev/dri/card0
    group_add:
      - "109"
    volumes:
      - jellyfin-config:/config
      - /mnt/storage/media/tv:/data/tvshows
      - /mnt/storage/media/movies:/data/movies
      - /mnt/storage/media/music:/data/music
      - /mnt/storage/media/audiobooks:/data/audiobooks
      - /mnt/cachehdd/jellyfin-cache:/cache
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.5'
          memory: 2G
        reservations:
          cpus: '0.75'
          memory: 1G

  # Jellyseerr - Media request management for Jellyfin
  jellyseerr:
    image: fallenbagel/jellyseerr:${JELLYSEERR_TAG:-latest}
    container_name: jellyseerr
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:5055:5055"
    environment:
      <<: *common-env
    volumes:
      - jellyseerr-config:/app/config
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M

  # Audiobookshelf - Audiobook and podcast server
  audiobookshelf:
    image: ghcr.io/advplyr/audiobookshelf:${AUDIOBOOKSHELF_TAG:-latest}
    container_name: audiobookshelf
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:13378:80"
    environment:
      <<: *common-env
    volumes:
      - audiobookshelf-config:/config
      - /mnt/cachehdd/audiobookshelf/metadata:/metadata
      - /mnt/storage/media/audiobooks:/audiobooks
      - /mnt/storage/media/podcasts:/podcasts
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M

  ################################################################################
  # DOWNLOAD CLIENTS (behind VPN)
  ################################################################################

  # qBittorrent - Torrent client
  qbittorrent:
    image: lscr.io/linuxserver/qbittorrent:${QBITTORRENT_TAG:-latest}
    container_name: qbittorrent
    logging: *default-logging
    network_mode: "service:gluetun"
    entrypoint: /qb-init.sh
    environment:
      <<: *common-env
      WEBUI_PORT: 8282
    volumes:
      - qbittorrent-config:/config
      - /mnt/storage/downloads:/downloads
      - /mnt/cachehdd/qbittorrent-incomplete:/incomplete
      - ./qbittorrent-init.sh:/qb-init.sh:ro
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:8282/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    labels:
      - "autoheal=true"

  # Aria2 - Download manager
  aria2:
    image: p3terx/aria2-pro:${ARIA2_TAG:-latest}
    container_name: aria2
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
      RPC_SECRET: ${ARIA2_RPC_SECRET}
      RPC_PORT: 6800
      LISTEN_PORT: 6888
      DISK_CACHE: 64M
      IPV6_MODE: "false"
    volumes:
      - aria2-config:/config
      - /mnt/storage/downloads:/downloads
      - /mnt/cachehdd/aria2-incomplete:/incomplete
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:6800/jsonrpc || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M
    labels:
      - "autoheal=true"

  # AriaNg - Aria2 web UI
  ariang:
    image: p3terx/ariang:${ARIANG_TAG:-latest}
    container_name: ariang
    logging: *default-logging
    network_mode: "service:gluetun"
    command: --port 80 --ipv6-mode false
    depends_on:
      - aria2
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.1'
          memory: 64M

  # slskd - Soulseek P2P client for music sharing
  slskd:
    image: ghcr.io/slskd/slskd:${SLSKD_TAG:-latest}
    container_name: slskd
    logging: *default-logging
    network_mode: "service:gluetun"
    entrypoint: sh /init.sh
    environment:
      <<: *common-env
      SLSKD_HTTP_PORT: 2234
      SLSKD_SLSK_LISTEN_PORT: 50000
      SLSKD_USERNAME: ${SLSKD_USER:-admin}
      SLSKD_PASSWORD: ${SLSKD_PASSWORD}
      SLSKD_SOULSEEK_USERNAME: ${SLSKD_SOULSEEK_USERNAME}
      SLSKD_SOULSEEK_PASSWORD: ${SLSKD_SOULSEEK_PASSWORD}
      SLSKD_METRICS: "false"
      SLSKD_API_KEY: ${SLSKD_API_KEY}
    volumes:
      - slskd-config:/app
      - /mnt/cachehdd/slskd/logs:/app/logs
      - /mnt/storage/slskd-shared:/var/slskd/shared
      - /mnt/cachehdd/slskd-incomplete:/var/slskd/incomplete
      - ./slskd-init.sh:/init.sh:ro
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://127.0.0.1:2234/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    labels:
      - "autoheal=true"

  ################################################################################
  # PHOTO MANAGEMENT
  ################################################################################

  # Immich - Photo management and AI tagging
  immich-postgres:
    image: tensorchord/pgvecto-rs:${IMMICH_POSTGRES_TAG:-pg16-v0.2.0}
    container_name: immich-postgres
    logging: *default-logging
    shm_size: 128mb
    command:
      - "postgres"
      - "-c"
      - "shared_buffers=128MB"
      - "-c"
      - "effective_cache_size=384MB"
      - "-c"
      - "work_mem=8MB"
      - "-c"
      - "maintenance_work_mem=64MB"
      - "-c"
      - "max_connections=50"
    environment:
      POSTGRES_USER: immich
      POSTGRES_PASSWORD: ${IMMICH_DB_PASSWORD}
      POSTGRES_DB: immich
    volumes:
      - immich-postgres:/var/lib/postgresql/data
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U immich"]
      interval: 30s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M

  immich-server:
    image: ghcr.io/immich-app/immich-server:${IMMICH_TAG:-release}
    container_name: immich-server
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:2283:3001"
    environment:
      <<: *common-env
      DB_HOSTNAME: immich-postgres
      DB_USERNAME: immich
      DB_PASSWORD: ${IMMICH_DB_PASSWORD}
      DB_DATABASE_NAME: immich
      REDIS_HOSTNAME: redis-cache
      REDIS_DBINDEX: 0
      UPLOAD_LOCATION: /usr/src/app/upload
    volumes:
      - /mnt/storage/photos:/usr/src/app/upload
      - /etc/localtime:/etc/localtime:ro
    networks:
      - potatostack
    depends_on:
      - immich-postgres
      - redis-cache
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G

  immich-machine-learning:
    image: ghcr.io/immich-app/immich-machine-learning:${IMMICH_TAG:-release}
    container_name: immich-ml
    logging: *default-logging
    environment:
      <<: *common-env
    volumes:
      - /mnt/cachehdd/immich-ml-cache:/cache
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  ################################################################################
  # MONITORING & OBSERVABILITY
  ################################################################################

  # Loki - Log aggregation
  loki:
    image: grafana/loki:${LOKI_TAG:-latest}
    container_name: loki
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:3100:3100"
    command: -config.file=/etc/loki/loki.yml
    volumes:
      - ./config/loki:/etc/loki
      - /mnt/cachehdd/loki/data:/loki
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M

  # Promtail - Log collector
  promtail:
    image: grafana/promtail:${PROMTAIL_TAG:-latest}
    container_name: promtail
    logging: *default-logging
    command: -config.file=/etc/promtail/promtail.yml
    volumes:
      - ./config/promtail:/etc/promtail
      - /var/log:/var/log:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - potatostack
    depends_on:
      - loki
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M

  # Fritzbox Exporter - Router metrics
  fritzbox-exporter:
    image: pdreker/fritz_exporter:${FRITZBOX_EXPORTER_TAG:-latest}
    container_name: fritzbox-exporter
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:9787:9787"
    environment:
      FRITZ_USERNAME: ${FRITZ_USERNAME:-}
      FRITZ_PASSWORD: ${FRITZ_PASSWORD}
      FRITZ_HOSTNAME: ${FRITZ_HOSTNAME:-fritz.box}
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M

  # Beszel - Lightweight monitoring for systems and Docker (SOTA 2025)
  beszel:
    image: henrygd/beszel:${BESZEL_TAG:-latest}
    container_name: beszel
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8090:8090"
    environment:
      <<: *common-env
    volumes:
      - beszel-data:/app/beszel_data
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:8090/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M
        reservations:
          cpus: '0.1'
          memory: 64M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.beszel.rule=Host(`monitor.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.beszel.entrypoints=websecure"
      - "traefik.http.routers.beszel.tls=true"
      - "traefik.http.services.beszel.loadbalancer.server.port=8090"

  # Beszel Agent - Monitoring agent for remote systems
  beszel-agent:
    image: henrygd/beszel-agent:${BESZEL_TAG:-latest}
    container_name: beszel-agent
    logging: *default-logging
    environment:
      PORT: 45876
      KEY: ${BESZEL_AGENT_KEY}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.1'
          memory: 64M

  # Uptime Kuma - Uptime monitoring
  uptime-kuma:
    image: louislam/uptime-kuma:${UPTIME_KUMA_TAG:-latest}
    container_name: uptime-kuma
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:3001:3001"
    environment:
      <<: *common-env
    volumes:
      - uptime-kuma-data:/app/data
    networks:
      - potatostack
    restart: unless-stopped
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.uptime.rule=Host(`uptime.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.uptime.entrypoints=websecure"
      - "traefik.http.routers.uptime.tls=true"
      - "traefik.http.services.uptime.loadbalancer.server.port=3001"
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M

  ################################################################################
  # AUTOMATION & WORKFLOWS
  ################################################################################

  # n8n - Workflow automation
  n8n:
    image: n8nio/n8n:${N8N_TAG:-latest}
    container_name: n8n
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:5678:5678"
    environment:
      <<: *common-env
      N8N_BASIC_AUTH_ACTIVE: "true"
      N8N_BASIC_AUTH_USER: ${N8N_USER:-admin}
      N8N_BASIC_AUTH_PASSWORD: ${N8N_PASSWORD}
      N8N_HOST: n8n.${HOST_DOMAIN:-local.domain}
      N8N_PORT: 5678
      N8N_PROTOCOL: https
      WEBHOOK_URL: https://n8n.${HOST_DOMAIN:-local.domain}/
      GENERIC_TIMEZONE: Europe/Berlin
      DB_TYPE: postgresdb
      DB_POSTGRESDB_HOST: postgres
      DB_POSTGRESDB_PORT: 5432
      DB_POSTGRESDB_DATABASE: n8n
      DB_POSTGRESDB_USER: postgres
      DB_POSTGRESDB_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
      EXECUTIONS_MODE: queue
      QUEUE_BULL_REDIS_HOST: redis-cache
      QUEUE_BULL_REDIS_PORT: 6379
    volumes:
      - n8n-data:/home/node/.n8n
    networks:
      - potatostack
    depends_on:
      - postgres
      - redis-cache
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M

  # Healthchecks - Cron monitoring
  healthchecks:
    image: lscr.io/linuxserver/healthchecks:${HEALTHCHECKS_TAG:-latest}
    container_name: healthchecks
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8001:8000"
    environment:
      <<: *common-env
      SITE_ROOT: https://healthchecks.${HOST_DOMAIN:-local.domain}
      SITE_NAME: PotatoStack Healthchecks
      SUPERUSER_EMAIL: ${HEALTHCHECKS_ADMIN_EMAIL}
      SUPERUSER_PASSWORD: ${HEALTHCHECKS_ADMIN_PASSWORD}
      SECRET_KEY: ${HEALTHCHECKS_SECRET_KEY}
      DB: postgres
      DB_HOST: postgres
      DB_PORT: 5432
      DB_NAME: healthchecks
      DB_USER: postgres
      DB_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
    volumes:
      - healthchecks-data:/config
    networks:
      - potatostack
    depends_on:
      - postgres
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M

  ################################################################################
  # UTILITIES & TOOLS
  ################################################################################

  # Rustypaste - Pastebin
  rustypaste:
    image: orhunp/rustypaste:${RUSTYPASTE_TAG:-latest}
    container_name: rustypaste
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8085:8000"
    environment:
      <<: *common-env
    volumes:
      - rustypaste-data:/app/data
      - ./config/rustypaste/config.toml:/app/config.toml:ro
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M

  # Stirling PDF - PDF tools
  stirling-pdf:
    image: frooodle/s-pdf:${STIRLING_PDF_TAG:-latest}
    container_name: stirling-pdf
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8086:8080"
    environment:
      <<: *common-env
      DOCKER_ENABLE_SECURITY: "false"
      INSTALL_BOOK_AND_ADVANCED_HTML_OPS: "true"
      LANGS: de_DE,en_US
    volumes:
      - stirling-pdf-data:/usr/share/tessdata
      - stirling-pdf-configs:/configs
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.75'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 512M

  # Linkding - Bookmark manager
  linkding:
    image: sissbruecker/linkding:${LINKDING_TAG:-latest}
    container_name: linkding
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:9091:9090"
    environment:
      <<: *common-env
      LD_DB_ENGINE: postgres
      LD_DB_HOST: postgres
      LD_DB_PORT: 5432
      LD_DB_DATABASE: linkding
      LD_DB_USER: postgres
      LD_DB_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
      LD_SUPERUSER_NAME: ${LINKDING_ADMIN_USER:-admin}
      LD_SUPERUSER_PASSWORD: ${LINKDING_ADMIN_PASSWORD}
    volumes:
      - linkding-data:/etc/linkding/data
    networks:
      - potatostack
    depends_on:
      - postgres
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M

  # Cal.com - Calendar scheduling
  calcom:
    image: calcom/cal.com:${CALCOM_TAG:-latest}
    container_name: calcom
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:3003:3000"
    environment:
      <<: *common-env
      DATABASE_URL: postgresql://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/calcom
      NEXT_PUBLIC_WEBAPP_URL: https://cal.${HOST_DOMAIN:-local.domain}
      NEXTAUTH_SECRET: ${CALCOM_NEXTAUTH_SECRET}
      CALENDSO_ENCRYPTION_KEY: ${CALCOM_ENCRYPTION_KEY}
    networks:
      - potatostack
    depends_on:
      - postgres
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M

  # Code Server - VS Code in browser
  code-server:
    image: lscr.io/linuxserver/code-server:${CODE_SERVER_TAG:-latest}
    container_name: code-server
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8443:8443"
    environment:
      <<: *common-env
      PASSWORD: ${CODE_SERVER_PASSWORD}
      SUDO_PASSWORD: ${CODE_SERVER_SUDO_PASSWORD}
    volumes:
      - code-server-config:/config
      - /mnt/storage/projects:/config/workspace
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 1G

  # Excalidraw - Sketching tool
  excalidraw:
    image: excalidraw/excalidraw:${EXCALIDRAW_TAG:-latest}
    container_name: excalidraw
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8088:80"
    environment:
      <<: *common-env
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M

  # Atuin - Shell history sync
  atuin:
    image: ghcr.io/atuinsh/atuin:${ATUIN_TAG:-latest}
    container_name: atuin
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8889:8888"
    environment:
      <<: *common-env
      ATUIN_DB_URI: postgres://postgres:${POSTGRES_SUPER_PASSWORD}@postgres/atuin
    command: server start
    volumes:
      - atuin-config:/config
    networks:
      - potatostack
    depends_on:
      - postgres
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M

  # IT-Tools - Collection of handy online tools (SOTA 2025)
  it-tools:
    image: corentinth/it-tools:${IT_TOOLS_TAG:-latest}
    container_name: it-tools
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8091:80"
    environment:
      <<: *common-env
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:80/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.ittools.rule=Host(`tools.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.ittools.entrypoints=websecure"
      - "traefik.http.routers.ittools.tls=true"
      - "traefik.http.services.ittools.loadbalancer.server.port=80"

  # Memos - Lightweight note-taking (SOTA 2025)
  memos:
    image: neosmemo/memos:${MEMOS_TAG:-latest}
    container_name: memos
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:5230:5230"
    environment:
      <<: *common-env
    volumes:
      - memos-data:/var/opt/memos
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:5230/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.memos.rule=Host(`memos.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.memos.entrypoints=websecure"
      - "traefik.http.routers.memos.tls=true"
      - "traefik.http.services.memos.loadbalancer.server.port=5230"

  # Paperless-ngx - Document management system (SOTA 2025)
  paperless-ngx:
    image: ghcr.io/paperless-ngx/paperless-ngx:${PAPERLESS_TAG:-latest}
    container_name: paperless-ngx
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8092:8000"
    environment:
      <<: *common-env
      PAPERLESS_REDIS: redis://redis-cache:6379/1
      PAPERLESS_DBHOST: postgres
      PAPERLESS_DBNAME: paperless
      PAPERLESS_DBUSER: postgres
      PAPERLESS_DBPASS: ${POSTGRES_SUPER_PASSWORD}
      PAPERLESS_URL: https://docs.${HOST_DOMAIN:-local.domain}
      PAPERLESS_SECRET_KEY: ${PAPERLESS_SECRET_KEY}
      PAPERLESS_ADMIN_USER: ${PAPERLESS_ADMIN_USER:-admin}
      PAPERLESS_ADMIN_PASSWORD: ${PAPERLESS_ADMIN_PASSWORD}
      PAPERLESS_OCR_LANGUAGE: deu+eng
      PAPERLESS_TIME_ZONE: Europe/Berlin
    volumes:
      - paperless-data:/usr/src/paperless/data
      - paperless-media:/usr/src/paperless/media
      - /mnt/storage/paperless/consume:/usr/src/paperless/consume
      - /mnt/storage/paperless/export:/usr/src/paperless/export
    networks:
      - potatostack
    depends_on:
      - postgres
      - redis-cache
      - storage-init
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:8000/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.paperless.rule=Host(`docs.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.paperless.entrypoints=websecure"
      - "traefik.http.routers.paperless.tls=true"
      - "traefik.http.services.paperless.loadbalancer.server.port=8000"

  # Pingvin Share - Secure file sharing (SOTA 2025)
  pingvin-share:
    image: stonith404/pingvin-share:${PINGVIN_TAG:-latest}
    container_name: pingvin-share
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:3007:3000"
    environment:
      <<: *common-env
      DATABASE_URL: postgresql://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/pingvin
    volumes:
      - pingvin-data:/opt/app/backend/data
    networks:
      - potatostack
    depends_on:
      - postgres
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:3000/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.pingvin.rule=Host(`share.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.pingvin.entrypoints=websecure"
      - "traefik.http.routers.pingvin.tls=true"
      - "traefik.http.services.pingvin.loadbalancer.server.port=3000"

  ################################################################################
  # DEVELOPMENT & GIT
  ################################################################################

  # Gitea - Git hosting
  gitea:
    image: gitea/gitea:${GITEA_TAG:-latest}
    container_name: gitea
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:3004:3000"
      - "${HOST_BIND:-192.168.178.40}:2222:22"
    environment:
      <<: *common-env
      GITEA__database__DB_TYPE: postgres
      GITEA__database__HOST: postgres:5432
      GITEA__database__NAME: gitea
      GITEA__database__USER: postgres
      GITEA__database__PASSWD: ${POSTGRES_SUPER_PASSWORD}
      GITEA__server__DOMAIN: git.${HOST_DOMAIN:-local.domain}
      GITEA__server__ROOT_URL: https://git.${HOST_DOMAIN:-local.domain}/
      GITEA__server__SSH_DOMAIN: git.${HOST_DOMAIN:-local.domain}
      GITEA__security__INSTALL_LOCK: "true"
      GITEA__cache__ENABLED: "true"
      GITEA__cache__ADAPTER: redis
      GITEA__cache__HOST: redis://redis-cache:6379/0
      GITEA__session__PROVIDER: redis
      GITEA__session__PROVIDER_CONFIG: redis-cache:6379
      GITEA__queue__TYPE: redis
      GITEA__queue__CONN_STR: redis://redis-cache:6379/1
    volumes:
      - gitea-data:/data
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    networks:
      - potatostack
    depends_on:
      - postgres
      - redis-cache
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M

  # Gitea Runner - CI/CD for Gitea
  gitea-runner:
    image: gitea/act_runner:${GITEA_RUNNER_TAG:-latest}
    container_name: gitea-runner
    logging: *default-logging
    environment:
      <<: *common-env
      GITEA_INSTANCE_URL: http://gitea:3000
      GITEA_RUNNER_REGISTRATION_TOKEN: ${GITEA_RUNNER_TOKEN}
      GITEA_RUNNER_NAME: docker-runner
    volumes:
      - gitea-runner-data:/data
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - potatostack
    depends_on:
      - gitea
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M

  # Sentry - Error tracking
  sentry-postgres:
    image: postgres:${POSTGRES_TAG:-16-alpine}
    container_name: sentry-postgres
    logging: *default-logging
    environment:
      POSTGRES_USER: sentry
      POSTGRES_PASSWORD: ${SENTRY_DB_PASSWORD}
      POSTGRES_DB: sentry
    volumes:
      - sentry-postgres:/var/lib/postgresql/data
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U sentry"]
      interval: 30s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M

  sentry:
    image: sentry:${SENTRY_TAG:-latest}
    container_name: sentry
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:9092:9000"
    environment:
      <<: *common-env
      SENTRY_SECRET_KEY: ${SENTRY_SECRET_KEY}
      SENTRY_POSTGRES_HOST: sentry-postgres
      SENTRY_DB_USER: sentry
      SENTRY_DB_PASSWORD: ${SENTRY_DB_PASSWORD}
      SENTRY_REDIS_HOST: redis-cache
      SENTRY_REDIS_DB: 2
    volumes:
      - sentry-data:/var/lib/sentry/files
    networks:
      - potatostack
    depends_on:
      - sentry-postgres
      - redis-cache
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M

  ################################################################################
  # AI & SPECIAL APPLICATIONS
  ################################################################################

  # Open WebUI - LLM interface
  open-webui:
    image: ghcr.io/open-webui/open-webui:${OPEN_WEBUI_TAG:-main}
    container_name: open-webui
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:3005:8080"
    environment:
      <<: *common-env
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      WEBUI_SECRET_KEY: ${OPEN_WEBUI_SECRET_KEY}
    volumes:
      - open-webui-data:/app/backend/data
    networks:
      - potatostack
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 1G

  # OctoBot - AI crypto trading
  octobot:
    image: drakkarsoftware/octobot:${OCTOBOT_TAG:-latest}
    container_name: octobot
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:5001:5001"
    environment:
      <<: *common-env
      CONFIG_PATH: /octobot/user/config.json
    volumes:
      - octobot-data:/octobot/user
      - octobot-tentacles:/octobot/tentacles
      - octobot-logs:/octobot/logs
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M

  # Pinchflat - YouTube downloader
  pinchflat:
    image: ghcr.io/kieraneglin/pinchflat:${PINCHFLAT_TAG:-latest}
    container_name: pinchflat
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8945:8945"
    environment:
      <<: *common-env
    volumes:
      - pinchflat-config:/config
      - /mnt/storage/media/youtube:/downloads
      - /mnt/cachehdd/pinchflat-incomplete:/incomplete
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M

  ################################################################################
  # DASHBOARD & CONTAINER MANAGEMENT
  ################################################################################

  # Homarr - Modern drag-and-drop dashboard with 30+ integrations (SOTA 2025)
  homarr:
    image: ghcr.io/homarr-labs/homarr:${HOMARR_TAG:-latest}
    container_name: homarr
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:7575:7575"
    environment:
      <<: *common-env
      SECRET_ENCRYPTION_KEY: ${HOMARR_SECRET_KEY}
      DATABASE_URL: postgresql://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/homarr
    volumes:
      - homarr-data:/app/data/configs
      - homarr-icons:/app/public/icons
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - potatostack
    depends_on:
      - postgres
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:7575/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.homarr.rule=Host(`home.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.homarr.entrypoints=websecure"
      - "traefik.http.routers.homarr.tls=true"
      - "traefik.http.services.homarr.loadbalancer.server.port=7575"

  # Dockge - Modern Docker Compose stack manager (SOTA 2025)
  dockge:
    image: louislam/dockge:${DOCKGE_TAG:-latest}
    container_name: dockge
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:5001:5001"
    environment:
      <<: *common-env
      DOCKGE_STACKS_DIR: /opt/stacks
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - dockge-data:/app/data
      - /opt/stacks:/opt/stacks
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:5001/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.dockge.rule=Host(`dockge.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.dockge.entrypoints=websecure"
      - "traefik.http.routers.dockge.tls=true"
      - "traefik.http.services.dockge.loadbalancer.server.port=5001"

  ################################################################################
  # ENTERPRISE SECURITY & SECRETS MANAGEMENT
  ################################################################################

  # HashiCorp Vault - Enterprise secrets management
  vault:
    image: hashicorp/vault:${VAULT_TAG:-latest}
    container_name: vault
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8200:8200"
    environment:
      VAULT_ADDR: http://0.0.0.0:8200
      VAULT_API_ADDR: http://0.0.0.0:8200
      VAULT_LOCAL_CONFIG: '{"backend":{"file":{"path":"/vault/data"}},"default_lease_ttl":"168h","max_lease_ttl":"720h","ui":true,"listener":{"tcp":{"address":"0.0.0.0:8200","tls_disable":1}}}'
    volumes:
      - vault-data:/vault/data
      - vault-logs:/vault/logs
    cap_add:
      - IPC_LOCK
    command: server
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "vault", "status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.vault.rule=Host(`vault.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.vault.entrypoints=websecure"
      - "traefik.http.routers.vault.tls=true"
      - "traefik.http.services.vault.loadbalancer.server.port=8200"

  # Fail2Ban - Intrusion prevention
  fail2ban:
    image: crazymax/fail2ban:${FAIL2BAN_TAG:-latest}
    container_name: fail2ban
    logging: *default-logging
    network_mode: host
    cap_add:
      - NET_ADMIN
      - NET_RAW
    environment:
      <<: *common-env
      F2B_DB_PURGE_AGE: 30d
      F2B_LOG_LEVEL: INFO
      F2B_IPTABLES_CHAIN: DOCKER-USER
    volumes:
      - fail2ban-data:/data
      - /var/log:/var/log:ro
      - traefik-logs:/var/log/traefik:ro
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M
        reservations:
          cpus: '0.1'
          memory: 64M

  # Trivy - Vulnerability scanner
  trivy:
    image: aquasec/trivy:${TRIVY_TAG:-latest}
    container_name: trivy
    logging: *default-logging
    command: ["server", "--listen", "0.0.0.0:8081"]
    ports:
      - "${HOST_BIND:-192.168.178.40}:8081:8081"
    environment:
      <<: *common-env
      TRIVY_LISTEN: "0.0.0.0:8081"
      TRIVY_CACHE_DIR: /root/.cache/trivy
    volumes:
      - trivy-cache:/root/.cache/
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M

  # Alertmanager - Alert routing and management
  alertmanager:
    image: prom/alertmanager:${ALERTMANAGER_TAG:-latest}
    container_name: alertmanager
    logging: *default-logging
    command:
      - '--config.file=/etc/alertmanager/config.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://alertmanager.${HOST_DOMAIN:-local.domain}'
    environment:
      <<: *common-env
    volumes:
      - ./config/alertmanager:/etc/alertmanager:ro
      - alertmanager-data:/alertmanager
    ports:
      - "${HOST_BIND:-192.168.178.40}:9093:9093"
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://127.0.0.1:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M
        reservations:
          cpus: '0.1'
          memory: 64M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.alertmanager.rule=Host(`alerts.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.alertmanager.entrypoints=websecure"
      - "traefik.http.routers.alertmanager.tls=true"
      - "traefik.http.services.alertmanager.loadbalancer.server.port=9093"

  ################################################################################
  # SYSTEM UTILITIES
  ################################################################################

  # Diun - Docker Image Update Notifier (SOTA 2025 best practice)
  # Monitors for updates but doesn't auto-update, giving full control
  diun:
    image: crazymax/diun:${DIUN_TAG:-latest}
    container_name: diun
    logging: *default-logging
    command: serve
    environment:
      <<: *common-env
      LOG_LEVEL: info
      LOG_JSON: "false"
      DIUN_WATCH_WORKERS: 20
      DIUN_WATCH_SCHEDULE: "0 */6 * * *"
      DIUN_WATCH_JITTER: 30s
      DIUN_PROVIDERS_DOCKER: "true"
      DIUN_PROVIDERS_DOCKER_WATCHBYDEFAULT: "true"
      DIUN_NOTIF_GOTIFY_ENDPOINT: ${DIUN_GOTIFY_ENDPOINT:-}
      DIUN_NOTIF_GOTIFY_TOKEN: ${DIUN_GOTIFY_TOKEN:-}
      DIUN_NOTIF_DISCORD_WEBHOOKURL: ${DIUN_DISCORD_WEBHOOK:-}
      DIUN_NOTIF_TELEGRAM_TOKEN: ${DIUN_TELEGRAM_TOKEN:-}
      DIUN_NOTIF_TELEGRAM_CHATIDS: ${DIUN_TELEGRAM_CHATIDS:-}
    volumes:
      - diun-data:/data
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M

  # Autoheal - Container health recovery
  autoheal:
    image: willfarrell/autoheal:${AUTOHEAL_TAG:-latest}
    container_name: autoheal
    logging: *default-logging
    environment:
      AUTOHEAL_CONTAINER_LABEL: autoheal
      AUTOHEAL_INTERVAL: 60
      AUTOHEAL_START_PERIOD: 300
      AUTOHEAL_DEFAULT_STOP_TIMEOUT: 10
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.1'
          memory: 64M
        reservations:
          cpus: '0.05'
          memory: 32M

  ################################################################################
  # Gluetun Monitor - VPN Connection Monitor & Auto-Restart
  ################################################################################
  gluetun-monitor:
    image: alpine:${ALPINE_TAG:-3.21}
    container_name: gluetun-monitor
    logging: *default-logging
    user: "0:0"
    command: sh /monitor.sh
    environment:
      - GLUETUN_URL=http://gluetun:8000
      - CHECK_INTERVAL=${GLUETUN_CHECK_INTERVAL:-10}
      - RESTART_CONTAINERS=prowlarr sonarr radarr lidarr readarr bazarr qbittorrent aria2 ariang slskd
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./gluetun-monitor.sh:/monitor.sh:ro
    networks:
      - potatostack
    depends_on:
      gluetun:
        condition: service_started
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.05"
          memory: 16M
        reservations:
          cpus: '0.025'
          memory: 8M
  # Kopia - Fast and secure backup server
  kopia:
    image: kopia/kopia:${KOPIA_TAG:-latest}
    container_name: kopia
    logging: *default-logging
    hostname: kopia-server
    ports:
      - "${HOST_BIND:-192.168.178.40}:51515:51515"
    environment:
      <<: *common-env
      KOPIA_PASSWORD: ${KOPIA_PASSWORD}
      KOPIA_SERVER_USER: ${KOPIA_SERVER_USER:-admin}
      KOPIA_SERVER_PASSWORD: ${KOPIA_SERVER_PASSWORD}
      KOPIA_CONFIG_PATH: /app/config/repository.config
      KOPIA_CACHE_DIRECTORY: /app/cache
      KOPIA_LOG_DIR: /app/logs
    command:
      - server
      - start
      - --address=0.0.0.0:51515
      - --tls-generate-cert
      - --tls-generate-rsa-key-size=2048
      - --server-username=${KOPIA_SERVER_USER:-admin}
      - --server-password=${KOPIA_SERVER_PASSWORD}
      - --log-level=warn
      - --file-log-level=warn
      - --override-hostname=kopia-server
      - --override-username=all-users
    volumes:
      - /mnt/storage/kopia/repository:/repository
      - kopia-config:/app/config
      - /mnt/cachehdd/kopia-cache:/app/cache
      - kopia-logs:/app/logs
      # Backup targets
      - vaultwarden-data:/data/vaultwarden:ro
      - /mnt/storage/syncthing:/data/syncthing:ro
      - /mnt/storage/downloads:/data/downloads:ro
      - /mnt/storage/slskd-shared:/data/slskd-shared:ro
      - /mnt/storage/photos:/data/photos:ro
      - /mnt/storage/media:/data/media:ro
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-fk", "https://127.0.0.1:51515/api/v1/repo/status"]
      interval: 120s
      timeout: 20s
      retries: 3
      start_period: 60s

################################################################################
# NETWORKS
################################################################################
networks:
  potatostack:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: br-potato-main
    ipam:
      config:
        - subnet: 172.22.0.0/16

################################################################################
# VOLUMES
################################################################################
volumes:
  # Databases
  postgres-data:
  mongo-data:
  mongo-config:
  redis-cache-data:

  # Authentication
  authentik-postgres:
  authentik-media:
  authentik-custom-templates:
  authentik-certs:
  vaultwarden-data:

  # Reverse Proxy
  traefik-certs:

  # VPN
  gluetun-config:
  tailscale-data:

  # Cloud Storage
  nextcloud-aio-mastercontainer:
  syncthing-config:

  # Media Management
  prowlarr-config:
  sonarr-config:
  radarr-config:
  lidarr-config:
  readarr-config:
  bazarr-config:
  maintainerr-data:
  jellyfin-config:
  jellyseerr-config:
  audiobookshelf-config:

  # Downloads
  qbittorrent-config:
  aria2-config:
  slskd-config:

  # Photos
  immich-postgres:

  # Monitoring
  uptime-kuma-data:

  # Automation
  n8n-data:
  healthchecks-data:

  # Utilities
  rustypaste-data:
  stirling-pdf-data:
  stirling-pdf-configs:
  linkding-data:
  code-server-config:
  atuin-config:

  # Development
  gitea-data:
  gitea-runner-data:
  sentry-postgres:
  sentry-data:

  # AI & Special
  open-webui-data:
  octobot-data:
  octobot-tentacles:
  octobot-logs:
  pinchflat-config:

  # Dashboard & Container Management
  homarr-data:
  homarr-icons:
  dockge-data:

  # Security
  crowdsec-db:
  crowdsec-config:
  traefik-logs:

  # DNS & Ad Blocking
  adguard-work:
  adguard-conf:

  # Monitoring
  beszel-data:

  # Finance
  actual-data:

  # Utilities
  paperless-data:
  paperless-media:
  pingvin-data:
  memos-data:

  # System
  diun-data:

  # Backups
  kopia-config:
  kopia-logs:

  # Enterprise Security & Secrets
  vault-data:
  vault-logs:
  fail2ban-data:
  trivy-cache:

  # Enterprise Observability
  alertmanager-data:
