################################################################################
# PotatoStack Main - Optimized for Mini PC (16GB RAM)
# Full-featured self-hosted stack with monitoring, automation, and media
# Target: Mini PC with 16GB RAM, 4+ core CPU, 1GB ethernet
################################################################################

x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"
    compress: "true"

x-common-env: &common-env
  TZ: Europe/Berlin
  PUID: 1000
  PGID: 1000

services:
  ################################################################################
  # Storage Init - Creates required directories on startup
  ################################################################################
  storage-init:
    image: alpine:${ALPINE_TAG:-latest}
    container_name: storage-init
    command: sh /init-storage.sh
    privileged: true
    environment:
      <<: *common-env
      SLSKD_API_KEY: ${SLSKD_API_KEY:-}
      SYNCTHING_API_KEY: ${SYNCTHING_API_KEY:-}
      ARIA2_RPC_SECRET: ${ARIA2_RPC_SECRET:-}
      SNAPSHOT_CRON_SCHEDULE: ${SNAPSHOT_CRON_SCHEDULE:-0 3 * * *}
      SNAPSHOT_PATHS: ${SNAPSHOT_PATHS:-/data}
      SNAPSHOT_LOG_FILE: ${SNAPSHOT_LOG_FILE:-/mnt/storage/kopia/stack-snapshot.log}
    volumes:
      - /mnt/storage:/mnt/storage
      - /mnt/cachehdd:/mnt/cachehdd
      - /mnt/ssd/docker-data:/mnt/ssd/docker-data
      - ./init-storage.sh:/init-storage.sh:ro
      - ./stack-snapshot.sh:/stack-snapshot.sh:ro
      - shared-keys:/keys
    network_mode: none
    restart: "no"

  ################################################################################
  # CORE DATABASES
  ################################################################################

  # PostgreSQL - Primary database with pgvector for embeddings (SOTA 2025)
  postgres:
    image: pgvector/pgvector:${POSTGRES_TAG:-pg16-alpine}
    container_name: postgres
    logging: *default-logging
    shm_size: 1gb
    environment:
      <<: *common-env
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
      POSTGRES_MULTIPLE_DATABASES: ${POSTGRES_DATABASES:-nextcloud,authentik,gitea,woodpecker,immich,calibre,linkding,n8n,healthchecks,stirlingpdf,atuin,homarr,paperless,openwebui,miniflux,grafana}
    command:
      - "postgres"
      - "-c"
      - "shared_buffers=1GB"
      - "-c"
      - "effective_cache_size=3GB"
      - "-c"
      - "work_mem=16MB"
      - "-c"
      - "maintenance_work_mem=256MB"
      - "-c"
      - "max_connections=100"
      - "-c"
      - "checkpoint_completion_target=0.9"
      - "-c"
      - "wal_buffers=16MB"
      - "-c"
      - "default_statistics_target=100"
      - "-c"
      - "random_page_cost=1.1"
      - "-c"
      - "effective_io_concurrency=200"
      - "-c"
      - "max_worker_processes=8"
      - "-c"
      - "max_parallel_workers=4"
      - "-c"
      - "max_parallel_workers_per_gather=2"
    volumes:
      - /mnt/ssd/docker-data/postgres:/var/lib/postgresql/data
      - ./scripts/init-postgres-multiple-dbs.sh:/docker-entrypoint-initdb.d/init-postgres-multiple-dbs.sh:ro
    tmpfs:
      - /tmp:exec,mode=1777
      - /var/run/postgresql:exec,mode=1777
    networks:
      - potatostack
    depends_on:
      - authentik-server
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 512M
        reservations:
          cpus: "0.5"
          memory: 512M

  # PgBouncer - PostgreSQL connection pooling (SOTA 2025)
  pgbouncer:
    image: edoburu/pgbouncer:${PGBOUNCER_TAG:-latest}
    container_name: pgbouncer
    logging: *default-logging
    environment:
      DATABASE_URL: postgres://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/postgres
      POOL_MODE: transaction
      DEFAULT_POOL_SIZE: 50
      MAX_CLIENT_CONN: 200
      MAX_DB_CONNECTIONS: 100
    networks:
      - potatostack
    depends_on:
      - postgres
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "pg_isready", "-h", "127.0.0.1", "-p", "5432"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 64M

  # MongoDB - Document database (SOTA 2025)
  mongo:
    image: mongo:${MONGO_TAG:-7-jammy}
    container_name: mongo
    logging: *default-logging
    command:
      - "mongod"
      - "--wiredTigerCacheSizeGB=1.5"
      - "--wiredTigerJournalCompressor=snappy"
      - "--bind_ip_all"
    environment:
      <<: *common-env
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_ROOT_PASSWORD}
    volumes:
      - /mnt/ssd/docker-data/mongo:/data/db
      - /mnt/ssd/docker-data/mongo-config:/data/configdb
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 512M
        reservations:
          cpus: "0.5"
          memory: 512M

  # Redis Shared Cache - SOTA 2025 consolidated (N8n, Gitea, Immich, Paperless, Sentry)
  redis-cache:
    image: redis:${REDIS_TAG:-7-alpine}
    container_name: redis-cache
    logging: *default-logging
    command: redis-server --maxmemory 1gb --maxmemory-policy allkeys-lfu --databases 16 --activedefrag yes --lazyfree-lazy-eviction yes --lazyfree-lazy-expire yes --lazyfree-lazy-server-del yes --save 60 1000 --appendonly yes --appendfsync everysec
    volumes:
      - /mnt/ssd/docker-data/redis-cache:/data
    tmpfs:
      - /tmp:exec,mode=1777
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 512M

  ################################################################################
  # SECURITY & INTRUSION PREVENTION
  ################################################################################

  # CrowdSec - Modern IPS/IDS with community threat intelligence (SOTA 2025)
  crowdsec:
    image: crowdsecurity/crowdsec:${CROWDSEC_TAG:-latest}
    container_name: crowdsec
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:6060:6060" # Prometheus metrics
    environment:
      <<: *common-env
      COLLECTIONS: "crowdsecurity/traefik crowdsecurity/http-cve crowdsecurity/whitelist-good-actors crowdsecurity/nginx crowdsecurity/linux"
      GID: "1000"
      METRICS_ENABLED: "true"
    volumes:
      - /mnt/ssd/docker-data/crowdsec-db:/var/lib/crowdsec/data/
      - /mnt/ssd/docker-data/crowdsec-config:/etc/crowdsec/
      - /var/log:/var/log:ro
      - traefik-logs:/var/log/traefik:ro
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "cscli", "version"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 128M

  # CrowdSec Traefik Bouncer - Blocks malicious IPs at reverse proxy level
  crowdsec-traefik-bouncer:
    image: fbonalair/traefik-crowdsec-bouncer:${CROWDSEC_BOUNCER_TAG:-latest}
    container_name: crowdsec-traefik-bouncer
    logging: *default-logging
    environment:
      CROWDSEC_BOUNCER_API_KEY: ${CROWDSEC_BOUNCER_KEY}
      CROWDSEC_AGENT_HOST: crowdsec:8080
      GIN_MODE: release
    networks:
      - potatostack
    depends_on:
      - crowdsec
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M

  ################################################################################
  # REVERSE PROXY & SSL
  ################################################################################

  # Traefik - Modern reverse proxy with automatic SSL (SOTA 2025)
  traefik:
    image: traefik:${TRAEFIK_TAG:-latest}
    container_name: traefik
    logging: *default-logging
    security_opt:
      - no-new-privileges:true
    ports:
      - "80:80"
      - "443:443"
      - "${HOST_BIND:-192.168.178.40}:8080:8080" # Dashboard
    environment:
      <<: *common-env
    command:
      # EntryPoints
      - "--entrypoints.web.address=:80"
      - "--entrypoints.web.http.redirections.entrypoint.to=websecure"
      - "--entrypoints.web.http.redirections.entrypoint.scheme=https"
      - "--entrypoints.websecure.address=:443"
      - "--entrypoints.websecure.http.tls=true"
      # Providers
      - "--providers.docker=true"
      - "--providers.docker.exposedbydefault=false"
      - "--providers.docker.network=potatostack"
      # API & Dashboard
      - "--api.dashboard=true"
      - "--api.insecure=false"
      # SSL/TLS
      - "--certificatesresolvers.letsencrypt.acme.email=${ACME_EMAIL}"
      - "--certificatesresolvers.letsencrypt.acme.storage=/letsencrypt/acme.json"
      - "--certificatesresolvers.letsencrypt.acme.httpchallenge.entrypoint=web"
      # Observability
      - "--log.level=INFO"
      - "--accesslog=true"
      - "--metrics.prometheus=true"
      - "--metrics.prometheus.addServicesLabels=true"
      - "--metrics.prometheus.addEntryPointsLabels=true"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - traefik-certs:/letsencrypt
    networks:
      - potatostack
    restart: unless-stopped
    labels:
      - "traefik.enable=true"
      # Dashboard router
      - "traefik.http.routers.traefik.rule=Host(`traefik.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.traefik.entrypoints=websecure"
      - "traefik.http.routers.traefik.service=api@internal"
      - "traefik.http.routers.traefik.tls=true"
      # Security headers middleware
      - "traefik.http.middlewares.security-headers.headers.customResponseHeaders.X-Robots-Tag=noindex,nofollow,nosnippet,noarchive,notranslate,noimageindex"
      - "traefik.http.middlewares.security-headers.headers.sslRedirect=true"
      - "traefik.http.middlewares.security-headers.headers.stsSeconds=315360000"
      - "traefik.http.middlewares.security-headers.headers.stsIncludeSubdomains=true"
      - "traefik.http.middlewares.security-headers.headers.stsPreload=true"
      - "traefik.http.middlewares.security-headers.headers.forceSTSHeader=true"
      - "traefik.http.middlewares.security-headers.headers.frameDeny=true"
      - "traefik.http.middlewares.security-headers.headers.contentTypeNosniff=true"
      - "traefik.http.middlewares.security-headers.headers.browserXssFilter=true"
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M

  ################################################################################
  # AUTHENTICATION & SECURITY
  ################################################################################

  # Authentik - SSO and 2FA provider (SOTA 2025 - no Redis needed in v2025.10+)
  authentik-server:
    image: ghcr.io/goauthentik/server:${AUTHENTIK_TAG:-latest}
    container_name: authentik-server
    logging: *default-logging
    command: server
    ports:
      - "${HOST_BIND:-192.168.178.40}:9000:9000"
      - "${HOST_BIND:-192.168.178.40}:9443:9443"
    environment:
      <<: *common-env
      AUTHENTIK_POSTGRESQL__HOST: postgres
      AUTHENTIK_POSTGRESQL__USER: postgres
      AUTHENTIK_POSTGRESQL__NAME: authentik
      AUTHENTIK_POSTGRESQL__PASSWORD: ${POSTGRES_SUPER_PASSWORD}
      AUTHENTIK_SECRET_KEY: ${AUTHENTIK_SECRET_KEY}
      AUTHENTIK_ERROR_REPORTING__ENABLED: "false"
    volumes:
      - authentik-media:/media
      - authentik-custom-templates:/templates
    networks:
      - potatostack
    depends_on:
      - postgres
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 512M

  authentik-worker:
    image: ghcr.io/goauthentik/server:${AUTHENTIK_TAG:-latest}
    container_name: authentik-worker
    logging: *default-logging
    command: worker
    environment:
      <<: *common-env
      AUTHENTIK_POSTGRESQL__HOST: postgres
      AUTHENTIK_POSTGRESQL__USER: postgres
      AUTHENTIK_POSTGRESQL__NAME: authentik
      AUTHENTIK_POSTGRESQL__PASSWORD: ${POSTGRES_SUPER_PASSWORD}
      AUTHENTIK_SECRET_KEY: ${AUTHENTIK_SECRET_KEY}
    volumes:
      - authentik-media:/media
      - authentik-certs:/certs
    networks:
      - potatostack
    depends_on:
      - postgres
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 128M

  # OAuth2-Proxy - Unified SSO gateway (OIDC/Auth Server)
  oauth2-proxy:
    image: quay.io/oauth2-proxy/oauth2-proxy:${OAUTH2_PROXY_TAG:-latest}
    container_name: oauth2-proxy
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:4180:4180"
    environment:
      <<: *common-env
      OAUTH2_PROXY_PROVIDER: oidc
      OAUTH2_PROXY_OIDC_ISSUER_URL: ${OAUTH2_PROXY_OIDC_ISSUER_URL}
      OAUTH2_PROXY_CLIENT_ID: ${OAUTH2_PROXY_CLIENT_ID}
      OAUTH2_PROXY_CLIENT_SECRET: ${OAUTH2_PROXY_CLIENT_SECRET}
      OAUTH2_PROXY_COOKIE_SECRET: ${OAUTH2_PROXY_COOKIE_SECRET}
      OAUTH2_PROXY_REDIRECT_URL: ${OAUTH2_PROXY_REDIRECT_URL}
      OAUTH2_PROXY_EMAIL_DOMAINS: ${OAUTH2_PROXY_EMAIL_DOMAINS:-*}
      OAUTH2_PROXY_COOKIE_DOMAINS: ${OAUTH2_PROXY_COOKIE_DOMAINS:-.${HOST_DOMAIN:-local.domain}}
      OAUTH2_PROXY_WHITELIST_DOMAINS: ${OAUTH2_PROXY_WHITELIST_DOMAINS:-.${HOST_DOMAIN:-local.domain}}
      OAUTH2_PROXY_HTTP_ADDRESS: 0.0.0.0:4180
      OAUTH2_PROXY_REVERSE_PROXY: "true"
      OAUTH2_PROXY_SET_XAUTHREQUEST: "true"
      OAUTH2_PROXY_UPSTREAMS: static://200
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.oauth2.rule=Host(`auth.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.oauth2.entrypoints=websecure"
      - "traefik.http.routers.oauth2.tls=true"
      - "traefik.http.services.oauth2.loadbalancer.server.port=4180"
      - "traefik.http.middlewares.oauth2-proxy.forwardauth.address=http://oauth2-proxy:4180/oauth2/auth"
      - "traefik.http.middlewares.oauth2-proxy.forwardauth.trustForwardHeader=true"
      - "traefik.http.middlewares.oauth2-proxy.forwardauth.authResponseHeaders=X-Auth-Request-User,X-Auth-Request-Email,Authorization"

  # Vaultwarden - Password manager and 2FA aggregator
  vaultwarden:
    image: vaultwarden/server:${VAULTWARDEN_TAG:-latest}
    container_name: vaultwarden
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8888:80"
      - "${HOST_BIND:-192.168.178.40}:3012:3012"
    environment:
      <<: *common-env
      DOMAIN: https://vault.${HOST_DOMAIN:-local.domain}
      ROCKET_PORT: 80
      WEBSOCKET_ENABLED: "true"
      WEBSOCKET_PORT: 3012
      SIGNUPS_ALLOWED: ${VAULTWARDEN_SIGNUPS_ALLOWED:-false}
      INVITATIONS_ALLOWED: ${VAULTWARDEN_INVITATIONS_ALLOWED:-true}
      ADMIN_TOKEN: ${VAULTWARDEN_ADMIN_TOKEN}
      DATABASE_URL: /data/db.sqlite3
      ICON_CACHE_TTL: 2592000
      LOG_LEVEL: warn
    volumes:
      - vaultwarden-data:/data
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:80/alive"]
      interval: 60s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.vaultwarden.rule=Host(`vault.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.vaultwarden.entrypoints=websecure"
      - "traefik.http.routers.vaultwarden.tls=true"
      - "traefik.http.services.vaultwarden.loadbalancer.server.port=80"
      - "traefik.http.routers.vaultwarden.middlewares=security-headers@docker"

  ################################################################################
  # DNS & AD BLOCKING
  ################################################################################

  # AdGuard Home - DNS-level ad blocking with encrypted DNS (SOTA 2025)
  adguardhome:
    image: adguard/adguardhome:${ADGUARD_TAG:-latest}
    container_name: adguardhome
    logging: *default-logging
    ports:
      - "53:53/tcp"
      - "53:53/udp"
      - "${HOST_BIND:-192.168.178.40}:3053:3000"
      - "${HOST_BIND:-192.168.178.40}:8053:80"
    environment:
      <<: *common-env
    volumes:
      - adguard-work:/opt/adguardhome/work
      - adguard-conf:/opt/adguardhome/conf
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:80/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.adguard.rule=Host(`dns.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.adguard.entrypoints=websecure"
      - "traefik.http.routers.adguard.tls=true"
      - "traefik.http.services.adguard.loadbalancer.server.port=80"

  ################################################################################
  # VPN & NETWORKING
  ################################################################################

  # Gluetun - VPN client with killswitch
  gluetun:
    image: qmcgaw/gluetun:${GLUETUN_TAG:-latest}
    container_name: gluetun
    logging: *default-logging
    cap_add:
      - NET_ADMIN
    devices:
      - /dev/net/tun:/dev/net/tun
    ports:
      - "${HOST_BIND:-192.168.178.40}:8000:8000" # Gluetun control
      - "${HOST_BIND:-192.168.178.40}:8989:8989" # Sonarr
      - "${HOST_BIND:-192.168.178.40}:7878:7878" # Radarr
      - "${HOST_BIND:-192.168.178.40}:9696:9696" # Prowlarr
      - "${HOST_BIND:-192.168.178.40}:6767:6767" # Bazarr
      - "${HOST_BIND:-192.168.178.40}:8787:8787" # Readarr
      - "${HOST_BIND:-192.168.178.40}:8686:8686" # Lidarr
      - "${HOST_BIND:-192.168.178.40}:8282:8282" # qBittorrent WebUI
      - "${HOST_BIND:-192.168.178.40}:51413:51413" # qBittorrent peer
      - "${HOST_BIND:-192.168.178.40}:51413:51413/udp"
      - "${HOST_BIND:-192.168.178.40}:6800:6800" # aria2 RPC
      - "${HOST_BIND:-192.168.178.40}:6880:80" # aria2 WebUI
      - "${HOST_BIND:-192.168.178.40}:6888:6888" # aria2 BT
      - "${HOST_BIND:-192.168.178.40}:2234:2234" # slskd WebUI
      - "${HOST_BIND:-192.168.178.40}:50000:50000" # slskd peer
    environment:
      <<: *common-env
      VPN_SERVICE_PROVIDER: ${VPN_PROVIDER:-surfshark}
      VPN_TYPE: ${VPN_TYPE:-openvpn}
      OPENVPN_USER: ${VPN_USER}
      OPENVPN_PASSWORD: ${VPN_PASSWORD}
      OPENVPN_PROTOCOL: ${VPN_PROTOCOL:-udp}
      OPENVPN_VERBOSITY: ${VPN_OPENVPN_VERBOSITY:-0}
      OPENVPN_FLAGS: "${VPN_OPENVPN_FLAGS:---tls-timeout 10 --connect-timeout 30 --connect-retry-max 3 --mssfix 1300}"
      SERVER_COUNTRIES: ${VPN_COUNTRY:-Germany}
      LOG_LEVEL: ${VPN_LOG_LEVEL:-info}
      FIREWALL_OUTBOUND_SUBNETS: ${LAN_NETWORK:-192.168.178.0/24}
      FIREWALL_VPN_INPUT_PORTS: ${VPN_INPUT_PORTS:-51413,50000,6888}
      FIREWALL: "on"
      DNS_ADDRESS: ${VPN_DNS:-1.1.1.1}
      DOT: "off"
      HTTPPROXY: "off"
      SHADOWSOCKS: "off"
      HTTP_CONTROL_SERVER_ADDRESS: :8000
      HTTP_CONTROL_SERVER_LOG: "off"
      HTTP_CONTROL_SERVER_AUTH: "off"
      IPV6: "off"
      UPDATER_PERIOD: 24h
      HEALTH_VPN_DURATION_INITIAL: 60s
      HEALTH_VPN_DURATION_ADDITION: 10s
    volumes:
      - gluetun-config:/gluetun
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "pgrep", "openvpn"]
      interval: 120s
      timeout: 10s
      retries: 3
      start_period: 120s
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 128M
    labels:
      - "autoheal=true"

  # WireGuard - High-performance VPN server (SOTA 2025)
  wireguard:
    image: lscr.io/linuxserver/wireguard:${WIREGUARD_TAG:-latest}
    container_name: wireguard
    logging: *default-logging
    cap_add:
      - NET_ADMIN
      - SYS_MODULE
    ports:
      - "${HOST_BIND:-192.168.178.40}:${WIREGUARD_SERVERPORT:-51820}:51820/udp"
    environment:
      <<: *common-env
      SERVERURL: ${WIREGUARD_SERVERURL:-auto}
      SERVERPORT: ${WIREGUARD_SERVERPORT:-51820}
      PEERS: ${WIREGUARD_PEERS:-vps,android,laptop,tablet,raspberry}
      PEERDNS: ${WIREGUARD_PEERDNS:-auto}
      INTERNAL_SUBNET: ${WIREGUARD_INTERNAL_SUBNET:-10.13.13.0/24}
      ALLOWEDIPS: ${WIREGUARD_ALLOWEDIPS:-0.0.0.0/0}
      LOG_CONFS: ${WIREGUARD_LOG_CONFS:-true}
    volumes:
      - /mnt/ssd/docker-data/wireguard:/config
      - /lib/modules:/lib/modules:ro
    sysctls:
      - net.ipv4.conf.all.src_valid_mark=1
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M

  # Tailscale - Mesh VPN for remote access (SOTA 2025 - easiest option)
  tailscale:
    image: tailscale/tailscale:${TAILSCALE_TAG:-latest}
    container_name: tailscale
    logging: *default-logging
    hostname: potatostack
    environment:
      <<: *common-env
      TS_AUTHKEY: ${TAILSCALE_AUTHKEY}
      TS_STATE_DIR: /var/lib/tailscale
      TS_USERSPACE: "false"
    volumes:
      - tailscale-data:/var/lib/tailscale
      - /dev/net/tun:/dev/net/tun
    cap_add:
      - NET_ADMIN
      - SYS_MODULE
    network_mode: host
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M

  ################################################################################
  # CLOUD STORAGE & FILE SYNC
  ################################################################################

  # Nextcloud AIO - All-in-One with Collabora, Talk, Whiteboard, etc. (SOTA 2025)
  nextcloud-aio:
    image: nextcloud/all-in-one:${NEXTCLOUD_AIO_TAG:-latest}
    container_name: nextcloud-aio-mastercontainer
    logging: *default-logging
    init: true
    ports:
      - "${HOST_BIND:-192.168.178.40}:8080:8080" # AIO interface
      - "${HOST_BIND:-192.168.178.40}:8443:8443" # Nextcloud
    environment:
      <<: *common-env
      APACHE_PORT: 11000
      APACHE_IP_BINDING: 0.0.0.0
      NEXTCLOUD_DATADIR: /mnt/storage/nextcloud
      NEXTCLOUD_MOUNT: /mnt/storage/nextcloud
      NEXTCLOUD_UPLOAD_LIMIT: 10G
      NEXTCLOUD_MAX_TIME: 3600
      NEXTCLOUD_MEMORY_LIMIT: 512M
      SKIP_DOMAIN_VALIDATION: "true"
    volumes:
      - nextcloud-aio-mastercontainer:/mnt/docker-aio-config
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /mnt/storage/nextcloud:/mnt/storage/nextcloud:rw
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 128M

  # Syncthing - P2P file sync
  syncthing:
    image: lscr.io/linuxserver/syncthing:${SYNCTHING_TAG:-latest}
    container_name: syncthing
    logging: *default-logging
    entrypoint: bash /syncthing-init.sh
    ports:
      - "${HOST_BIND:-192.168.178.40}:8384:8384"
      - "${HOST_BIND:-192.168.178.40}:22000:22000/tcp"
      - "${HOST_BIND:-192.168.178.40}:22000:22000/udp"
      - "${HOST_BIND:-192.168.178.40}:21027:21027/udp"
    environment:
      <<: *common-env
    volumes:
      - syncthing-config:/config
      - /mnt/storage/syncthing:/data
      - /mnt/cachehdd/syncthing-versions:/data/.stversions
      - shared-keys:/keys:ro
      - ./syncthing-init.sh:/syncthing-init.sh:ro
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8384/rest/noauth/health"]
      interval: 120s
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 128M

  ################################################################################
  # KNOWLEDGE MANAGEMENT
  ################################################################################

  ################################################################################
  # RSS & NEWS AGGREGATION
  ################################################################################

  # Miniflux - Minimalist RSS reader with Postgres backend (SOTA 2025)
  miniflux:
    image: miniflux/miniflux:${MINIFLUX_TAG:-latest}
    container_name: miniflux
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8093:8080"
    environment:
      <<: *common-env
      DATABASE_URL: postgres://postgres:${POSTGRES_SUPER_PASSWORD}@pgbouncer:5432/miniflux?sslmode=disable
      RUN_MIGRATIONS: "1"
      CREATE_ADMIN: "1"
      ADMIN_USERNAME: ${MINIFLUX_ADMIN_USER:-admin}
      ADMIN_PASSWORD: ${MINIFLUX_ADMIN_PASSWORD}
      BASE_URL: https://rss.${HOST_DOMAIN:-local.domain}
      POLLING_FREQUENCY: 60
      BATCH_SIZE: 100
      WORKER_POOL_SIZE: 5
      METRICS_COLLECTOR: "1"
      METRICS_ALLOWED_NETWORKS: 172.22.0.0/16
    networks:
      - potatostack
    depends_on:
      - postgres
      - pgbouncer
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "/usr/bin/miniflux", "-healthcheck", "auto"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 64M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.miniflux.rule=Host(`rss.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.miniflux.entrypoints=websecure"
      - "traefik.http.routers.miniflux.tls=true"
      - "traefik.http.services.miniflux.loadbalancer.server.port=8080"

  ################################################################################
  # FINANCE
  ################################################################################

  # Actual Budget - Modern budgeting with bank sync (SOTA 2025)
  actual-budget:
    image: actualbudget/actual-server:${ACTUAL_TAG:-latest}
    container_name: actual-budget
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:5006:5006"
    environment:
      <<: *common-env
    volumes:
      - actual-data:/data
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:5006/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.actual.rule=Host(`budget.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.actual.entrypoints=websecure"
      - "traefik.http.routers.actual.tls=true"
      - "traefik.http.services.actual.loadbalancer.server.port=5006"

  ################################################################################
  # MEDIA MANAGEMENT - *ARR STACK
  ################################################################################

  # Prowlarr - Indexer manager
  prowlarr:
    image: lscr.io/linuxserver/prowlarr:${PROWLARR_TAG:-latest}
    container_name: prowlarr
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
    volumes:
      - prowlarr-config:/config
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:9696/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 128M
    labels:
      - "autoheal=true"

  # Sonarr - TV show management
  sonarr:
    image: lscr.io/linuxserver/sonarr:${SONARR_TAG:-latest}
    container_name: sonarr
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
    volumes:
      - sonarr-config:/config
      - /mnt/storage/media/tv:/tv
      - /mnt/storage/downloads:/downloads
      - /mnt/cachehdd/qbittorrent-incomplete:/downloads/incomplete
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:8989/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 128M
        reservations:
          cpus: "0.25"
          memory: 128M
    labels:
      - "autoheal=true"

  # Radarr - Movie management
  radarr:
    image: lscr.io/linuxserver/radarr:${RADARR_TAG:-latest}
    container_name: radarr
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
    volumes:
      - radarr-config:/config
      - /mnt/storage/media/movies:/movies
      - /mnt/storage/downloads:/downloads
      - /mnt/cachehdd/qbittorrent-incomplete:/downloads/incomplete
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:7878/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 128M
        reservations:
          cpus: "0.25"
          memory: 128M
    labels:
      - "autoheal=true"

  # Lidarr - Music management
  lidarr:
    image: lscr.io/linuxserver/lidarr:${LIDARR_TAG:-latest}
    container_name: lidarr
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
    volumes:
      - lidarr-config:/config
      - /mnt/storage/media/music:/music
      - /mnt/storage/downloads:/downloads
      - /mnt/cachehdd/qbittorrent-incomplete:/downloads/incomplete
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:8686/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 128M
        reservations:
          cpus: "0.25"
          memory: 128M
    labels:
      - "autoheal=true"

  # Readarr - Ebook management
  readarr:
    image: lscr.io/linuxserver/readarr:${READARR_TAG:-develop}
    container_name: readarr
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
    volumes:
      - readarr-config:/config
      - /mnt/storage/media/books:/books
      - /mnt/storage/downloads:/downloads
      - /mnt/cachehdd/qbittorrent-incomplete:/downloads/incomplete
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:8787/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 128M
        reservations:
          cpus: "0.25"
          memory: 128M
    labels:
      - "autoheal=true"

  # Bazarr - Subtitle management
  bazarr:
    image: lscr.io/linuxserver/bazarr:${BAZARR_TAG:-latest}
    container_name: bazarr
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
    volumes:
      - bazarr-config:/config
      - /mnt/storage/media/movies:/movies
      - /mnt/storage/media/tv:/tv
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      sonarr:
        condition: service_started
      radarr:
        condition: service_started
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:6767/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 128M
    labels:
      - "autoheal=true"

  # Maintainerr - Media library cleanup
  maintainerr:
    image: jorenn92/maintainerr:${MAINTAINERR_TAG:-latest}
    container_name: maintainerr
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:6246:6246"
    environment:
      <<: *common-env
    volumes:
      - maintainerr-data:/opt/data
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M

  ################################################################################
  # MEDIA SERVERS & REQUEST MANAGEMENT
  ################################################################################

  # Jellyfin - Media server (SOTA 2025 with HW acceleration support)
  jellyfin:
    image: lscr.io/linuxserver/jellyfin:${JELLYFIN_TAG:-latest}
    container_name: jellyfin
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8096:8096"
      - "${HOST_BIND:-192.168.178.40}:8920:8920"
      - "${HOST_BIND:-192.168.178.40}:7359:7359/udp"
      - "${HOST_BIND:-192.168.178.40}:1900:1900/udp"
    environment:
      <<: *common-env
    # Uncomment group_add and devices for Intel/AMD hardware acceleration
    # Get render group ID with: getent group render | cut -d: -f3
    # group_add:
    #   - "107"  # render group - adjust to match your system
    # devices:
    #   - /dev/dri/renderD128:/dev/dri/renderD128
    devices:
      - /dev/dri/renderD128:/dev/dri/renderD128
      - /dev/dri/card0:/dev/dri/card0
    group_add:
      - "109"
    volumes:
      - jellyfin-config:/config
      - /mnt/storage/media/tv:/data/tvshows
      - /mnt/storage/media/movies:/data/movies
      - /mnt/storage/media/music:/data/music
      - /mnt/storage/media/audiobooks:/data/audiobooks
      - /mnt/cachehdd/jellyfin-cache:/cache
    tmpfs:
      - /tmp:exec,mode=1777,size=500m
      - /transcode:exec,mode=1777,size=2g
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.5"
          memory: 768M
        reservations:
          cpus: "0.75"
          memory: 1G

  # Jellyseerr - Media request management for Jellyfin
  jellyseerr:
    image: fallenbagel/jellyseerr:${JELLYSEERR_TAG:-latest}
    container_name: jellyseerr
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:5055:5055"
    environment:
      <<: *common-env
    volumes:
      - jellyseerr-config:/app/config
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M

  # Audiobookshelf - Audiobook and podcast server
  audiobookshelf:
    image: ghcr.io/advplyr/audiobookshelf:${AUDIOBOOKSHELF_TAG:-latest}
    container_name: audiobookshelf
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:13378:80"
    environment:
      <<: *common-env
    volumes:
      - audiobookshelf-config:/config
      - /mnt/cachehdd/audiobookshelf/metadata:/metadata
      - /mnt/storage/media/audiobooks:/audiobooks
      - /mnt/storage/media/podcasts:/podcasts
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 128M

  ################################################################################
  # DOWNLOAD CLIENTS (behind VPN)
  ################################################################################

  # qBittorrent - Torrent client
  qbittorrent:
    image: lscr.io/linuxserver/qbittorrent:${QBITTORRENT_TAG:-latest}
    container_name: qbittorrent
    logging: *default-logging
    network_mode: "service:gluetun"
    entrypoint: /qb-init.sh
    environment:
      <<: *common-env
      WEBUI_PORT: 8282
    volumes:
      - qbittorrent-config:/config
      - /mnt/storage/downloads:/downloads
      - /mnt/cachehdd/qbittorrent-incomplete:/incomplete
      - ./qbittorrent-init.sh:/qb-init.sh:ro
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:8282/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 128M
        reservations:
          cpus: "0.25"
          memory: 128M
    labels:
      - "autoheal=true"

  # Aria2 - Download manager
  aria2:
    image: p3terx/aria2-pro:${ARIA2_TAG:-latest}
    container_name: aria2
    logging: *default-logging
    network_mode: "service:gluetun"
    entrypoint: sh /aria2-init.sh
    environment:
      <<: *common-env
      RPC_PORT: 6800
      LISTEN_PORT: 6888
      DISK_CACHE: 64M
      IPV6_MODE: "false"
    volumes:
      - aria2-config:/config
      - /mnt/storage/downloads/aria2:/downloads
      - /mnt/cachehdd/aria2-incomplete:/incomplete
      - ./aria2-init.sh:/aria2-init.sh:ro
      - shared-keys:/keys:ro
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:6800/jsonrpc || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 128M
    labels:
      - "autoheal=true"

  # AriaNg - Aria2 web UI
  ariang:
    image: p3terx/ariang:${ARIANG_TAG:-latest}
    container_name: ariang
    logging: *default-logging
    network_mode: "service:gluetun"
    command: --port 80
    depends_on:
      - aria2
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M

  # slskd - Soulseek P2P client for music sharing (SOTA 2025)
  slskd:
    image: ghcr.io/slskd/slskd:${SLSKD_TAG:-latest}
    container_name: slskd
    logging: *default-logging
    network_mode: "service:gluetun"
    entrypoint: sh /init.sh
    environment:
      <<: *common-env
      SLSKD_HTTP_PORT: 2234
      SLSKD_SLSK_LISTEN_PORT: 50000
      SLSKD_USERNAME: ${SLSKD_USER:-admin}
      SLSKD_PASSWORD: ${SLSKD_PASSWORD}
      SLSKD_SOULSEEK_USERNAME: ${SLSKD_SOULSEEK_USERNAME}
      SLSKD_SOULSEEK_PASSWORD: ${SLSKD_SOULSEEK_PASSWORD}
      SLSKD_METRICS: "false"
      SLSKD_API_KEY: ${SLSKD_API_KEY}
      SLSKD_SHARED_DIR: /var/slskd/shared
    volumes:
      - slskd-config:/app
      - /mnt/cachehdd/slskd/logs:/app/logs
      - /mnt/storage/slskd-shared:/var/slskd/shared
      - /mnt/cachehdd/slskd-incomplete:/var/slskd/incomplete
      - shared-keys:/keys:ro
      - ./slskd-init.sh:/init.sh:ro
    tmpfs:
      - /tmp:exec,mode=1777,size=256m
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://127.0.0.1:2234/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: "0.75"
          memory: 128M
        reservations:
          cpus: "0.25"
          memory: 128M
    labels:
      - "autoheal=true"

  ################################################################################
  # PHOTO MANAGEMENT
  ################################################################################

  # Immich - Photo management and AI tagging
  immich-server:
    image: ghcr.io/immich-app/immich-server:${IMMICH_TAG:-release}
    container_name: immich-server
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:2283:3001"
    environment:
      <<: *common-env
      DB_HOSTNAME: postgres
      DB_USERNAME: postgres
      DB_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
      DB_DATABASE_NAME: immich
      REDIS_HOSTNAME: redis-cache
      REDIS_DBINDEX: 0
      UPLOAD_LOCATION: /usr/src/app/upload
    volumes:
      - /mnt/storage/photos:/usr/src/app/upload
      - /etc/localtime:/etc/localtime:ro
    networks:
      - potatostack
    depends_on:
      - postgres
      - redis-cache
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 768M

  immich-machine-learning:
    image: ghcr.io/immich-app/immich-machine-learning:${IMMICH_TAG:-release}
    container_name: immich-ml
    logging: *default-logging
    environment:
      <<: *common-env
    volumes:
      - /mnt/cachehdd/immich-ml-cache:/cache
    networks:
      - potatostack
    tmpfs:
      - /tmp:exec,mode=1777,size=500m
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 768M
        reservations:
          cpus: "0.5"
          memory: 1G

  ################################################################################
  # MONITORING & OBSERVABILITY
  ################################################################################

  # Prometheus - Metrics collection and time-series database (SOTA 2025)
  prometheus:
    image: prom/prometheus:${PROMETHEUS_TAG:-latest}
    container_name: prometheus
    logging: *default-logging
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=7d"
      - "--storage.tsdb.retention.size=5GB"
      - "--storage.tsdb.min-block-duration=2h"
      - "--storage.tsdb.max-block-duration=2h"
      - "--web.console.libraries=/usr/share/prometheus/console_libraries"
      - "--web.console.templates=/usr/share/prometheus/consoles"
      - "--web.enable-lifecycle"
      - "--web.enable-admin-api"
    ports:
      - "${HOST_BIND:-192.168.178.40}:9090:9090"
    environment:
      <<: *common-env
    volumes:
      - ./config/prometheus:/etc/prometheus:ro
      - /mnt/cachehdd/prometheus:/prometheus
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://127.0.0.1:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 128M
        reservations:
          cpus: "0.25"
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.prometheus.rule=Host(`prometheus.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.prometheus.entrypoints=websecure"
      - "traefik.http.routers.prometheus.tls=true"
      - "traefik.http.services.prometheus.loadbalancer.server.port=9090"

  # Thanos Sidecar - Long-term metrics storage (SOTA 2025)
  thanos-sidecar:
    image: quay.io/thanos/thanos:${THANOS_TAG:-latest}
    container_name: thanos-sidecar
    logging: *default-logging
    command:
      - "sidecar"
      - "--tsdb.path=/prometheus"
      - "--prometheus.url=http://prometheus:9090"
      - "--grpc-address=0.0.0.0:10901"
      - "--http-address=0.0.0.0:10902"
      - "--objstore.config-file=/etc/thanos/bucket.yml"
    ports:
      - "${HOST_BIND:-192.168.178.40}:10902:10902"
    environment:
      <<: *common-env
    volumes:
      - /mnt/cachehdd/prometheus:/prometheus
      - ./config/thanos:/etc/thanos:ro
    networks:
      - potatostack
    depends_on:
      - prometheus
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 128M

  # Thanos Store - Long-term storage gateway
  thanos-store:
    image: quay.io/thanos/thanos:${THANOS_TAG:-latest}
    container_name: thanos-store
    logging: *default-logging
    command:
      - "store"
      - "--data-dir=/var/thanos/store"
      - "--grpc-address=0.0.0.0:10901"
      - "--http-address=0.0.0.0:10902"
      - "--objstore.config-file=/etc/thanos/bucket.yml"
    volumes:
      - thanos-store-data:/var/thanos/store
      - ./config/thanos:/etc/thanos:ro
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 128M

  # Thanos Query - Unified query interface
  thanos-query:
    image: quay.io/thanos/thanos:${THANOS_TAG:-latest}
    container_name: thanos-query
    logging: *default-logging
    command:
      - "query"
      - "--grpc-address=0.0.0.0:10901"
      - "--http-address=0.0.0.0:10902"
      - "--store=thanos-sidecar:10901"
      - "--store=thanos-store:10901"
      - "--query.replica-label=replica"
    ports:
      - "${HOST_BIND:-192.168.178.40}:10903:10902"
    networks:
      - potatostack
    depends_on:
      - thanos-sidecar
      - thanos-store
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://127.0.0.1:10902/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.thanos.rule=Host(`thanos.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.thanos.entrypoints=websecure"
      - "traefik.http.routers.thanos.tls=true"
      - "traefik.http.services.thanos.loadbalancer.server.port=10902"

  # Thanos Compactor - Metrics compaction and downsampling
  thanos-compactor:
    image: quay.io/thanos/thanos:${THANOS_TAG:-latest}
    container_name: thanos-compactor
    logging: *default-logging
    command:
      - "compact"
      - "--data-dir=/var/thanos/compact"
      - "--objstore.config-file=/etc/thanos/bucket.yml"
      - "--http-address=0.0.0.0:10902"
      - "--wait"
      - "--retention.resolution-raw=30d"
      - "--retention.resolution-5m=90d"
      - "--retention.resolution-1h=365d"
    volumes:
      - thanos-compact-data:/var/thanos/compact
      - ./config/thanos:/etc/thanos:ro
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 128M

  # Grafana - Metrics visualization and dashboards (SOTA 2025)
  grafana:
    image: grafana/grafana:${GRAFANA_TAG:-latest}
    container_name: grafana
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:3002:3000"
    environment:
      <<: *common-env
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_INSTALL_PLUGINS: grafana-clock-panel,grafana-simple-json-datasource,grafana-piechart-panel
      GF_SERVER_ROOT_URL: https://grafana.${HOST_DOMAIN:-local.domain}
      GF_DATABASE_TYPE: postgres
      GF_DATABASE_HOST: pgbouncer:5432
      GF_DATABASE_NAME: grafana
      GF_DATABASE_USER: postgres
      GF_DATABASE_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
      GF_DATABASE_SSL_MODE: disable
    volumes:
      - grafana-data:/var/lib/grafana
      - ./config/grafana/provisioning:/etc/grafana/provisioning:ro
    networks:
      - potatostack
    depends_on:
      - postgres
      - pgbouncer
      - prometheus
      - loki
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:3000/api/health || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 128M
        reservations:
          cpus: "0.25"
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.grafana.rule=Host(`grafana.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.grafana.entrypoints=websecure"
      - "traefik.http.routers.grafana.tls=true"
      - "traefik.http.services.grafana.loadbalancer.server.port=3000"

  # cAdvisor - Container metrics for Prometheus (SOTA 2025)
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:${CADVISOR_TAG:-latest}
    container_name: cadvisor
    logging: *default-logging
    privileged: true
    devices:
      - /dev/kmsg
    ports:
      - "${HOST_BIND:-192.168.178.40}:8089:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    command:
      - "--housekeeping_interval=30s"
      - "--docker_only=true"
      - "--disable_metrics=percpu,sched,tcp,udp,disk,diskIO,accelerator,hugetlb,referenced_memory,cpu_topology,resctrl"
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 128M

  # Netdata - Real-time system monitoring (SOTA 2025 - ultra-lightweight)
  netdata:
    image: netdata/netdata:${NETDATA_TAG:-latest}
    container_name: netdata
    logging: *default-logging
    hostname: potatostack-main
    ports:
      - "${HOST_BIND:-192.168.178.40}:19999:19999"
    cap_add:
      - SYS_PTRACE
      - SYS_ADMIN
    security_opt:
      - apparmor:unconfined
    environment:
      <<: *common-env
      NETDATA_CLAIM_TOKEN: ${NETDATA_CLAIM_TOKEN:-}
      NETDATA_CLAIM_ROOMS: ${NETDATA_CLAIM_ROOMS:-}
      NETDATA_CLAIM_URL: https://app.netdata.cloud
      DOCKER_HOST: /var/run/docker.sock
    volumes:
      - netdata-config:/etc/netdata
      - netdata-lib:/var/lib/netdata
      - netdata-cache:/var/cache/netdata
      - /etc/passwd:/host/etc/passwd:ro
      - /etc/group:/host/etc/group:ro
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /etc/os-release:/host/etc/os-release:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:19999/api/v1/info || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 64M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.netdata.rule=Host(`netdata.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.netdata.entrypoints=websecure"
      - "traefik.http.routers.netdata.tls=true"
      - "traefik.http.services.netdata.loadbalancer.server.port=19999"

  # Loki - Log aggregation
  loki:
    image: grafana/loki:${LOKI_TAG:-latest}
    container_name: loki
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:3100:3100"
    command: -config.file=/etc/loki/loki.yml
    volumes:
      - ./config/loki:/etc/loki
      - /mnt/cachehdd/loki/data:/loki
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 128M

  # Promtail - Log collector
  promtail:
    image: grafana/promtail:${PROMTAIL_TAG:-latest}
    container_name: promtail
    logging: *default-logging
    command: -config.file=/etc/promtail/promtail.yml
    volumes:
      - ./config/promtail:/etc/promtail
      - /var/log:/var/log:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - potatostack
    depends_on:
      - loki
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M

  # Parseable - Lightweight log analytics (Loki alternative)
  parseable:
    image: parseable/parseable:${PARSEABLE_TAG:-latest}
    container_name: parseable
    logging: *default-logging
    command: ["parseable", "local-store"]
    ports:
      - "${HOST_BIND:-192.168.178.40}:8094:8000"
    environment:
      <<: *common-env
      P_USERNAME: ${PARSEABLE_USERNAME:-admin}
      P_PASSWORD: ${PARSEABLE_PASSWORD:-admin}
      P_ADDR: ${PARSEABLE_ADDR:-0.0.0.0:8000}
      P_FS_DIR: ${PARSEABLE_FS_DIR:-/data}
    volumes:
      - /mnt/ssd/docker-data/parseable:/data
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.parseable.rule=Host(`parseable.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.parseable.entrypoints=websecure"
      - "traefik.http.routers.parseable.tls=true"
      - "traefik.http.services.parseable.loadbalancer.server.port=8000"

  # Scrutiny - HDD SMART monitoring
  scrutiny:
    image: ghcr.io/analogj/scrutiny:${SCRUTINY_TAG:-master-omnibus}
    container_name: scrutiny
    logging: *default-logging
    cap_add:
      - SYS_RAWIO
      - SYS_ADMIN
    ports:
      - "${HOST_BIND:-192.168.178.40}:8087:8080"
    volumes:
      - /run/udev:/run/udev:ro
      - /mnt/ssd/docker-data/scrutiny/config:/opt/scrutiny/config
      - /mnt/ssd/docker-data/scrutiny/influxdb:/opt/scrutiny/influxdb
    devices:
      - "${SCRUTINY_DEVICE_1:-/dev/sda}"
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.scrutiny.rule=Host(`scrutiny.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.scrutiny.entrypoints=websecure"
      - "traefik.http.routers.scrutiny.tls=true"
      - "traefik.http.services.scrutiny.loadbalancer.server.port=8080"

  # Uptime Kuma - Uptime monitoring
  uptime-kuma:
    image: louislam/uptime-kuma:${UPTIME_KUMA_TAG:-latest}
    container_name: uptime-kuma
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:3001:3001"
    environment:
      <<: *common-env
    volumes:
      - uptime-kuma-data:/app/data
    networks:
      - potatostack
    restart: unless-stopped
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.uptime.rule=Host(`uptime.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.uptime.entrypoints=websecure"
      - "traefik.http.routers.uptime.tls=true"
      - "traefik.http.services.uptime.loadbalancer.server.port=3001"
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M

  ################################################################################
  # AUTOMATION & WORKFLOWS
  ################################################################################

  # n8n - Workflow automation
  n8n:
    image: n8nio/n8n:${N8N_TAG:-latest}
    container_name: n8n
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:5678:5678"
    environment:
      <<: *common-env
      N8N_BASIC_AUTH_ACTIVE: "true"
      N8N_BASIC_AUTH_USER: ${N8N_USER:-admin}
      N8N_BASIC_AUTH_PASSWORD: ${N8N_PASSWORD}
      N8N_HOST: n8n.${HOST_DOMAIN:-local.domain}
      N8N_PORT: 5678
      N8N_PROTOCOL: https
      WEBHOOK_URL: https://n8n.${HOST_DOMAIN:-local.domain}/
      GENERIC_TIMEZONE: Europe/Berlin
      DB_TYPE: postgresdb
      DB_POSTGRESDB_HOST: postgres
      DB_POSTGRESDB_PORT: 5432
      DB_POSTGRESDB_DATABASE: n8n
      DB_POSTGRESDB_USER: postgres
      DB_POSTGRESDB_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
      EXECUTIONS_MODE: queue
      QUEUE_BULL_REDIS_HOST: redis-cache
      QUEUE_BULL_REDIS_PORT: 6379
    volumes:
      - /mnt/ssd/docker-data/n8n:/home/node/.n8n
    networks:
      - potatostack
    depends_on:
      - postgres
      - redis-cache
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 128M

  # Healthchecks - Cron monitoring
  healthchecks:
    image: lscr.io/linuxserver/healthchecks:${HEALTHCHECKS_TAG:-latest}
    container_name: healthchecks
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8001:8000"
    environment:
      <<: *common-env
      SITE_ROOT: https://healthchecks.${HOST_DOMAIN:-local.domain}
      SITE_NAME: PotatoStack Healthchecks
      SUPERUSER_EMAIL: ${HEALTHCHECKS_ADMIN_EMAIL}
      SUPERUSER_PASSWORD: ${HEALTHCHECKS_ADMIN_PASSWORD}
      SECRET_KEY: ${HEALTHCHECKS_SECRET_KEY}
      DB: postgres
      DB_HOST: postgres
      DB_PORT: 5432
      DB_NAME: healthchecks
      DB_USER: postgres
      DB_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
    volumes:
      - healthchecks-data:/config
    networks:
      - potatostack
    depends_on:
      - postgres
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M

  ################################################################################
  # UTILITIES & TOOLS
  ################################################################################

  # Rustypaste - Pastebin
  rustypaste:
    image: orhunp/rustypaste:${RUSTYPASTE_TAG:-latest}
    container_name: rustypaste
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8787:8000"
    environment:
      <<: *common-env
      CONFIG: /config/config.toml
    volumes:
      - /mnt/storage/rustypaste:/data
      - ./config/rustypaste/config.toml:/config/config.toml:ro
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://127.0.0.1:8000 || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 15s
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 64M
    labels:
      - "autoheal=true"
      - "traefik.enable=true"
      - "traefik.http.routers.rustypaste.rule=PathPrefix(`/paste`)"
      - "traefik.http.routers.rustypaste.entrypoints=websecure"
      - "traefik.http.routers.rustypaste.tls=true"
      - "traefik.http.services.rustypaste.loadbalancer.server.port=8000"

  # Stirling-PDF - PDF tools
  stirling-pdf:
    image: frooodle/s-pdf:${STIRLING_PDF_TAG:-latest}
    container_name: stirling-pdf
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8086:8080"
    environment:
      <<: *common-env
      DOCKER_ENABLE_SECURITY: "false"
      INSTALL_BOOK_AND_ADVANCED_HTML_OPS: "true"
      LANGS: de_DE,en_US
    volumes:
      - stirling-pdf-data:/usr/share/tessdata
      - stirling-pdf-configs:/configs
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.75"
          memory: 384M
        reservations:
          cpus: "0.25"
          memory: 512M

  # Linkding - Bookmark manager
  linkding:
    image: sissbruecker/linkding:${LINKDING_TAG:-latest}
    container_name: linkding
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:9091:9090"
    environment:
      <<: *common-env
      LD_DB_ENGINE: postgres
      LD_DB_HOST: postgres
      LD_DB_PORT: 5432
      LD_DB_DATABASE: linkding
      LD_DB_USER: postgres
      LD_DB_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
      LD_SUPERUSER_NAME: ${LINKDING_ADMIN_USER:-admin}
      LD_SUPERUSER_PASSWORD: ${LINKDING_ADMIN_PASSWORD}
    volumes:
      - linkding-data:/etc/linkding/data
    networks:
      - potatostack
    depends_on:
      - postgres
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M

  # Cal.com - Calendar scheduling
  code-server:
    image: lscr.io/linuxserver/code-server:${CODE_SERVER_TAG:-latest}
    container_name: code-server
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8443:8443"
    environment:
      <<: *common-env
      PASSWORD: ${CODE_SERVER_PASSWORD}
      SUDO_PASSWORD: ${CODE_SERVER_SUDO_PASSWORD}
    volumes:
      - code-server-config:/config
      - /mnt/storage/projects:/config/workspace
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 384M

  # Excalidraw - Sketching tool
  atuin:
    image: ghcr.io/atuinsh/atuin:${ATUIN_TAG:-latest}
    container_name: atuin
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8889:8888"
    environment:
      <<: *common-env
      ATUIN_DB_URI: postgres://postgres:${POSTGRES_SUPER_PASSWORD}@postgres/atuin
    command: server start
    volumes:
      - atuin-config:/config
    networks:
      - potatostack
    depends_on:
      - postgres
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M

  # IT-Tools - Collection of handy online tools (SOTA 2025)
  it-tools:
    image: corentinth/it-tools:${IT_TOOLS_TAG:-latest}
    container_name: it-tools
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8091:80"
    environment:
      <<: *common-env
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:80/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.ittools.rule=Host(`tools.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.ittools.entrypoints=websecure"
      - "traefik.http.routers.ittools.tls=true"
      - "traefik.http.services.ittools.loadbalancer.server.port=80"

  # DuckDB - Ad-hoc analytics container
  duckdb:
    image: duckdb/duckdb:${DUCKDB_TAG:-latest}
    container_name: duckdb
    logging: *default-logging
    command: ["sleep", "infinity"]
    volumes:
      - /mnt/storage/duckdb:/data
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M

  # Memos - Lightweight note-taking (SOTA 2025)
  paperless-ngx:
    image: ghcr.io/paperless-ngx/paperless-ngx:${PAPERLESS_TAG:-latest}
    container_name: paperless-ngx
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8092:8000"
    environment:
      <<: *common-env
      PAPERLESS_REDIS: redis://redis-cache:6379/1
      PAPERLESS_DBHOST: postgres
      PAPERLESS_DBNAME: paperless
      PAPERLESS_DBUSER: postgres
      PAPERLESS_DBPASS: ${POSTGRES_SUPER_PASSWORD}
      PAPERLESS_URL: https://docs.${HOST_DOMAIN:-local.domain}
      PAPERLESS_SECRET_KEY: ${PAPERLESS_SECRET_KEY}
      PAPERLESS_ADMIN_USER: ${PAPERLESS_ADMIN_USER:-admin}
      PAPERLESS_ADMIN_PASSWORD: ${PAPERLESS_ADMIN_PASSWORD}
      PAPERLESS_OCR_LANGUAGE: deu+eng
      PAPERLESS_TIME_ZONE: Europe/Berlin
    volumes:
      - /mnt/ssd/docker-data/paperless-data:/usr/src/paperless/data
      - /mnt/storage/paperless/media:/usr/src/paperless/media
      - /mnt/storage/paperless/consume:/usr/src/paperless/consume
      - /mnt/storage/paperless/export:/usr/src/paperless/export
    networks:
      - potatostack
    depends_on:
      - postgres
      - redis-cache
      - storage-init
    tmpfs:
      - /tmp:exec,mode=1777,size=1g
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:8000/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 768M
        reservations:
          cpus: "0.5"
          memory: 1G
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.paperless.rule=Host(`docs.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.paperless.entrypoints=websecure"
      - "traefik.http.routers.paperless.tls=true"
      - "traefik.http.services.paperless.loadbalancer.server.port=8000"

  # Pingvin Share - Secure file sharing (SOTA 2025)
  ################################################################################
  # DEVELOPMENT & GIT
  ################################################################################

  # Gitea - Git hosting
  gitea:
    image: gitea/gitea:${GITEA_TAG:-latest}
    container_name: gitea
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:3004:3000"
      - "${HOST_BIND:-192.168.178.40}:2222:22"
    environment:
      <<: *common-env
      GITEA__database__DB_TYPE: postgres
      GITEA__database__HOST: postgres:5432
      GITEA__database__NAME: gitea
      GITEA__database__USER: postgres
      GITEA__database__PASSWD: ${POSTGRES_SUPER_PASSWORD}
      GITEA__server__DOMAIN: git.${HOST_DOMAIN:-local.domain}
      GITEA__server__ROOT_URL: https://git.${HOST_DOMAIN:-local.domain}/
      GITEA__server__SSH_DOMAIN: git.${HOST_DOMAIN:-local.domain}
      GITEA__security__INSTALL_LOCK: "true"
      GITEA__cache__ENABLED: "true"
      GITEA__cache__ADAPTER: redis
      GITEA__cache__HOST: redis://redis-cache:6379/0
      GITEA__session__PROVIDER: redis
      GITEA__session__PROVIDER_CONFIG: redis-cache:6379
      GITEA__queue__TYPE: redis
      GITEA__queue__CONN_STR: redis://redis-cache:6379/1
    volumes:
      - /mnt/ssd/docker-data/gitea:/data
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    networks:
      - potatostack
    depends_on:
      - postgres
      - redis-cache
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 384M

  # Gitea Runner - CI/CD for Gitea
  gitea-runner:
    image: gitea/act_runner:${GITEA_RUNNER_TAG:-latest}
    container_name: gitea-runner
    logging: *default-logging
    environment:
      <<: *common-env
      GITEA_INSTANCE_URL: http://gitea:3000
      GITEA_RUNNER_REGISTRATION_TOKEN: ${GITEA_RUNNER_TOKEN}
      GITEA_RUNNER_NAME: docker-runner
    volumes:
      - gitea-runner-data:/data
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - potatostack
    depends_on:
      - gitea
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M

  # Woodpecker CI - Gitea integrated CI/CD (SOTA 2025)
  woodpecker-server:
    image: woodpeckerci/woodpecker-server:${WOODPECKER_TAG:-latest}
    container_name: woodpecker-server
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:3006:8000"
    environment:
      <<: *common-env
      WOODPECKER_HOST: https://ci.${HOST_DOMAIN:-local.domain}
      WOODPECKER_SERVER_ADDR: :8000
      WOODPECKER_GRPC_ADDR: :9000
      WOODPECKER_GRPC_SECRET: ${WOODPECKER_AGENT_SECRET}
      WOODPECKER_DATABASE_DRIVER: postgres
      WOODPECKER_DATABASE_DATASOURCE: postgres://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/woodpecker?sslmode=disable
      WOODPECKER_OPEN: "false"
      WOODPECKER_ADMIN: ${WOODPECKER_ADMIN}
      WOODPECKER_GITEA: "true"
      WOODPECKER_GITEA_URL: http://gitea:3000
      WOODPECKER_GITEA_CLIENT: ${WOODPECKER_GITEA_CLIENT}
      WOODPECKER_GITEA_SECRET: ${WOODPECKER_GITEA_SECRET}
    networks:
      - potatostack
    depends_on:
      - postgres
      - gitea
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.woodpecker.rule=Host(`ci.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.woodpecker.entrypoints=websecure"
      - "traefik.http.routers.woodpecker.tls=true"
      - "traefik.http.services.woodpecker.loadbalancer.server.port=8000"

  woodpecker-agent:
    image: woodpeckerci/woodpecker-agent:${WOODPECKER_AGENT_TAG:-latest}
    container_name: woodpecker-agent
    logging: *default-logging
    environment:
      <<: *common-env
      WOODPECKER_SERVER: woodpecker-server:9000
      WOODPECKER_AGENT_SECRET: ${WOODPECKER_AGENT_SECRET}
      WOODPECKER_BACKEND: docker
      WOODPECKER_BACKEND_DOCKER_HOST: unix:///var/run/docker.sock
      WOODPECKER_BACKEND_DOCKER_NETWORK: potatostack
      WOODPECKER_MAX_WORKFLOWS: ${WOODPECKER_MAX_WORKFLOWS:-2}
      WOODPECKER_GRPC_SECURE: "false"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - potatostack
    depends_on:
      - woodpecker-server
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M

  # Sentry - Error tracking
  ################################################################################
  # AI & SPECIAL APPLICATIONS
  ################################################################################

  # Open WebUI - LLM interface
  open-webui:
    image: ghcr.io/open-webui/open-webui:${OPEN_WEBUI_TAG:-main}
    container_name: open-webui
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:3005:8080"
    environment:
      <<: *common-env
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      WEBUI_SECRET_KEY: ${OPEN_WEBUI_SECRET_KEY}
      DATABASE_URL: postgresql://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/openwebui
      VECTOR_DB: pgvector
    volumes:
      - open-webui-data:/app/backend/data
    networks:
      - potatostack
    depends_on:
      - postgres
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 384M

  # Pinchflat - YouTube downloader
  pinchflat:
    image: ghcr.io/kieraneglin/pinchflat:${PINCHFLAT_TAG:-latest}
    container_name: pinchflat
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8945:8945"
    environment:
      <<: *common-env
    volumes:
      - pinchflat-config:/config
      - /mnt/storage/media/youtube:/downloads
      - /mnt/cachehdd/pinchflat-incomplete:/incomplete
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 128M

  ################################################################################
  # DASHBOARD & CONTAINER MANAGEMENT
  ################################################################################

  # Homarr - Modern drag-and-drop dashboard with 30+ integrations (SOTA 2025)
  homarr:
    image: ghcr.io/homarr-labs/homarr:${HOMARR_TAG:-latest}
    container_name: homarr
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:7575:7575"
    environment:
      <<: *common-env
      SECRET_ENCRYPTION_KEY: ${HOMARR_SECRET_KEY}
      DATABASE_URL: postgresql://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/homarr
    volumes:
      - homarr-data:/app/data/configs
      - homarr-icons:/app/public/icons
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - potatostack
    depends_on:
      - postgres
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:7575/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 128M
        reservations:
          cpus: "0.25"
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.homarr.rule=Host(`home.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.homarr.entrypoints=websecure"
      - "traefik.http.routers.homarr.tls=true"
      - "traefik.http.services.homarr.loadbalancer.server.port=7575"

  # Dockge - Modern Docker Compose stack manager (SOTA 2025)
  ################################################################################
  # ENTERPRISE SECURITY & SECRETS MANAGEMENT
  ################################################################################

  # HashiCorp Vault - Enterprise secrets management
  fail2ban:
    image: crazymax/fail2ban:${FAIL2BAN_TAG:-latest}
    container_name: fail2ban
    logging: *default-logging
    network_mode: host
    cap_add:
      - NET_ADMIN
      - NET_RAW
    environment:
      <<: *common-env
      F2B_DB_PURGE_AGE: 30d
      F2B_LOG_LEVEL: INFO
      F2B_IPTABLES_CHAIN: DOCKER-USER
    volumes:
      - fail2ban-data:/data
      - /var/log:/var/log:ro
      - traefik-logs:/var/log/traefik:ro
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 64M

  # Trivy - Vulnerability scanner
  trivy:
    image: aquasec/trivy:${TRIVY_TAG:-latest}
    container_name: trivy
    logging: *default-logging
    command: ["server", "--listen", "0.0.0.0:8081"]
    ports:
      - "${HOST_BIND:-192.168.178.40}:8081:8081"
    environment:
      <<: *common-env
      TRIVY_LISTEN: "0.0.0.0:8081"
      TRIVY_CACHE_DIR: /root/.cache/trivy
    volumes:
      - trivy-cache:/root/.cache/
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 128M
        reservations:
          cpus: "0.25"
          memory: 128M

  # Alertmanager - Alert routing and management
  alertmanager:
    image: prom/alertmanager:${ALERTMANAGER_TAG:-latest}
    container_name: alertmanager
    logging: *default-logging
    command:
      - "--config.file=/etc/alertmanager/config.yml"
      - "--storage.path=/alertmanager"
      - "--web.external-url=http://alertmanager.${HOST_DOMAIN:-local.domain}"
    environment:
      <<: *common-env
    volumes:
      - ./config/alertmanager:/etc/alertmanager:ro
      - alertmanager-data:/alertmanager
    ports:
      - "${HOST_BIND:-192.168.178.40}:9093:9093"
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://127.0.0.1:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 64M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.alertmanager.rule=Host(`alerts.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.alertmanager.entrypoints=websecure"
      - "traefik.http.routers.alertmanager.tls=true"
      - "traefik.http.services.alertmanager.loadbalancer.server.port=9093"

  ################################################################################
  # SYSTEM UTILITIES
  ################################################################################

  # Diun - Docker Image Update Notifier (SOTA 2025 best practice)
  # Monitors for updates but doesn't auto-update, giving full control
  diun:
    image: crazymax/diun:${DIUN_TAG:-latest}
    container_name: diun
    logging: *default-logging
    command: serve
    environment:
      <<: *common-env
      LOG_LEVEL: info
      LOG_JSON: "false"
      DIUN_WATCH_WORKERS: 20
      DIUN_WATCH_SCHEDULE: "0 */6 * * *"
      DIUN_WATCH_JITTER: 30s
      DIUN_PROVIDERS_DOCKER: "true"
      DIUN_PROVIDERS_DOCKER_WATCHBYDEFAULT: "true"
      DIUN_NOTIF_GOTIFY_ENDPOINT: ${DIUN_GOTIFY_ENDPOINT:-}
      DIUN_NOTIF_GOTIFY_TOKEN: ${DIUN_GOTIFY_TOKEN:-}
      DIUN_NOTIF_DISCORD_WEBHOOKURL: ${DIUN_DISCORD_WEBHOOK:-}
      DIUN_NOTIF_TELEGRAM_TOKEN: ${DIUN_TELEGRAM_TOKEN:-}
      DIUN_NOTIF_TELEGRAM_CHATIDS: ${DIUN_TELEGRAM_CHATIDS:-}
    volumes:
      - diun-data:/data
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M

  # Autoheal - Container health recovery
  autoheal:
    image: willfarrell/autoheal:${AUTOHEAL_TAG:-latest}
    container_name: autoheal
    logging: *default-logging
    environment:
      AUTOHEAL_CONTAINER_LABEL: autoheal
      AUTOHEAL_INTERVAL: 60
      AUTOHEAL_START_PERIOD: 300
      AUTOHEAL_DEFAULT_STOP_TIMEOUT: 10
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M
        reservations:
          cpus: "0.05"
          memory: 32M

  ################################################################################
  # Gluetun Monitor - VPN Connection Monitor & Auto-Restart
  ################################################################################
  gluetun-monitor:
    image: alpine:${ALPINE_TAG:-3.21}
    container_name: gluetun-monitor
    logging: *default-logging
    user: "0:0"
    entrypoint: sh /monitor.sh
    environment:
      - GLUETUN_URL=http://gluetun:8000
      - CHECK_INTERVAL=${GLUETUN_CHECK_INTERVAL:-10}
      - RESTART_CONTAINERS=prowlarr sonarr radarr lidarr readarr bazarr qbittorrent aria2 ariang slskd
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./gluetun-monitor.sh:/monitor.sh:ro
    networks:
      - potatostack
    depends_on:
      gluetun:
        condition: service_started
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.05"
          memory: 16M
        reservations:
          cpus: "0.025"
          memory: 8M

  ################################################################################
  # Velld - Database Backup Scheduler (Postgres, Mongo, Redis)
  ################################################################################
  velld-api:
    image: ghcr.io/dendianugerah/velld/api:${VELLD_API_TAG:-latest}
    container_name: velld-api
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:8085:8080"
    environment:
      <<: *common-env
      JWT_SECRET: ${VELLD_JWT_SECRET}
      ENCRYPTION_KEY: ${VELLD_ENCRYPTION_KEY}
      ADMIN_USERNAME_CREDENTIAL: ${VELLD_ADMIN_USERNAME}
      ADMIN_PASSWORD_CREDENTIAL: ${VELLD_ADMIN_PASSWORD}
      ALLOW_REGISTER: ${VELLD_ALLOW_REGISTER:-false}
    volumes:
      - /mnt/ssd/docker-data/velld:/app/data
      - /mnt/storage/velld/backups:/app/backups
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M

  velld-web:
    image: ghcr.io/dendianugerah/velld/web:${VELLD_WEB_TAG:-latest}
    container_name: velld-web
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.40}:3010:3000"
    environment:
      <<: *common-env
      NEXT_PUBLIC_API_URL: ${VELLD_API_URL}
      ALLOW_REGISTER: ${VELLD_ALLOW_REGISTER:-false}
    networks:
      - potatostack
    depends_on:
      - velld-api
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M

  # Snapshot Scheduler - Cron-based Kopia snapshots
  snapshot-scheduler:
    image: docker:${DOCKER_CLI_TAG:-27.2.1}-cli
    container_name: snapshot-scheduler
    logging: *default-logging
    entrypoint: sh -c "chmod 600 /etc/crontabs/root 2>/dev/null || true; crond -f -l 8"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /mnt/ssd/docker-data/cron:/etc/crontabs
      - /mnt/storage/kopia:/mnt/storage/kopia
      - ./stack-snapshot.sh:/stack-snapshot.sh:ro
    networks:
      - potatostack
    depends_on:
      - kopia
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M
  # Kopia - Fast and secure backup server
  kopia:
    image: kopia/kopia:${KOPIA_TAG:-latest}
    container_name: kopia
    logging: *default-logging
    hostname: kopia-server
    entrypoint: sh /kopia-init.sh
    ports:
      - "${HOST_BIND:-192.168.178.40}:51515:51515"
    environment:
      <<: *common-env
      KOPIA_PASSWORD: ${KOPIA_PASSWORD}
      KOPIA_SERVER_USER: ${KOPIA_SERVER_USER:-admin}
      KOPIA_SERVER_PASSWORD: ${KOPIA_SERVER_PASSWORD}
      KOPIA_CONFIG_PATH: /app/config/repository.config
      KOPIA_CACHE_DIRECTORY: /app/cache
      KOPIA_LOG_DIR: /app/logs
    volumes:
      - /mnt/storage/kopia/repository:/repository
      - kopia-config:/app/config
      - /mnt/cachehdd/kopia-cache:/app/cache
      - kopia-logs:/app/logs
      - ./kopia-init.sh:/kopia-init.sh:ro
      # Backup targets
      - vaultwarden-data:/data/vaultwarden:ro
      - /mnt/storage/syncthing:/data/syncthing:ro
      - /mnt/storage/downloads:/data/downloads:ro
      - /mnt/storage/slskd-shared:/data/slskd-shared:ro
      - /mnt/storage/photos:/data/photos:ro
      - /mnt/storage/media:/data/media:ro
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 384M
    healthcheck:
      test: ["CMD", "kopia", "repository", "status"]
      interval: 120s
      timeout: 20s
      retries: 3
      start_period: 60s

################################################################################
# NETWORKS
################################################################################
networks:
  potatostack:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: br-potato-main
    ipam:
      config:
        - subnet: 172.22.0.0/16

################################################################################
# VOLUMES
################################################################################
volumes:
  # Databases
  postgres-data:
  mongo-data:
  mongo-config:
  redis-cache-data:

  # Authentication
  authentik-media:
  authentik-custom-templates:
  authentik-certs:
  vaultwarden-data:

  # Reverse Proxy
  traefik-certs:

  # VPN
  gluetun-config:
  tailscale-data:

  # Cloud Storage
  nextcloud-aio-mastercontainer:
  syncthing-config:

  # Media Management
  prowlarr-config:
  sonarr-config:
  radarr-config:
  lidarr-config:
  readarr-config:
  bazarr-config:
  maintainerr-data:
  jellyfin-config:
  jellyseerr-config:
  audiobookshelf-config:

  # Downloads
  qbittorrent-config:
  aria2-config:
  slskd-config:

  # Shared secrets
  shared-keys:

  # Monitoring
  uptime-kuma-data:
  prometheus-data:
  grafana-data:
  netdata-config:
  netdata-lib:
  netdata-cache:
  thanos-store-data:
  thanos-compact-data:

  # Automation
  n8n-data:
  healthchecks-data:

  # Utilities
  stirling-pdf-data:
  stirling-pdf-configs:
  linkding-data:
  code-server-config:
  atuin-config:

  # Development
  gitea-data:
  gitea-runner-data:
  sentry-data:

  # AI & Special
  open-webui-data:
  octobot-data:
  octobot-tentacles:
  octobot-logs:
  pinchflat-config:

  # Dashboard & Container Management
  homarr-data:
  homarr-icons:

  # Security
  crowdsec-db:
  crowdsec-config:
  traefik-logs:

  # DNS & Ad Blocking
  adguard-work:
  adguard-conf:

  # Monitoring

  # Finance
  actual-data:

  # Utilities
  paperless-data:
  paperless-media:

  # System
  diun-data:

  # Backups
  kopia-config:
  kopia-logs:

  # Enterprise Security & Secrets
  fail2ban-data:
  trivy-cache:

  # Enterprise Observability
  alertmanager-data:
