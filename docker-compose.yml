################################################################################
# POTATOSTACK - Fully Integrated Stack for Le Potato SBC
# Optimized for ARM64 with 2GB RAM
# Includes: VPN (Gluetun), P2P (qBittorrent, slskd/Soulseek), Backups (Kopia),
#           Storage (Nextcloud), Monitoring (Prometheus, Grafana, Loki),
#           Management (Portainer, Diun, Uptime Kuma, Dozzle),
#           Git (Gitea), Reverse Proxy (Nginx Proxy Manager), Dashboard (Homepage),
#           Security (Vaultwarden, Authelia SSO), Finance (Firefly III + FinTS),
#           Photos (Immich - self-hosted Google Photos alternative)
################################################################################

networks:
  vpn:
    driver: bridge
    name: potatostack_vpn
  monitoring:
    driver: bridge
    name: potatostack_monitoring
  proxy:
    driver: bridge
    name: potatostack_proxy
  default:
    driver: bridge
    name: potatostack_default

volumes:
  portainer_data:
  prometheus_data:
  grafana_data:
  loki_data:
  alertmanager_data:
  mariadb_data:
  nextcloud_data:
  gitea_data:
  postgres_data:
  npm_data:
  npm_ssl:
  npm_db:
  vaultwarden_data:
  firefly_data:
  firefly_upload:
  fints_importer_data:
  immich_model_cache:

services:
  ############################################################################
  # VPN STACK - Gluetun (Universal VPN Client) with Surfshark + Killswitch
  # Replaces ilteoood/docker-surfshark for better reliability, metrics, and firewall
  ############################################################################

  gluetun:
    image: qmcgaw/gluetun:${GLUETUN_TAG:-latest}
    container_name: gluetun
    hostname: gluetun-vpn
    restart: unless-stopped
    cap_add:
      - NET_ADMIN
    devices:
      - /dev/net/tun:/dev/net/tun
    environment:
      # VPN Provider Configuration
      - VPN_SERVICE_PROVIDER=surfshark
      - VPN_TYPE=${SURFSHARK_CONNECTION_TYPE:-openvpn} # openvpn or wireguard

      # OpenVPN Credentials (from .env)
      - OPENVPN_USER=${SURFSHARK_USER}
      - OPENVPN_PASSWORD=${SURFSHARK_PASSWORD}

      # Server Selection
      - SERVER_COUNTRIES=${SURFSHARK_COUNTRY:-Netherlands}
      - SERVER_CITIES=${SURFSHARK_CITY:-Amsterdam}

      # Firewall & Killswitch (CRITICAL - enforces VPN-only traffic)
      - FIREWALL_OUTBOUND_SUBNETS=${LAN_NETWORK:-192.168.178.0/24} # Allow LAN access
      - FIREWALL_VPN_INPUT_PORTS=6881 # Allow P2P incoming
      - FIREWALL_INPUT_PORTS= # Block all other incoming (security)
      - FIREWALL=on # Enable built-in killswitch firewall

      # DNS Configuration
      - DNS_ADDRESS=${VPN_DNS:-1.1.1.1}
      - DOT=off # Disable DNS-over-TLS (optional, enable for privacy)

      # Health Check Server (HTTP API on port 8000)
      - HTTPPROXY=off # Disable HTTP proxy (not needed)
      - SHADOWSOCKS=off # Disable Shadowsocks proxy (not needed)
      - HTTP_CONTROL_SERVER_ADDRESS=:8000
      - HTTP_CONTROL_SERVER_LOG=on

      # Timezone
      - TZ=Europe/Berlin

      # Advanced Settings
      - HEALTH_VPN_DURATION_INITIAL=20s # Initial VPN health check delay
      - HEALTH_VPN_DURATION_ADDITION=5s # Additional time per retry
      - UPDATER_PERIOD=24h # Update server list daily
      - PUBLICIP_PERIOD=12h # Check public IP every 12h
      - LOG_LEVEL=info

      # IPv6 (disable for compatibility)
      - IPV6=off

    networks:
      - vpn
      - monitoring

    ports:
      # qBittorrent WebUI
      - "${HOST_BIND:-0.0.0.0}:8080:8080"
      # slskd WebUI
      - "${HOST_BIND:-0.0.0.0}:2234:2234"
      # P2P ports (BitTorrent, Soulseek)
      - "${HOST_BIND:-0.0.0.0}:6881:6881"
      - "${HOST_BIND:-0.0.0.0}:6881:6881/udp"
      # Gluetun HTTP Control Server (health checks, metrics)
      - "${HOST_BIND:-0.0.0.0}:8000:8000"

    # Security: Mark source packets for VPN routing
    sysctls:
      - net.ipv4.conf.all.src_valid_mark=1
      - net.ipv6.conf.all.disable_ipv6=1

    # Healthcheck: Use Gluetun's built-in HTTP control server
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--spider",
          "--quiet",
          "http://localhost:8000/v1/publicip/ip",
        ]
      interval: 1m
      timeout: 10s
      retries: 3
      start_period: 30s

    # Resource limits (Le Potato 2GB optimized)
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 1

    logging:
      options:
        max-size: "10m"
        max-file: "3"

    labels:
      - "homepage.group=Infrastructure"
      - "homepage.name=Gluetun VPN"
      - "homepage.icon=si-wireguard"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:8000"
      - "homepage.description=VPN killswitch (Surfshark)"

  qbittorrent:
    image: lscr.io/linuxserver/qbittorrent:${QBITTORRENT_TAG:-latest}
    container_name: qbittorrent
    # hostname removed - conflicts with network_mode: service:gluetun
    restart: unless-stopped
    network_mode: service:gluetun
    depends_on:
      gluetun:
        condition: service_healthy
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Berlin
      - WEBUI_PORT=8080
      - TORRENTING_PORT=6881
      # Resource tuning for 2GB RAM
      - BT_MAX_OPEN_FILES=100
      - BT_MAX_CONNECTIONS_GLOBAL=200
      - BT_MAX_UPLOADS_GLOBAL=20
    volumes:
      - /mnt/seconddrive/qbittorrent/config:/config
      - /mnt/cachehdd/torrents:/downloads
      - /mnt/cachehdd/torrents/incomplete:/incomplete
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    labels:
      - "homepage.group=Media & Downloads"
      - "homepage.name=qBittorrent"
      - "homepage.icon=si-qbittorrent"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:8080"
      - "homepage.description=Torrent client via VPN"
      - "homepage.widget.type=qbittorrent"
      - "homepage.widget.url=http://gluetun:8080"
      - "homepage.widget.username=admin"
      - "homepage.widget.password={{HOMEPAGE_VAR_QBITTORRENT_PASSWORD}}"
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"

  slskd:
    image: ghcr.io/slskd/slskd:${SLSKD_TAG:-latest} # slskd - Modern Soulseek daemon with web UI
    container_name: slskd
    # hostname removed - conflicts with network_mode: service:gluetun
    restart: unless-stopped
    network_mode: service:gluetun
    depends_on:
      gluetun:
        condition: service_healthy
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Berlin
      - SLSKD_HTTP_PORT=2234
      - SLSKD_SLSK_LISTEN_PORT=50000
      - SLSKD_USERNAME=${SLSKD_USER:-admin}
      - SLSKD_PASSWORD=${SLSKD_PASSWORD}
      - SLSKD_METRICS=true
      - SLSKD_METRICS_URL=http://localhost:5031/metrics
    volumes:
      - /mnt/seconddrive/slskd/config:/app
      - /mnt/seconddrive/slskd/logs:/app/logs
      - /mnt/cachehdd/soulseek:/var/slskd/shared
      - /mnt/cachehdd/soulseek/incomplete:/var/slskd/incomplete
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:2234 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    labels:
      - "homepage.group=Media & Downloads"
      - "homepage.name=slskd (Soulseek)"
      - "homepage.icon=slskd.png"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:2234"
      - "homepage.description=Soulseek P2P via VPN (slskd client)"
    mem_limit: 384m
    mem_reservation: 192m
    cpus: 1
    logging:
      options:
        max-size: "10m"
        max-file: "3"

  ############################################################################
  # BACKUP - Kopia Server (from existing config)
  ############################################################################

  kopia:
    image: kopia/kopia:${KOPIA_TAG:-latest}
    container_name: kopia_server
    hostname: lepotato-backup
    restart: unless-stopped
    networks:
      - monitoring
      - proxy
    ports:
      - "${HOST_BIND:-0.0.0.0}:51515:51515"
      - "${HOST_BIND:-0.0.0.0}:51516:51516"
    environment:
      - TZ=Europe/Berlin
      - KOPIA_PASSWORD=${KOPIA_PASSWORD}
      - GOGC=50
      - GOMAXPROCS=2
      - KOPIA_CONFIG_PATH=/app/config/repository.config
      - KOPIA_CACHE_DIRECTORY=/app/cache
      - KOPIA_LOG_DIR=/app/logs
      - KOPIA_PROMETHEUS_ENABLED=true
      - KOPIA_PROMETHEUS_LISTEN_ADDR=:51516
    volumes:
      - /mnt/seconddrive/kopia/repository:/repository
      - /mnt/seconddrive/kopia/config:/app/config
      - /mnt/seconddrive/kopia/cache:/app/cache
      - /mnt/seconddrive/kopia/logs:/app/logs
      - /mnt/seconddrive/kopia/tmp:/tmp
      - /:/host:ro
    cap_add:
      - SYS_ADMIN
    security_opt:
      - no-new-privileges:false
    cap_drop:
      - ALL
      - SYS_ADMIN  # Keep only the required cap_add
    devices:
      - /dev/fuse:/dev/fuse
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:51515/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    command:
      - server
      - start
      - --address=0.0.0.0:51515
      - --tls-generate-cert
      - --tls-generate-rsa-key-size=4096
      - --server-username=${KOPIA_SERVER_USER:-admin}
      - --server-password=${KOPIA_SERVER_PASSWORD}
      - --log-level=info
      - --file-log-level=debug
      - --metrics-listen-addr=:51516
    mem_limit: 384m
    mem_reservation: 192m
    cpus: 1.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "homepage.group=Storage & Backup"
      - "homepage.name=Kopia"
      - "homepage.icon=kopia.png"
      - "homepage.href=https://${HOST_ADDR:-192.168.178.40}:51515"
      - "homepage.description=Encrypted backups"

  ############################################################################
  # DATABASE BACKUPS - Daily mysqldump and pg_dump
  ############################################################################

  nextcloud-db-backup:
    image: mariadb:${MARIADB_TAG:-10.11}
    container_name: nextcloud_db_backup
    hostname: nextcloud-db-backup
    restart: unless-stopped
    networks:
      - default
    environment:
      - MYSQL_PASSWORD=${NEXTCLOUD_DB_PASSWORD}
    volumes:
      - /mnt/seconddrive/backups/db:/backups
    depends_on:
      mariadb:
        condition: service_healthy
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    command: >
      sh -c "
        set -o pipefail;
        while true; do
          echo 'Backing up Nextcloud database...';
          mysqldump -h mariadb -u nextcloud --password=$$MYSQL_PASSWORD nextcloud | gzip > /backups/nextcloud-db-$$(date +%Y-%m-%d-%H%M).sql.gz && echo 'Nextcloud backup complete.' || echo 'Nextcloud backup failed.';
          sleep 1d;
        done
      "
    logging:
      options:
        max-size: "2m"
        max-file: "2"

  gitea-db-backup:
    image: postgres:${POSTGRES_TAG:-14-alpine}
    container_name: gitea_db_backup
    hostname: gitea-db-backup
    restart: unless-stopped
    networks:
      - default
    environment:
      - PGPASSWORD=${GITEA_DB_PASSWORD}
    volumes:
      - /mnt/seconddrive/backups/db:/backups
    depends_on:
      postgres:
        condition: service_healthy
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    command: >
      sh -c "
        set -o pipefail;
        while true; do
          echo 'Backing up Gitea database...';
          pg_dump -h postgres -U gitea -d gitea | gzip > /backups/gitea-db-$$(date +%Y-%m-%d-%H%M).sql.gz && echo 'Gitea backup complete.' || echo 'Gitea backup failed.';
          sleep 1d;
        done
      "
    logging:
      options:
        max-size: "2m"
        max-file: "2"

  vaultwarden-backup:
    profiles: ["apps"]  # OPTIONAL: Enable with --profile apps
    image: alpine:${ALPINE_TAG:-latest}
    container_name: vaultwarden_backup
    hostname: vaultwarden-backup
    restart: unless-stopped
    networks:
      - default
    volumes:
      - vaultwarden_data:/vaultwarden:ro
      - /mnt/seconddrive/backups/vaultwarden:/backups
    depends_on:
      vaultwarden:
        condition: service_healthy
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    command: >
      sh -c "
        set -o pipefail;
        apk add --no-cache sqlite gzip tar;
        while true; do
          echo 'Backing up Vaultwarden data...';
          BACKUP_DATE=$$(date +%Y-%m-%d-%H%M);
          mkdir -p /tmp/vw-backup;
          sqlite3 /vaultwarden/db.sqlite3 '.backup /tmp/vw-backup/db.sqlite3' &&
          cp -r /vaultwarden/attachments /tmp/vw-backup/ 2>/dev/null || true;
          cp -r /vaultwarden/sends /tmp/vw-backup/ 2>/dev/null || true;
          cp -r /vaultwarden/icon_cache /tmp/vw-backup/ 2>/dev/null || true;
          cp /vaultwarden/rsa_key* /tmp/vw-backup/ 2>/dev/null || true;
          tar -czf /backups/vaultwarden-$$BACKUP_DATE.tar.gz -C /tmp/vw-backup . &&
          echo 'Vaultwarden backup complete.' || echo 'Vaultwarden backup failed.';
          rm -rf /tmp/vw-backup;
          sleep 1d;
        done
      "
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.25
    logging:
      options:
        max-size: "2m"
        max-file: "2"

  firefly-db-backup:
    profiles: ["heavy"]  # OPTIONAL: Enable with --profile heavy
    image: mariadb:${MARIADB_TAG:-10.11}
    container_name: firefly_db_backup
    hostname: firefly-db-backup
    restart: unless-stopped
    networks:
      - default
    environment:
      - MYSQL_PASSWORD=${FIREFLY_DB_PASSWORD}
    volumes:
      - /mnt/seconddrive/backups/db:/backups
    depends_on:
      mariadb:
        condition: service_healthy
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    command: >
      sh -c "
        set -o pipefail;
        while true; do
          echo 'Backing up Firefly III database...';
          mysqldump -h mariadb -u firefly --password=$$MYSQL_PASSWORD firefly | gzip > /backups/firefly-db-$$(date +%Y-%m-%d-%H%M).sql.gz && echo 'Firefly backup complete.' || echo 'Firefly backup failed.';
          sleep 1d;
        done
      "
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.25
    logging:
      options:
        max-size: "2m"
        max-file: "2"

  ############################################################################
  # MONITORING STACK - Prometheus, Grafana, Loki, Thanos, Alertmanager
  ############################################################################

  prometheus:
    image: prom/prometheus:${PROMETHEUS_TAG:-latest}
    container_name: prometheus
    hostname: prometheus
    restart: unless-stopped
    networks:
      - monitoring
      - proxy
    ports:
      - "${HOST_BIND:-0.0.0.0}:9090:9090"
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    read_only: false
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=${PROMETHEUS_RETENTION_DAYS:-14d}"
      - "--web.console.libraries=/etc/prometheus/console_libraries"
      - "--web.console.templates=/etc/prometheus/consoles"
      - "--web.enable-lifecycle"
    volumes:
      - ./config/prometheus:/etc/prometheus
      - prometheus_data:/prometheus
      - /var/run/docker.sock:/var/run/docker.sock:ro
    depends_on:
      - node-exporter
      - cadvisor
      - smartctl-exporter
    mem_limit: 192m
    mem_reservation: 96m
    cpus: 0.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "homepage.group=Monitoring"
      - "homepage.name=Prometheus"
      - "homepage.icon=si-prometheus"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:9090"
      - "homepage.description=Metrics collection"

  grafana:
    image: grafana/grafana:${GRAFANA_TAG:-latest}
    container_name: grafana
    hostname: grafana
    restart: unless-stopped
    networks:
      - monitoring
      - proxy
    ports:
      - "${HOST_BIND:-0.0.0.0}:3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
      - GF_INSTALL_PLUGINS=grafana-piechart-panel,grafana-clock-panel
      - GF_SERVER_ROOT_URL=%(protocol)s://%(domain)s:%(http_port)s/grafana/
      - GF_ANALYTICS_REPORTING_ENABLED=false
      # OAuth2 via Authelia
      - GF_AUTH_GENERIC_OAUTH_ENABLED=true
      - GF_AUTH_GENERIC_OAUTH_NAME=Authelia
      - GF_AUTH_GENERIC_OAUTH_CLIENT_ID=grafana
      - GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET=${GRAFANA_OIDC_SECRET}
      - GF_AUTH_GENERIC_OAUTH_SCOPES=openid profile email groups
      - GF_AUTH_GENERIC_OAUTH_AUTH_URL=https://authelia.lepotato.local/api/oidc/authorization
      - GF_AUTH_GENERIC_OAUTH_TOKEN_URL=https://authelia.lepotato.local/api/oidc/token
      - GF_AUTH_GENERIC_OAUTH_API_URL=https://authelia.lepotato.local/api/oidc/userinfo
      - GF_AUTH_GENERIC_OAUTH_ALLOW_SIGN_UP=true
      - GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH=contains(groups[*], 'admins') && 'Admin' || 'Viewer'
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana/provisioning:/etc/grafana/provisioning
    depends_on:
      - prometheus
      - loki
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --spider --quiet http://localhost:3000/api/health || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    mem_limit: 192m
    mem_reservation: 96m
    cpus: 0.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "homepage.group=Monitoring"
      - "homepage.name=Grafana"
      - "homepage.icon=si-grafana"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:3000"
      - "homepage.description=Dashboards"

  loki:
    image: grafana/loki:${LOKI_TAG:-latest}
    container_name: loki
    hostname: loki
    restart: unless-stopped
    networks:
      - monitoring
    ports:
      - "${HOST_BIND:-0.0.0.0}:3100:3100"
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    read_only: false
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./config/loki:/etc/loki
      - loki_data:/loki
    mem_limit: 256m
    mem_reservation: 128m
    cpus: 0.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"

  promtail:
    image: grafana/promtail:${PROMTAIL_TAG:-latest}
    container_name: promtail
    hostname: promtail
    restart: unless-stopped
    networks:
      - monitoring
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    volumes:
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /mnt/seconddrive/kopia/logs:/kopia-logs:ro
      - /mnt/seconddrive/qbittorrent/config/logs:/qbittorrent-logs:ro
      - /mnt/seconddrive/slskd/logs:/slskd-logs:ro
      - ./config/promtail:/etc/promtail
      - /var/lib/promtail:/var/lib/promtail
    command: -config.file=/etc/promtail/config.yml
    depends_on:
      - loki
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"

  alertmanager:
    image: prom/alertmanager:${ALERTMANAGER_TAG:-latest}
    container_name: alertmanager
    hostname: alertmanager
    restart: unless-stopped
    networks:
      - monitoring
      - proxy
    ports:
      - "${HOST_BIND:-0.0.0.0}:9093:9093"
    environment:
      - ALERT_EMAIL_USER=${ALERT_EMAIL_USER}
      - ALERT_EMAIL_PASSWORD=${ALERT_EMAIL_PASSWORD}
      - ALERT_EMAIL_TO=${ALERT_EMAIL_TO}
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    read_only: false
    volumes:
      - ./config/alertmanager:/etc/alertmanager
      - alertmanager_data:/alertmanager
    command:
      - "--config.file=/etc/alertmanager/config.yml"
      - "--storage.path=/alertmanager"
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "homepage.group=Monitoring"
      - "homepage.name=Alertmanager"
      - "homepage.icon=si-prometheus"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:9093"
      - "homepage.description=Alert routing"

  ############################################################################
  # SYSTEM MONITORING - Node Exporter, cAdvisor, SMART
  ############################################################################

  node-exporter:
    image: prom/node-exporter:${NODE_EXPORTER_TAG:-latest}
    container_name: node-exporter
    hostname: node-exporter
    restart: unless-stopped
    networks:
      - monitoring
    ports:
      - "${HOST_BIND:-0.0.0.0}:9100:9100"
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    command:
      - "--path.procfs=/host/proc"
      - "--path.sysfs=/host/sys"
      - "--path.rootfs=/rootfs"
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    mem_limit: 64m
    mem_reservation: 32m
    cpus: 0.25
    logging:
      options:
        max-size: "5m"
        max-file: "2"

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:${CADVISOR_TAG:-latest}
    container_name: cadvisor
    hostname: cadvisor
    restart: unless-stopped
    networks:
      - monitoring
    ports:
      - "${HOST_BIND:-0.0.0.0}:8081:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker:/var/lib/docker:ro
      - /dev/disk:/dev/disk:ro
    devices:
      - /dev/kmsg
    privileged: true
    mem_limit: 64m
    mem_reservation: 32m
    cpus: 0.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    command:
      - "--docker_only=true"
      - "--housekeeping_interval=15s"
      - "--max_housekeeping_interval=1m"
      - "--store_container_labels=false"
      - "--disable_metrics=advtcp,sched,hugetlb,percpu,process"

  smartctl-exporter:
    image: prometheuscommunity/smartctl-exporter:${SMARTCTL_EXPORTER_TAG:-latest}
    container_name: smartctl-exporter
    hostname: smartctl-exporter
    restart: unless-stopped
    networks:
      - monitoring
    ports:
      - "${HOST_BIND:-0.0.0.0}:9633:9633"
    privileged: true
    user: root
    mem_limit: 64m
    mem_reservation: 32m
    cpus: 0.25
    logging:
      options:
        max-size: "5m"
        max-file: "2"

  netdata:
    profiles: ["monitoring-extra"]  # OPTIONAL: Redundant with Prometheus/Grafana
    image: netdata/netdata:${NETDATA_TAG:-latest}
    container_name: netdata
    hostname: lepotato-netdata
    restart: unless-stopped
    networks:
      - monitoring
      - proxy
    ports:
      - "${HOST_BIND:-0.0.0.0}:19999:19999"
    cap_add:
      - SYS_PTRACE
      - SYS_ADMIN
    security_opt:
      - no-new-privileges:false
      - apparmor:unconfined
    # cap_drop: ALL removed - was too restrictive
      - SYS_PTRACE  # Keep only the required cap_add
      - SYS_ADMIN
    environment:
      - NETDATA_CLAIM_TOKEN=${NETDATA_CLAIM_TOKEN}
      - NETDATA_CLAIM_URL=https://app.netdata.cloud
      - NETDATA_CLAIM_ROOMS=${NETDATA_CLAIM_ROOMS}
      - DOCKER_HOST=/var/run/docker.sock
    volumes:
      - /etc/passwd:/host/etc/passwd:ro
      - /etc/group:/host/etc/group:ro
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /mnt/seconddrive:/host/mnt/seconddrive:ro
      - /mnt/cachehdd:/host/mnt/cachehdd:ro
    labels:
      - "homepage.group=Monitoring"
      - "homepage.name=Netdata"
      - "homepage.icon=netdata.png"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:19999"
      - "homepage.description=Real-time system monitoring"
    mem_limit: 256m
    mem_reservation: 128m
    cpus: 1
    logging:
      options:
        max-size: "10m"
        max-file: "3"

  blackbox-exporter:
    profiles: ["monitoring-extra"]  # OPTIONAL: Can disable if memory-constrained
    image: prom/blackbox-exporter:${BLACKBOX_EXPORTER_TAG:-latest}
    container_name: blackbox-exporter
    hostname: blackbox-exporter
    restart: unless-stopped
    networks:
      - monitoring
      - proxy
      - default
    ports:
      - "${HOST_BIND:-0.0.0.0}:9115:9115"
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    volumes:
      - ./config/blackbox:/config
    command:
      - "--config.file=/config/blackbox.yml"
      - "--web.listen-address=:9115"
    mem_limit: 64m
    mem_reservation: 32m
    cpus: 0.25
    logging:
      options:
        max-size: "5m"
        max-file: "2"
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9115"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    depends_on:
      - prometheus
    labels:
      - "homepage.group=Monitoring"
      - "homepage.name=Blackbox Exporter"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:9115"
      - "homepage.description=HTTP/TCP service monitoring"

  ############################################################################
  # SPEEDTEST EXPORTER - Internet Speed & ISP Health Monitoring
  ############################################################################

  speedtest-exporter:
    profiles: ["monitoring-extra"]  # OPTIONAL: Nice to have
    image: miguelndecarvalho/speedtest-exporter:${SPEEDTEST_EXPORTER_TAG:-latest}
    container_name: speedtest-exporter
    hostname: speedtest-exporter
    restart: unless-stopped
    networks:
      - monitoring
    environment:
      - SPEEDTEST_INTERVAL=3600  # Run speedtest every hour
      - SPEEDTEST_HOST=  # Use closest server (default)
      - SPEEDTEST_PORT=8080
    ports:
      - "${HOST_BIND:-0.0.0.0}:9798:9798"
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "homepage.group=Monitoring"
      - "homepage.name=Speedtest Exporter"
      - "homepage.href=http://speedtest-exporter:9798"
      - "homepage.icon=speedtest.png"

  ############################################################################
  # FRITZBOX EXPORTER - Router Monitoring (Fritz!Box 7530 AX DSL)
  ############################################################################

  fritzbox-exporter:
    profiles: ["monitoring-extra"]  # OPTIONAL: Nice to have
    image: pdreker/fritz_exporter:${FRITZBOX_EXPORTER_TAG:-latest}
    container_name: fritzbox-exporter
    hostname: fritzbox-exporter
    restart: unless-stopped
    networks:
      - monitoring
      - default
    environment:
      - FRITZ_USERNAME=${FRITZ_USERNAME:-}  # Fritz!Box username (usually empty for default)
      - FRITZ_PASSWORD=${FRITZ_PASSWORD:-}  # Fritz!Box password
      - FRITZ_HOSTNAME=${FRITZ_HOSTNAME:-fritz.box}  # Fritz!Box hostname/IP
      - FRITZ_PORT=${FRITZ_PORT:-49000}  # TR-064 port (default)
      - FRITZ_PROTOCOL=${FRITZ_PROTOCOL:-https}  # Protocol (http/https)
      - FRITZ_DEVICE_NAME=${FRITZ_DEVICE_NAME:-fritzbox-7530}  # Device identifier
    ports:
      - "${HOST_BIND:-0.0.0.0}:9042:9042"
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.25
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "homepage.group=Monitoring"
      - "homepage.name=Fritz!Box Exporter"
      - "homepage.href=http://fritzbox-exporter:9042"
      - "homepage.icon=router.png"
      - "homepage.icon=si-prometheus"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:9115"
      - "homepage.description=HTTP/TCP probes"

  ############################################################################
  # NEXTCLOUD - File sharing and sync
  ############################################################################

  mariadb:
    image: mariadb:${MARIADB_TAG:-10.11}
    container_name: mariadb
    hostname: mariadb
    restart: unless-stopped
    networks:
      - default
    environment:
      - MYSQL_ROOT_PASSWORD=${MARIADB_ROOT_PASSWORD}
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    volumes:
      - ./config/mariadb/init:/docker-entrypoint-initdb.d:ro
      - mariadb_data:/var/lib/mysql
      - ./config/mariadb/low-memory.cnf:/etc/mysql/conf.d/low-memory.cnf:ro
    command: --transaction-isolation=READ-COMMITTED --binlog-format=ROW --innodb-file-per-table=1 --skip-innodb-read-only-compressed
    healthcheck:
      test:
        [
          "CMD",
          "mysqladmin",
          "ping",
          "-h",
          "localhost",
          "-u",
          "root",
          "--password=${MARIADB_ROOT_PASSWORD}",
        ]
      interval: 30s
      timeout: 10s
      retries: 5
    mem_limit: 192m
    mem_reservation: 96m
    cpus: 0.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "homepage.group=Infrastructure"
      - "homepage.name=MariaDB (Shared)"
      - "homepage.icon=si-mariadb"
      - "homepage.description=Shared database for Nextcloud & Firefly"

  nextcloud:
    image: nextcloud:${NEXTCLOUD_TAG:-stable}
    container_name: nextcloud
    hostname: nextcloud
    restart: unless-stopped
    networks:
      - default
      - proxy
    ports:
      - "${HOST_BIND:-0.0.0.0}:8082:80"
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    environment:
      - MYSQL_HOST=mariadb
      - MYSQL_DATABASE=nextcloud
      - MYSQL_USER=nextcloud
      - MYSQL_PASSWORD=${NEXTCLOUD_DB_PASSWORD}
      - NEXTCLOUD_ADMIN_USER=${NEXTCLOUD_ADMIN_USER:-admin}
      - NEXTCLOUD_ADMIN_PASSWORD=${NEXTCLOUD_ADMIN_PASSWORD}
      - NEXTCLOUD_TRUSTED_DOMAINS=${HOST_ADDR:-192.168.178.40} lepotato.local
      - OVERWRITEPROTOCOL=https
      - PHP_MEMORY_LIMIT=512M
      - PHP_UPLOAD_LIMIT=10G
      - REDIS_HOST=redis
      - REDIS_HOST_PASSWORD=${REDIS_PASSWORD:-}
      - REDIS_HOST_DB=0
    volumes:
      - nextcloud_data:/var/www/html
      - /mnt/seconddrive/nextcloud:/data
      - /mnt/cachehdd/torrents:/external/torrents:ro
      - /mnt/cachehdd/soulseek:/external/soulseek:ro
    depends_on:
      - redis
      - mariadb
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:80/status.php || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s
    mem_limit: 256m
    mem_reservation: 128m
    cpus: 1
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "homepage.group=Storage & Backup"
      - "homepage.name=Nextcloud"
      - "homepage.icon=si-nextcloud"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:8082"
      - "homepage.description=Cloud storage"

  redis:
    image: redis:${REDIS_TAG:-7-alpine}
    container_name: redis
    hostname: redis
    restart: unless-stopped
    networks:
      - default
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    read_only: false
    command:
      [
        "redis-server",
        "--save",
        "",
        "--appendonly",
        "no",
        "--maxmemory",
        "128mb",
        "--maxmemory-policy",
        "allkeys-lru",
        "--databases",
        "16",
      ]
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.5
    logging:
      options:
        max-size: "5m"
        max-file: "2"
    labels:
      - "homepage.group=Infrastructure"
      - "homepage.name=Redis (Shared)"
      - "homepage.icon=si-redis"
      - "homepage.description=Shared cache for all services"

  ############################################################################
  # GITEA - Git server
  ############################################################################

  postgres:
    # Unified Postgres for Gitea + Immich (includes vector extension)
    image: tensorchord/pgvecto-rs:pg14-v0.2.0
    container_name: postgres
    hostname: postgres
    restart: unless-stopped
    networks:
      - default
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=${POSTGRES_SUPER_PASSWORD}
      - POSTGRES_DB=postgres
      # Pass app DB passwords for init script
      - GITEA_DB_PASSWORD=${GITEA_DB_PASSWORD}
      - IMMICH_DB_PASSWORD=${IMMICH_DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./config/postgres/init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d postgres"]
      interval: 30s
      timeout: 10s
      retries: 5
    # Conservative Postgres tuning for 2GB RAM SBC
    command: [
      "postgres",
      "-c", "shared_buffers=64MB",
      "-c", "work_mem=4MB",
      "-c", "maintenance_work_mem=64MB",
      "-c", "effective_cache_size=256MB",
      "-c", "max_connections=60",
      "-c", "random_page_cost=2.0",
      "-c", "jit=off"
    ]
    mem_limit: 256m
    mem_reservation: 128m
    cpus: 1
    logging:
      options:
        max-size: "10m"
        max-file: "3"

  gitea:
    image: gitea/gitea:${GITEA_TAG:-latest}
    container_name: gitea
    hostname: gitea
    restart: unless-stopped
    networks:
      - default
      - proxy
    ports:
      - "${HOST_BIND:-0.0.0.0}:3001:3000"
      - "${HOST_BIND:-0.0.0.0}:2222:22"
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    environment:
      - USER_UID=1000
      - USER_GID=1000
      - GITEA__database__DB_TYPE=postgres
      - GITEA__database__HOST=postgres:5432
      - GITEA__database__NAME=gitea
      - GITEA__database__USER=gitea
      - GITEA__database__PASSWD=${GITEA_DB_PASSWORD}
      - GITEA__cache__ADAPTER=redis
      - GITEA__cache__HOST=redis:6379/0
      - GITEA__session__PROVIDER=redis
      - GITEA__session__PROVIDER_CONFIG=redis:6379/0
      - GITEA__queue__TYPE=redis
      - GITEA__queue__CONN_STR=redis:6379/0
    volumes:
      - gitea_data:/data
      - /mnt/seconddrive/gitea:/data/gitea/repositories
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    depends_on:
      - redis
      - postgres
    mem_limit: 192m
    mem_reservation: 96m
    cpus: 0.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "homepage.group=Infrastructure"
      - "homepage.name=Gitea"
      - "homepage.icon=si-gitea"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:3001"
      - "homepage.description=Git server"

  ############################################################################
  # MANAGEMENT TOOLS - Portainer, Diun, Uptime Kuma, Dozzle
  ############################################################################

  portainer:
    image: portainer/portainer-ce:${PORTAINER_TAG:-latest}
    container_name: portainer
    hostname: portainer
    restart: unless-stopped
    networks:
      - default
      - proxy
    ports:
      - "${HOST_BIND:-0.0.0.0}:9000:9000"
      - "${HOST_BIND:-0.0.0.0}:9443:9443"
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - portainer_data:/data
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.5
    logging:
      options:
        max-size: "5m"
        max-file: "2"

  diun:
    image: crazymax/diun:${DIUN_TAG:-latest}
    container_name: diun
    hostname: diun
    restart: unless-stopped
    networks:
      - monitoring
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    environment:
      - TZ=Europe/Berlin
      - LOG_LEVEL=info
      - LOG_JSON=false
    volumes:
      - ./config/diun:/etc/diun:ro
      - ./data/diun:/data
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /etc/localtime:/etc/localtime:ro
    mem_limit: 64m
    mem_reservation: 32m
    cpus: 0.25
    logging:
      options:
        max-size: "5m"
        max-file: "2"
    labels:
      - "homepage.group=Management"
      - "homepage.name=Diun"
      - "homepage.icon=mdi-bell-alert"
      - "homepage.description=Docker image update notifications"
      - "diun.enable=true"

  uptime-kuma:
    profiles: ["monitoring-extra"]  # OPTIONAL: Redundant with Blackbox Exporter
    image: louislam/uptime-kuma:${UPTIME_KUMA_TAG:-latest}
    container_name: uptime-kuma
    hostname: uptime-kuma
    restart: unless-stopped
    networks:
      - default
      - proxy
      - monitoring
    ports:
      - "${HOST_BIND:-0.0.0.0}:3002:3001"
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    volumes:
      - /mnt/seconddrive/uptime-kuma:/app/data
      - /var/run/docker.sock:/var/run/docker.sock:ro
    mem_limit: 256m
    mem_reservation: 128m
    cpus: 0.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"

  dozzle:
    image: amir20/dozzle:${DOZZLE_TAG:-latest}
    container_name: dozzle
    hostname: dozzle
    restart: unless-stopped
    networks:
      - proxy
    ports:
      - "${HOST_BIND:-0.0.0.0}:8083:8080"
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      - DOZZLE_LEVEL=info
      - DOZZLE_TAILSIZE=300
      - DOZZLE_FILTER=status=running
    mem_limit: 64m
    mem_reservation: 32m
    cpus: 0.25
    logging:
      options:
        max-size: "5m"
        max-file: "2"

  ############################################################################
  # AUTOHEAL - Automatically restart unhealthy containers
  ############################################################################

  autoheal:
    image: willfarrell/autoheal:${AUTOHEAL_TAG:-latest}
    container_name: autoheal
    hostname: autoheal
    restart: unless-stopped
    networks:
      - default
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    environment:
      - AUTOHEAL_CONTAINER_LABEL=all # Monitor all containers with healthchecks
      - AUTOHEAL_INTERVAL=10 # Check every 10 seconds
      - AUTOHEAL_START_PERIOD=300 # Wait 5 minutes after container start
      - AUTOHEAL_DEFAULT_STOP_TIMEOUT=10 # Wait 10s before force-killing
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /etc/localtime:/etc/localtime:ro
    mem_limit: 64m
    mem_reservation: 32m
    cpus: 0.25
    logging:
      options:
        max-size: "5m"
        max-file: "2"
    labels:
      - "homepage.group=Management"
      - "homepage.name=Autoheal"
      - "homepage.icon=mdi-heart-pulse"
      - "homepage.description=Auto-restart unhealthy containers"

  ############################################################################
  # PASSWORD MANAGER - Vaultwarden (Bitwarden-compatible)
  ############################################################################

  vaultwarden:
    profiles: ["apps"]  # OPTIONAL: Enable with --profile apps (important but can use cloud alternative)
    image: vaultwarden/server:${VAULTWARDEN_TAG:-latest}
    container_name: vaultwarden
    hostname: vaultwarden
    restart: unless-stopped
    networks:
      - proxy
      - default
      - monitoring
    ports:
      - "${HOST_BIND:-0.0.0.0}:8084:80"
      - "${HOST_BIND:-0.0.0.0}:3012:3012"  # WebSocket port
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    environment:
      - TZ=Europe/Berlin
      - DOMAIN=https://vaultwarden.lepotato.local
      - ROCKET_PORT=80
      - WEBSOCKET_ENABLED=true
      - WEBSOCKET_PORT=3012
      - SIGNUPS_ALLOWED=${VAULTWARDEN_SIGNUPS_ALLOWED:-false}
      - INVITATIONS_ALLOWED=${VAULTWARDEN_INVITATIONS_ALLOWED:-true}
      - SHOW_PASSWORD_HINT=false
      - ADMIN_TOKEN=${VAULTWARDEN_ADMIN_TOKEN}
      - SMTP_HOST=smtp.gmail.com
      - SMTP_FROM=${ALERT_EMAIL_USER}
      - SMTP_PORT=587
      - SMTP_SECURITY=starttls
      - SMTP_USERNAME=${ALERT_EMAIL_USER}
      - SMTP_PASSWORD=${ALERT_EMAIL_PASSWORD}
      - LOG_LEVEL=info
      - EXTENDED_LOGGING=true
      - LOG_FILE=/data/vaultwarden.log
      # SSO via Authelia (OAuth2)
      - SSO_ENABLED=${VAULTWARDEN_SSO_ENABLED:-false}
      - SSO_ONLY=${VAULTWARDEN_SSO_ONLY:-false}
      - SSO_CLIENT_ID=vaultwarden
      - SSO_CLIENT_SECRET=${VAULTWARDEN_OIDC_SECRET}
      - SSO_AUTHORITY=https://authelia.lepotato.local
      # Database configuration (SQLite by default)
      - DATABASE_URL=/data/db.sqlite3
      # Icon cache
      - ICON_CACHE_TTL=2592000
      - ICON_CACHE_NEGTTL=259200
      # Security settings
      - PASSWORD_ITERATIONS=600000
      - REQUIRE_DEVICE_EMAIL=${VAULTWARDEN_REQUIRE_DEVICE_EMAIL:-false}
      - DISABLE_2FA_REMEMBER=${VAULTWARDEN_DISABLE_2FA_REMEMBER:-false}
    volumes:
      - vaultwarden_data:/data
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:80/alive || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "homepage.group=Security"
      - "homepage.name=Vaultwarden"
      - "homepage.icon=bitwarden.png"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:8084"
      - "homepage.description=Password manager (Bitwarden)"

  ############################################################################
  # SINGLE SIGN-ON (SSO) - Authelia with OAuth2/OIDC
  ############################################################################

  authelia:
    profiles: ["heavy"]  # OPTIONAL: Enable with --profile heavy (uses ~64MB)
    image: authelia/authelia:${AUTHELIA_TAG:-latest}
    container_name: authelia
    hostname: authelia
    restart: unless-stopped
    networks:
      - proxy
      - default
      - monitoring
    ports:
      - "${HOST_BIND:-0.0.0.0}:9091:9091"
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    environment:
      - TZ=Europe/Berlin
      - AUTHELIA_JWT_SECRET=${AUTHELIA_JWT_SECRET}
      - AUTHELIA_SESSION_SECRET=${AUTHELIA_SESSION_SECRET}
      - AUTHELIA_STORAGE_ENCRYPTION_KEY=${AUTHELIA_STORAGE_ENCRYPTION_KEY}
      - AUTHELIA_IDENTITY_PROVIDERS_OIDC_HMAC_SECRET=${AUTHELIA_OIDC_HMAC_SECRET}
      - AUTHELIA_IDENTITY_PROVIDERS_OIDC_ISSUER_PRIVATE_KEY=${AUTHELIA_OIDC_PRIVATE_KEY}
      - AUTHELIA_NOTIFIER_SMTP_USERNAME=${ALERT_EMAIL_USER}
      - AUTHELIA_NOTIFIER_SMTP_PASSWORD=${ALERT_EMAIL_PASSWORD}
    volumes:
      - ./config/authelia:/config
    depends_on:
      - redis
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9091/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "homepage.group=Infrastructure"
      - "homepage.name=Authelia SSO"
      - "homepage.icon=si-auth0"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:9091"
      - "homepage.description=Single Sign-On & 2FA"

  ############################################################################
  # REVERSE PROXY & DASHBOARD
  ############################################################################

  nginx-proxy-manager:
    image: jc21/nginx-proxy-manager:${NPM_TAG:-latest}
    container_name: nginx-proxy-manager
    hostname: npm
    restart: unless-stopped
    networks:
      - proxy
      - monitoring
    ports:
      - "${HOST_BIND:-0.0.0.0}:80:80"
      - "${HOST_BIND:-0.0.0.0}:443:443"
      - "${HOST_BIND:-0.0.0.0}:81:81"
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    environment:
      - DB_SQLITE_FILE=/data/database.sqlite
      - DISABLE_IPV6=true
    volumes:
      - npm_data:/data
      - npm_ssl:/etc/letsencrypt
    healthcheck:
      test: ["CMD-SHELL", "wget --spider --quiet http://localhost:81 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "homepage.group=Infrastructure"
      - "homepage.name=Nginx Proxy Manager"
      - "homepage.icon=si-nginx"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:81"
      - "homepage.description=Reverse proxy & SSL"

  homepage:
    image: ghcr.io/gethomepage/homepage:${HOMEPAGE_TAG:-latest}
    container_name: homepage
    hostname: homepage
    restart: unless-stopped
    networks:
      - proxy
      - monitoring
      - default
    ports:
      - "${HOST_BIND:-0.0.0.0}:3003:3000"
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    environment:
      - PUID=1000
      - PGID=1000
    volumes:
      - ./config/homepage:/app/config
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /mnt/seconddrive:/mnt/seconddrive:ro
      - /mnt/cachehdd:/mnt/cachehdd:ro
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.5
    logging:
      options:
        max-size: "5m"
        max-file: "2"

  ############################################################################
  # FINANCIAL PLANNING - Firefly III + FinTS Importer (Deutsche Bank)
  ############################################################################

  firefly-iii:
    profiles: ["heavy"]  # OPTIONAL: Enable with --profile heavy (uses ~192MB)
    image: fireflyiii/core:${FIREFLY_TAG:-latest}
    container_name: firefly-iii
    hostname: firefly-iii
    restart: unless-stopped
    networks:
      - default
      - proxy
      - monitoring
    ports:
      - "${HOST_BIND:-0.0.0.0}:8085:8080"
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    environment:
      - APP_KEY=${FIREFLY_APP_KEY}
      - DB_HOST=mariadb
      - DB_PORT=3306
      - DB_DATABASE=firefly
      - DB_USERNAME=firefly
      - DB_PASSWORD=${FIREFLY_DB_PASSWORD}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - TRUSTED_PROXIES=**
      - APP_URL=https://firefly.${HOST_DOMAIN:-lepotato.local}
      - TZ=Europe/Berlin
      - SITE_OWNER=${FIREFLY_SITE_OWNER:-admin@lepotato.local}
      - APP_ENV=production
      - APP_DEBUG=false
      - LOG_CHANNEL=stack
      - CACHE_DRIVER=redis
      - SESSION_DRIVER=redis
      - QUEUE_CONNECTION=redis
    volumes:
      - firefly_upload:/var/www/html/storage/upload
      - firefly_data:/var/www/html/storage/app
    depends_on:
      mariadb:
        condition: service_healthy
      redis:
        condition: service_started
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s
    mem_limit: 256m
    mem_reservation: 128m
    cpus: 1
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "homepage.group=Finance"
      - "homepage.name=Firefly III"
      - "homepage.icon=firefly.png"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:8085"
      - "homepage.description=Household finance & budgeting"

  firefly-worker:
    profiles: ["heavy"]  # OPTIONAL: Enable with --profile heavy
    image: fireflyiii/core:${FIREFLY_TAG:-latest}
    container_name: firefly-worker
    hostname: firefly-worker
    restart: unless-stopped
    networks:
      - default
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    environment:
      - APP_KEY=${FIREFLY_APP_KEY}
      - DB_HOST=mariadb
      - DB_PORT=3306
      - DB_DATABASE=firefly
      - DB_USERNAME=firefly
      - DB_PASSWORD=${FIREFLY_DB_PASSWORD}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=1
      - TZ=Europe/Berlin
      - APP_ENV=production
      - CACHE_DRIVER=redis
      - SESSION_DRIVER=redis
      - QUEUE_CONNECTION=redis
    volumes:
      - firefly_data:/var/www/html/storage/app
    depends_on:
      - firefly-iii
    command: php artisan queue:work --sleep=3 --tries=3 --max-time=60
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.5
    logging:
      options:
        max-size: "5m"
        max-file: "2"

  firefly-cron:
    profiles: ["heavy"]  # OPTIONAL: Enable with --profile heavy
    image: fireflyiii/core:${FIREFLY_TAG:-latest}
    container_name: firefly-cron
    hostname: firefly-cron
    restart: unless-stopped
    networks:
      - default
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    environment:
      - APP_KEY=${FIREFLY_APP_KEY}
      - DB_HOST=mariadb
      - DB_PORT=3306
      - DB_DATABASE=firefly
      - DB_USERNAME=firefly
      - DB_PASSWORD=${FIREFLY_DB_PASSWORD}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=1
      - TZ=Europe/Berlin
      - APP_ENV=production
      - CACHE_DRIVER=redis
      - SESSION_DRIVER=redis
      - QUEUE_CONNECTION=redis
    volumes:
      - firefly_data:/var/www/html/storage/app
    depends_on:
      - firefly-iii
    command: /entrypoint.sh
    mem_limit: 64m
    mem_reservation: 32m
    cpus: 0.25
    logging:
      options:
        max-size: "5m"
        max-file: "2"

  fints-importer:
    profiles: ["heavy"]  # OPTIONAL: Enable with --profile heavy
    image: benkl/firefly-iii-fints-importer:${FINTS_IMPORTER_TAG:-latest}
    container_name: fints-importer
    hostname: fints-importer
    restart: unless-stopped
    networks:
      - default
      - proxy
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    environment:
      - TZ=Europe/Berlin
      - FIREFLY_III_URL=http://firefly-iii:8080
      - FIREFLY_III_ACCESS_TOKEN=${FIREFLY_ACCESS_TOKEN}
      - IMPORT_DIR_ALLOWLIST=/data
    volumes:
      - fints_importer_data:/data
      - ./config/fints-importer:/import-configurations:ro
    ports:
      - "${HOST_BIND:-0.0.0.0}:8086:8080"
    depends_on:
      firefly-iii:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "homepage.group=Finance"
      - "homepage.name=FinTS Importer"
      - "homepage.icon=mdi-bank-transfer"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:8086"
      - "homepage.description=Deutsche Bank auto-import"

  fints-cron:
    profiles: ["heavy"]  # OPTIONAL: Enable with --profile heavy
    image: alpine:${ALPINE_TAG:-latest}
    container_name: fints-cron
    hostname: fints-cron
    restart: unless-stopped
    networks:
      - default
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    volumes:
      - fints_importer_data:/data:ro
      - ./config/fints-importer:/import-configurations:ro
    depends_on:
      - fints-importer
      - firefly-iii
    command: >
      sh -c "
        apk add --no-cache curl;
        while true; do
          echo 'Running FinTS import at $(date)...';
          curl -f -X POST 'http://fints-importer:8080/autoimport' || echo 'Import failed or no configuration found.';
          echo 'Next import in 24 hours...';
          sleep 86400;
        done
      "
    mem_limit: 64m
    mem_reservation: 32m
    cpus: 0.25
    logging:
      options:
        max-size: "5m"
        max-file: "2"

  ############################################################################
  # IMMICH - Self-hosted Google Photos Alternative
  # Mobile app support for Android and iOS/iPad
  ############################################################################

  # immich-db removed; unified under 'postgres'

  immich-server:
    profiles: ["heavy"]  # OPTIONAL: Enable with --profile heavy (uses ~384MB)
    image: ghcr.io/immich-app/immich-server:${IMMICH_TAG:-release}
    container_name: immich-server
    hostname: immich-server
    restart: unless-stopped
    networks:
      - default
      - proxy
      - monitoring
    ports:
      - "${HOST_BIND:-0.0.0.0}:2283:3001"
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    environment:
      - TZ=Europe/Berlin
      - DB_HOSTNAME=postgres
      - DB_DATABASE_NAME=immich
      - DB_USERNAME=immich
      - DB_PASSWORD=${IMMICH_DB_PASSWORD}
      - REDIS_HOSTNAME=redis
      - REDIS_PORT=6379
      - REDIS_DBINDEX=2
      # Upload size limits
      - UPLOAD_LOCATION=/usr/src/app/upload
      # Machine learning (disabled on Le Potato - too heavy)
      - IMMICH_MACHINE_LEARNING_ENABLED=false
      # Reverse proxy
      - IMMICH_SERVER_URL=http://${HOST_ADDR:-192.168.178.40}:2283
    volumes:
      - /mnt/seconddrive/immich/upload:/usr/src/app/upload
      - /mnt/seconddrive/immich/library:/usr/src/app/library
      - /etc/localtime:/etc/localtime:ro
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
    healthcheck:
      test: ["CMD-SHELL", "wget --spider --quiet http://localhost:3001/api/server-info/ping || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "homepage.group=Media & Storage"
      - "homepage.name=Immich"
      - "homepage.icon=immich.png"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:2283"
      - "homepage.description=Photo & video management"

  immich-microservices:
    profiles: ["heavy"]  # OPTIONAL: Enable with --profile heavy (uses ~192MB)
    image: ghcr.io/immich-app/immich-server:${IMMICH_TAG:-release}
    container_name: immich-microservices
    hostname: immich-microservices
    restart: unless-stopped
    networks:
      - default
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    environment:
      - TZ=Europe/Berlin
      - DB_HOSTNAME=postgres
      - DB_DATABASE_NAME=immich
      - DB_USERNAME=immich
      - DB_PASSWORD=${IMMICH_DB_PASSWORD}
      - REDIS_HOSTNAME=redis
      - REDIS_PORT=6379
      - REDIS_DBINDEX=2
      - IMMICH_MACHINE_LEARNING_ENABLED=false
    volumes:
      - /mnt/seconddrive/immich/upload:/usr/src/app/upload
      - /mnt/seconddrive/immich/library:/usr/src/app/library
      - /etc/localtime:/etc/localtime:ro
    depends_on:
      - postgres
      - redis
    command: ["start.sh", "microservices"]
    mem_limit: 384m
    mem_reservation: 192m
    cpus: 1
    logging:
      options:
        max-size: "10m"
        max-file: "3"

  ############################################################################
  # IMMICH DATABASE BACKUP - Daily pg_dump
  ############################################################################

  immich-db-backup:
    profiles: ["heavy"]  # OPTIONAL: Enable with --profile heavy
    image: postgres:${POSTGRES_TAG:-14-alpine}
    container_name: immich-db-backup
    hostname: immich-db-backup
    restart: unless-stopped
    networks:
      - default
    environment:
      - PGPASSWORD=${IMMICH_DB_PASSWORD}
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    volumes:
      - /mnt/seconddrive/backups/db:/backups
    depends_on:
      postgres:
        condition: service_healthy
    command: >
      sh -c "
        set -o pipefail;
        while true; do
          echo 'Backing up Immich database...';
          pg_dump -h postgres -U immich -d immich | gzip > /backups/immich-db-$$(date +%Y-%m-%d-%H%M).sql.gz && echo 'Immich backup complete.' || echo 'Immich backup failed.';
          sleep 1d;
        done
      "
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.25
    logging:
      options:
        max-size: "2m"
        max-file: "2"
