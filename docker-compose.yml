################################################################################
# PotatoStack Main - Optimized for Mini PC (16GB RAM)
# Full-featured self-hosted stack with monitoring, automation, and media
# Target: Mini PC with 16GB RAM, 4+ core CPU, 1GB ethernet
################################################################################

x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"
    compress: "true"

x-common-env: &common-env
  TZ: Europe/Berlin
  PUID: 1000
  PGID: 1000

services:
  ################################################################################
  # Storage Init - Creates required directories on startup
  ################################################################################
  storage-init:
    image: alpine:${ALPINE_TAG:-latest}
    container_name: storage-init
    command: sh /init-storage.sh
    privileged: true
    environment:
      <<: *common-env
      SLSKD_API_KEY: ${SLSKD_API_KEY:-}
      SYNCTHING_API_KEY: ${SYNCTHING_API_KEY:-}
      ARIA2_RPC_SECRET: ${ARIA2_RPC_SECRET:-}
      SNAPSHOT_CRON_SCHEDULE: ${SNAPSHOT_CRON_SCHEDULE:-0 3 * * *}
      SNAPSHOT_PATHS: ${SNAPSHOT_PATHS:-/data}
      SNAPSHOT_LOG_FILE: ${SNAPSHOT_LOG_FILE:-/mnt/storage/kopia/stack-snapshot.log}
    volumes:
      - /mnt/storage:/mnt/storage
      - /mnt/cachehdd:/mnt/cachehdd
      - /mnt/ssd/docker-data:/mnt/ssd/docker-data
      - ./scripts/init/init-storage.sh:/init-storage.sh:ro
      - ./scripts/backup/stack-snapshot.sh:/stack-snapshot.sh:ro
      - shared-keys:/keys
    network_mode: none
    restart: "no"

  ################################################################################
  # CORE DATABASES
  ################################################################################

  # PostgreSQL - Primary database with pgvector for embeddings (SOTA 2025)
  postgres:
    image: pgvector/pgvector:${POSTGRES_TAG:-pg16}
    container_name: postgres
    logging: *default-logging
    shm_size: 1gb
    environment:
      <<: *common-env
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
      POSTGRES_MULTIPLE_DATABASES: ${POSTGRES_DATABASES:-authentik,gitea,woodpecker,immich,calibre,linkding,healthchecks,atuin,homarr,miniflux,grafana,mealie}
    command:
      - "postgres"
      - "-c"
      - "shared_buffers=1GB"
      - "-c"
      - "effective_cache_size=3GB"
      - "-c"
      - "work_mem=16MB"
      - "-c"
      - "maintenance_work_mem=256MB"
      - "-c"
      - "max_connections=200"
      - "-c"
      - "checkpoint_completion_target=0.9"
      - "-c"
      - "wal_buffers=16MB"
      - "-c"
      - "default_statistics_target=100"
      - "-c"
      - "random_page_cost=1.1"
      - "-c"
      - "effective_io_concurrency=200"
      - "-c"
      - "max_worker_processes=8"
      - "-c"
      - "max_parallel_workers=4"
      - "-c"
      - "max_parallel_workers_per_gather=2"
    volumes:
      - /mnt/ssd/docker-data/postgres:/var/lib/postgresql/data
      - ./scripts/init/init-postgres-multiple-dbs.sh:/docker-entrypoint-initdb.d/init-postgres-multiple-dbs.sh:ro
    tmpfs:
      - /tmp:exec,mode=1777
      - /var/run/postgresql:exec,mode=1777
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 512M
        reservations:
          cpus: "0.5"
          memory: 512M
    labels:
      - "potatostack.alerts=critical"

  # PgBouncer - PostgreSQL connection pooling (SOTA 2025)
  pgbouncer:
    image: edoburu/pgbouncer:${PGBOUNCER_TAG:-latest}
    container_name: pgbouncer
    logging: *default-logging
    environment:
      DATABASE_URL: postgres://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/postgres
      POOL_MODE: transaction
      DEFAULT_POOL_SIZE: 50
      MAX_CLIENT_CONN: 200
      MAX_DB_CONNECTIONS: 100
    networks:
      - potatostack
    depends_on:
      - postgres
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "pg_isready", "-h", "127.0.0.1", "-p", "5432"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 64M
    labels:
      - "potatostack.alerts=critical"

  # MongoDB - Document database (SOTA 2025)
  mongo:
    image: mongo:${MONGO_TAG:-7-jammy}
    container_name: mongo
    logging: *default-logging
    command:
      - "mongod"
      - "--wiredTigerCacheSizeGB=1.5"
      - "--wiredTigerJournalCompressor=snappy"
      - "--bind_ip_all"
    environment:
      <<: *common-env
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_ROOT_PASSWORD}
    volumes:
      - /mnt/ssd/docker-data/mongo:/data/db
      - /mnt/ssd/docker-data/mongo-config:/data/configdb
    tmpfs:
      - /tmp:size=64M  # Prevents stale socket file on restart
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 512M
        reservations:
          cpus: "0.5"
          memory: 512M
    labels:
      - "potatostack.alerts=critical"

  # Redis Shared Cache - SOTA 2025 consolidated (N8n, Gitea, Immich, Paperless, Sentry)
  redis-cache:
    image: redis:${REDIS_TAG:-7-alpine}
    container_name: redis-cache
    logging: *default-logging
    command: redis-server --maxmemory 1gb --maxmemory-policy allkeys-lfu --databases 16 --activedefrag yes --lazyfree-lazy-eviction yes --lazyfree-lazy-expire yes --lazyfree-lazy-server-del yes --save 60 1000 --appendonly yes --appendfsync everysec
    volumes:
      - /mnt/ssd/docker-data/redis-cache:/data
    tmpfs:
      - /tmp:exec,mode=1777
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 512M
    labels:
      - "potatostack.alerts=critical"

  ################################################################################
  # SECURITY & INTRUSION PREVENTION
  ################################################################################

  # CrowdSec - Modern IPS/IDS with community threat intelligence (SOTA 2025)
  crowdsec:
    image: crowdsecurity/crowdsec:${CROWDSEC_TAG:-latest}
    container_name: crowdsec
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.158}:6060:6060" # Prometheus metrics
    environment:
      <<: *common-env
      COLLECTIONS: "crowdsecurity/traefik crowdsecurity/http-cve crowdsecurity/whitelist-good-actors crowdsecurity/nginx crowdsecurity/linux"
      GID: "1000"
      METRICS_ENABLED: "true"
    volumes:
      - /mnt/ssd/docker-data/crowdsec-db:/var/lib/crowdsec/data/
      - /mnt/ssd/docker-data/crowdsec-config:/etc/crowdsec/
      - /var/log:/var/log:ro
      - traefik-logs:/logs:ro
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:8080/health || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 128M

  # CrowdSec Traefik Bouncer - Blocks malicious IPs at reverse proxy level
  crowdsec-traefik-bouncer:
    image: fbonalair/traefik-crowdsec-bouncer:${CROWDSEC_BOUNCER_TAG:-latest}
    container_name: crowdsec-traefik-bouncer
    logging: *default-logging
    environment:
      CROWDSEC_BOUNCER_API_KEY: ${CROWDSEC_BOUNCER_KEY}
      CROWDSEC_AGENT_HOST: crowdsec:8080
      GIN_MODE: release
    networks:
      - potatostack
    depends_on:
      - crowdsec
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M

  ################################################################################
  # REVERSE PROXY & SSL
  ################################################################################

  # Traefik - Modern reverse proxy with automatic SSL (SOTA 2025)
  traefik:
    image: traefik:${TRAEFIK_TAG:-latest}
    container_name: traefik
    logging: *default-logging
    security_opt:
      - no-new-privileges:true
    ports:
      - "80:80"
      - "443:443"
      - "${HOST_BIND:-192.168.178.158}:8088:8080" # Dashboard (alt port to avoid conflicts)
    environment:
      <<: *common-env
    command:
      # Global
      - "--global.checknewversion=false"
      - "--global.sendanonymoususage=false"
      # EntryPoints
      - "--entrypoints.web.address=:80"
      - "--entrypoints.web.http.redirections.entrypoint.to=websecure"
      - "--entrypoints.web.http.redirections.entrypoint.scheme=https"
      - "--entrypoints.web.http.redirections.entrypoint.permanent=true"
      - "--entrypoints.websecure.address=:443"
      - "--entrypoints.websecure.http.tls=true"
      - "--entrypoints.websecure.http.tls.options=default"
      - "--entrypoints.traefik.address=:8080"
      # Providers
      - "--providers.docker=true"
      - "--providers.docker.exposedbydefault=false"
      - "--providers.docker.network=potatostack_potatostack"
      - "--providers.file.directory=/etc/traefik/dynamic"
      - "--providers.file.watch=true"
      # API & Dashboard
      - "--api.dashboard=true"
      - "--api.insecure=false"
      - "--ping=true"
      - "--ping.entrypoint=traefik"
      # Observability
      - "--log.level=INFO"
      - "--accesslog=true"
      - "--accesslog.filepath=/logs/access.log"
      - "--accesslog.format=json"
      - "--accesslog.fields.headers.defaultmode=drop"
      - "--log.filepath=/logs/traefik.log"
      - "--metrics.prometheus=true"
      - "--metrics.prometheus.addServicesLabels=true"
      - "--metrics.prometheus.addEntryPointsLabels=true"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./config/traefik:/etc/traefik/dynamic:ro
      - ./config/traefik/certs:/etc/traefik/certs:ro
      - traefik-certs:/letsencrypt
      - traefik-logs:/logs
    networks:
      - potatostack
    depends_on:
      crowdsec-traefik-bouncer:
        condition: service_started
    restart: unless-stopped
    labels:
      - "potatostack.alerts=critical"
      - "traefik.enable=true"
      # Dashboard router
      - "traefik.http.routers.traefik.rule=Host(`traefik.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.traefik.entrypoints=websecure"
      - "traefik.http.routers.traefik.service=api@internal"
      - "traefik.http.routers.traefik.tls=true"
      - "traefik.http.routers.traefik.middlewares=sso-chain@docker"
      - "traefik.http.routers.traefik-gui.rule=Host(`traefik-gui.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.traefik-gui.entrypoints=websecure"
      - "traefik.http.routers.traefik-gui.service=api@internal"
      - "traefik.http.routers.traefik-gui.tls=true"
      - "traefik.http.routers.traefik-gui.middlewares=sso-chain@docker"
      - "traefik.http.middlewares.sso-chain.chain.middlewares=security-headers"
      # Security headers middleware
      - "traefik.http.middlewares.security-headers.headers.customResponseHeaders.X-Robots-Tag=noindex,nofollow,nosnippet,noarchive,notranslate,noimageindex"
      - "traefik.http.middlewares.security-headers.headers.sslRedirect=true"
      - "traefik.http.middlewares.security-headers.headers.stsSeconds=315360000"
      - "traefik.http.middlewares.security-headers.headers.stsIncludeSubdomains=true"
      - "traefik.http.middlewares.security-headers.headers.stsPreload=true"
      - "traefik.http.middlewares.security-headers.headers.forceSTSHeader=true"
      - "traefik.http.middlewares.security-headers.headers.frameDeny=true"
      - "traefik.http.middlewares.security-headers.headers.contentTypeNosniff=true"
      - "traefik.http.middlewares.security-headers.headers.browserXssFilter=true"
      - "traefik.http.middlewares.security-headers.headers.referrerPolicy=no-referrer"
      - "traefik.http.middlewares.security-headers.headers.permissionsPolicy=geolocation=(), microphone=(), camera=()"
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M

  ################################################################################
  # AUTHENTICATION & SECURITY
  ################################################################################

  # Authentik - SSO and 2FA provider (DISABLED)
  # authentik-server:
  #   image: ghcr.io/goauthentik/server:${AUTHENTIK_TAG:-latest}
  #   container_name: authentik-server
  #   logging: *default-logging
  #   command: server
  #   ports:
  #     - "${HOST_BIND:-192.168.178.158}:9000:9000"
  #     - "${HOST_BIND:-192.168.178.158}:9443:9443"
  #   environment:
  #     <<: *common-env
  #     AUTHENTIK_POSTGRESQL__HOST: postgres
  #     AUTHENTIK_POSTGRESQL__USER: postgres
  #     AUTHENTIK_POSTGRESQL__NAME: authentik
  #     AUTHENTIK_POSTGRESQL__PASSWORD: ${POSTGRES_SUPER_PASSWORD}
  #     AUTHENTIK_REDIS__HOST: redis-cache
  #     AUTHENTIK_SECRET_KEY: ${AUTHENTIK_SECRET_KEY}
  #     AUTHENTIK_ERROR_REPORTING__ENABLED: "false"
  #   volumes:
  #     - authentik-media:/media
  #     - authentik-custom-templates:/templates
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - postgres
  #     - redis-cache
  #   restart: unless-stopped
  #   healthcheck:
  #     test:
  #       [
  #         "CMD",
  #         "python3",
  #         "-c",
  #         "import requests; requests.get('http://127.0.0.1:9000/-/health/live/')",
  #       ]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 60s
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "2.0"
  #         memory: 512M
  #   labels:
  #     - "traefik.enable=true"
  #     - "traefik.http.routers.authentik.rule=Host(`authentik.${HOST_DOMAIN:-local.domain}`)"
  #     - "traefik.http.routers.authentik.entrypoints=websecure"
  #     - "traefik.http.routers.authentik.tls=true"
  #     - "traefik.http.services.authentik.loadbalancer.server.port=9000"
  #     - "traefik.http.routers.authentik.middlewares=security-headers@docker"

  # authentik-worker:
  #   image: ghcr.io/goauthentik/server:${AUTHENTIK_TAG:-latest}
  #   container_name: authentik-worker
  #   logging: *default-logging
  #   command: worker
  #   environment:
  #     <<: *common-env
  #     AUTHENTIK_POSTGRESQL__HOST: postgres
  #     AUTHENTIK_POSTGRESQL__USER: postgres
  #     AUTHENTIK_POSTGRESQL__NAME: authentik
  #     AUTHENTIK_POSTGRESQL__PASSWORD: ${POSTGRES_SUPER_PASSWORD}
  #     AUTHENTIK_REDIS__HOST: redis-cache
  #     AUTHENTIK_SECRET_KEY: ${AUTHENTIK_SECRET_KEY}
  #   volumes:
  #     - authentik-media:/media
  #     - authentik-certs:/certs
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - postgres
  #     - redis-cache
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "1.0"
  #         memory: 512M

  # Vaultwarden - Password manager and 2FA aggregator
  vaultwarden:
    image: vaultwarden/server:${VAULTWARDEN_TAG:-latest}
    container_name: vaultwarden
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.158}:8888:80"
      - "${HOST_BIND:-192.168.178.158}:3012:3012"
    environment:
      <<: *common-env
      DOMAIN: https://vault.${HOST_DOMAIN:-local.domain}
      ROCKET_PORT: 80
      WEBSOCKET_ENABLED: "true"
      WEBSOCKET_PORT: 3012
      SIGNUPS_ALLOWED: ${VAULTWARDEN_SIGNUPS_ALLOWED:-false}
      INVITATIONS_ALLOWED: ${VAULTWARDEN_INVITATIONS_ALLOWED:-true}
      ADMIN_TOKEN: ${VAULTWARDEN_ADMIN_TOKEN}
      DATABASE_URL: /data/db.sqlite3
      ICON_CACHE_TTL: 2592000
      LOG_LEVEL: warn
    volumes:
      - vaultwarden-data:/data
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://127.0.0.1:80/alive || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.vaultwarden.rule=Host(`vault.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.vaultwarden.entrypoints=websecure"
      - "traefik.http.routers.vaultwarden.tls=true"
      - "traefik.http.services.vaultwarden.loadbalancer.server.port=80"
      - "traefik.http.routers.vaultwarden.middlewares=sso-chain@docker"

  ################################################################################
  # DNS & AD BLOCKING
  ################################################################################

  # AdGuard Home - DNS-level ad blocking with encrypted DNS (SOTA 2025)
  # DISABLED: Commented out per user request
  # adguardhome:
  #   image: adguard/adguardhome:${ADGUARD_TAG:-latest}
  #   container_name: adguardhome
  #   logging: *default-logging
  #   ports:
  #     - "53:53/tcp"
  #     - "53:53/udp"
  #     - "${HOST_BIND:-192.168.178.158}:3053:3000"
  #     - "${HOST_BIND:-192.168.178.158}:8053:80"
  #   environment:
  #     <<: *common-env
  #   volumes:
  #     - adguard-work:/opt/adguardhome/work
  #     - adguard-conf:/opt/adguardhome/conf
  #   networks:
  #     - potatostack
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:80/ || exit 1"]
  #     interval: 60s
  #     timeout: 10s
  #     retries: 3
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.25"
  #         memory: 128M
  #       reservations:
  #         cpus: "0.1"
  #         memory: 128M
  #   labels:
  #     - "traefik.enable=true"
  #     - "traefik.http.routers.adguard.rule=Host(`dns.${HOST_DOMAIN:-local.domain}`)"
  #     - "traefik.http.routers.adguard.entrypoints=websecure"
  #     - "traefik.http.routers.adguard.tls=true"
  #     - "traefik.http.routers.adguard.middlewares=sso-chain@docker"
  #     - "traefik.http.services.adguard.loadbalancer.server.port=80"

  ################################################################################
  # VPN & NETWORKING
  ################################################################################

  # Gluetun - VPN client with killswitch
  gluetun:
    image: qmcgaw/gluetun:${GLUETUN_TAG:-latest}
    container_name: gluetun
    logging: *default-logging
    cap_add:
      - NET_ADMIN
    devices:
      - /dev/net/tun:/dev/net/tun
    ports:
      - "${HOST_BIND:-192.168.178.158}:8008:8008" # Gluetun control (moved from 8000)
      - "${HOST_BIND:-192.168.178.158}:8989:8989" # Sonarr
      - "${HOST_BIND:-192.168.178.158}:7878:7878" # Radarr
      - "${HOST_BIND:-192.168.178.158}:9696:9696" # Prowlarr
      - "${HOST_BIND:-192.168.178.158}:6767:6767" # Bazarr
      - "${HOST_BIND:-192.168.178.158}:8787:8787" # Bookshelf (ebook manager)
      - "${HOST_BIND:-192.168.178.158}:8686:8686" # Lidarr
      - "${HOST_BIND:-192.168.178.158}:8282:8282" # qBittorrent WebUI
      - "${HOST_BIND:-192.168.178.158}:51413:51413" # qBittorrent peer
      - "${HOST_BIND:-192.168.178.158}:51413:51413/udp"
      # aria2 ports REMOVED
      - "${HOST_BIND:-192.168.178.158}:2234:2234" # slskd WebUI
      - "${HOST_BIND:-192.168.178.158}:50000:50000" # slskd peer
      - "${HOST_BIND:-192.168.178.158}:8097:8080" # SpotiFLAC
      - "${HOST_BIND:-192.168.178.158}:8945:8945" # Pinchflat
      - "${HOST_BIND:-192.168.178.158}:8076:8000" # pyLoad WebUI (internal 8000)
      - "${HOST_BIND:-192.168.178.158}:9666:9666" # pyLoad Click'n'Load
      - "${HOST_BIND:-192.168.178.158}:9900:9900" # Stash
    environment:
      <<: *common-env
      VPN_SERVICE_PROVIDER: ${VPN_PROVIDER:-surfshark}
      VPN_TYPE: ${VPN_TYPE:-wireguard}
      WIREGUARD_PRIVATE_KEY: ${WIREGUARD_PRIVATE_KEY}
      WIREGUARD_ADDRESSES: ${WIREGUARD_ADDRESSES}
      SERVER_COUNTRIES: ${VPN_COUNTRY:-Germany}
      LOG_LEVEL: ${VPN_LOG_LEVEL:-info}
      FIREWALL_OUTBOUND_SUBNETS: ${LAN_NETWORK:-192.168.178.0/24}
      FIREWALL_VPN_INPUT_PORTS: ${VPN_INPUT_PORTS:-51413,50000,6888}
      FIREWALL: "on"
      DNS_ADDRESS: ${VPN_DNS:-1.1.1.1}
      DOT: "off"
      HTTPPROXY: "off"
      SHADOWSOCKS: "off"
      HTTP_CONTROL_SERVER_ADDRESS: :8008  # Changed from 8000 to avoid pyload conflict
      HTTP_CONTROL_SERVER_LOG: "off"
      HTTP_CONTROL_SERVER_AUTH: "off"
      IPV6: "off"
      UPDATER_PERIOD: 24h
      HEALTH_VPN_DURATION_INITIAL: 60s
      HEALTH_VPN_DURATION_ADDITION: 10s
    volumes:
      - gluetun-config:/gluetun
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "/gluetun-entrypoint", "healthcheck"]
      interval: 120s
      timeout: 10s
      retries: 3
      start_period: 120s
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 128M
    labels:
      - "autoheal=true"
      - "potatostack.alerts=critical"
      - "traefik.enable=true"
      - "traefik.http.routers.gluetun.rule=Host(`gluetun.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.gluetun.entrypoints=websecure"
      - "traefik.http.routers.gluetun.tls=true"
      - "traefik.http.routers.gluetun.service=gluetun"
      - "traefik.http.services.gluetun.loadbalancer.server.port=8008"
      - "traefik.http.routers.gluetun.middlewares=sso-chain@docker"
      - "traefik.http.routers.prowlarr.rule=Host(`prowlarr.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.prowlarr.entrypoints=websecure"
      - "traefik.http.routers.prowlarr.tls=true"
      - "traefik.http.routers.prowlarr.service=prowlarr"
      - "traefik.http.services.prowlarr.loadbalancer.server.port=9696"
      - "traefik.http.routers.prowlarr.middlewares=sso-chain@docker"
      - "traefik.http.routers.sonarr.rule=Host(`sonarr.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.sonarr.entrypoints=websecure"
      - "traefik.http.routers.sonarr.tls=true"
      - "traefik.http.routers.sonarr.service=sonarr"
      - "traefik.http.services.sonarr.loadbalancer.server.port=8989"
      - "traefik.http.routers.sonarr.middlewares=sso-chain@docker"
      - "traefik.http.routers.radarr.rule=Host(`radarr.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.radarr.entrypoints=websecure"
      - "traefik.http.routers.radarr.tls=true"
      - "traefik.http.routers.radarr.service=radarr"
      - "traefik.http.services.radarr.loadbalancer.server.port=7878"
      - "traefik.http.routers.radarr.middlewares=sso-chain@docker"
      - "traefik.http.routers.lidarr.rule=Host(`lidarr.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.lidarr.entrypoints=websecure"
      - "traefik.http.routers.lidarr.tls=true"
      - "traefik.http.routers.lidarr.service=lidarr"
      - "traefik.http.services.lidarr.loadbalancer.server.port=8686"
      - "traefik.http.routers.lidarr.middlewares=sso-chain@docker"
      - "traefik.http.routers.bookshelf.rule=Host(`bookshelf.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.bookshelf.entrypoints=websecure"
      - "traefik.http.routers.bookshelf.tls=true"
      - "traefik.http.routers.bookshelf.service=bookshelf"
      - "traefik.http.services.bookshelf.loadbalancer.server.port=8787"
      - "traefik.http.routers.bookshelf.middlewares=sso-chain@docker"
      - "traefik.http.routers.bazarr.rule=Host(`bazarr.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.bazarr.entrypoints=websecure"
      - "traefik.http.routers.bazarr.tls=true"
      - "traefik.http.routers.bazarr.service=bazarr"
      - "traefik.http.services.bazarr.loadbalancer.server.port=6767"
      - "traefik.http.routers.bazarr.middlewares=sso-chain@docker"
      - "traefik.http.routers.qbittorrent.rule=Host(`qbittorrent.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.qbittorrent.entrypoints=websecure"
      - "traefik.http.routers.qbittorrent.tls=true"
      - "traefik.http.routers.qbittorrent.service=qbittorrent"
      - "traefik.http.services.qbittorrent.loadbalancer.server.port=8282"
      - "traefik.http.routers.qbittorrent.middlewares=sso-chain@docker"
      # aria2/ariang traefik labels REMOVED
      - "traefik.http.routers.slskd.rule=Host(`slskd.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.slskd.entrypoints=websecure"
      - "traefik.http.routers.slskd.tls=true"
      - "traefik.http.routers.slskd.service=slskd"
      - "traefik.http.services.slskd.loadbalancer.server.port=2234"
      - "traefik.http.routers.slskd.middlewares=sso-chain@docker"
      - "traefik.http.routers.pinchflat.rule=Host(`pinchflat.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.pinchflat.entrypoints=websecure"
      - "traefik.http.routers.pinchflat.tls=true"
      - "traefik.http.routers.pinchflat.service=pinchflat"
      - "traefik.http.services.pinchflat.loadbalancer.server.port=8945"
      - "traefik.http.routers.pinchflat.middlewares=sso-chain@docker"
      - "traefik.http.routers.spotiflac.rule=Host(`spotiflac.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.spotiflac.entrypoints=websecure"
      - "traefik.http.routers.spotiflac.tls=true"
      - "traefik.http.routers.spotiflac.service=spotiflac"
      - "traefik.http.services.spotiflac.loadbalancer.server.port=8080"
      - "traefik.http.routers.spotiflac.middlewares=sso-chain@docker"
      - "traefik.http.routers.pyload.rule=Host(`pyload.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.pyload.entrypoints=websecure"
      - "traefik.http.routers.pyload.tls=true"
      - "traefik.http.routers.pyload.service=pyload"
      - "traefik.http.services.pyload.loadbalancer.server.port=8000"
      - "traefik.http.routers.pyload.middlewares=sso-chain@docker"

  # WireGuard - REMOVED (using Gluetun + Tailscale instead)
  # wireguard:
  #   image: lscr.io/linuxserver/wireguard:${WIREGUARD_TAG:-latest}
  #   container_name: wireguard
  #   logging: *default-logging
  #   cap_add:
  #     - NET_ADMIN
  #     - SYS_MODULE
  #   ports:
  #     - "${HOST_BIND:-192.168.178.158}:${WIREGUARD_SERVERPORT:-51820}:51820/udp"
  #   environment:
  #     <<: *common-env
  #     SERVERURL: ${WIREGUARD_SERVERURL:-auto}
  #     SERVERPORT: ${WIREGUARD_SERVERPORT:-51820}
  #     PEERS: ${WIREGUARD_PEERS:-vps,android,laptop,tablet,raspberry}
  #     PEERDNS: ${WIREGUARD_PEERDNS:-auto}
  #     INTERNAL_SUBNET: ${WIREGUARD_INTERNAL_SUBNET:-10.13.13.0/24}
  #     ALLOWEDIPS: ${WIREGUARD_ALLOWEDIPS:-0.0.0.0/0}
  #     LOG_CONFS: ${WIREGUARD_LOG_CONFS:-true}
  #   volumes:
  #     - /mnt/ssd/docker-data/wireguard:/config
  #     - /lib/modules:/lib/modules:ro
  #   sysctls:
  #     - net.ipv4.conf.all.src_valid_mark=1
  #   networks:
  #     - potatostack
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.25"
  #         memory: 128M

  # Tailscale - Mesh VPN for remote access (SOTA 2025 - easiest option)
  tailscale:
    image: tailscale/tailscale:${TAILSCALE_TAG:-latest}
    container_name: tailscale
    logging: *default-logging
    hostname: potatostack
    environment:
      <<: *common-env
      TS_AUTHKEY: ${TAILSCALE_AUTHKEY}
      TS_STATE_DIR: /var/lib/tailscale
      TS_USERSPACE: "false"
    volumes:
      - tailscale-data:/var/lib/tailscale
      - /dev/net/tun:/dev/net/tun
      - tailscale-https-marker:/https-marker
    cap_add:
      - NET_ADMIN
      - SYS_MODULE
    network_mode: host
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "tailscale status --json 2>/dev/null | grep -q '\"Self\"' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 192M

  # Tailscale HTTPS Setup - Wraps HTTP ports with Tailscale TLS certificates
  tailscale-https-setup:
    image: alpine:${ALPINE_TAG:-latest}
    container_name: tailscale-https-setup
    logging: *default-logging
    entrypoint: ["/bin/sh", "/init.sh"]
    environment:
      <<: *common-env
      TAILSCALE_CONTAINER: tailscale
      # All service ports to expose via HTTPS
      # aria2 ports removed, pyload (8076) added, filestash (8095) added
      TAILSCALE_SERVE_PORTS: "7575,8088,3001,3002,8089,8096,5055,8989,7878,8686,9696,6767,8787,13378,4533,8945,9900,8282,8076,2234,8097,8008,2283,8090,8095,8080,8384,51515,3004,3006,9000,9443,8888,8288,9091,8093,5006,9090,3100,9093,10903,10902,8087,6060,8091,8001,8788,8060,8889,8081,3010,8085,9925,5984"
      # Special port mappings (external:internal)
      TAILSCALE_SERVE_SPECIAL: ""
      TAILSCALE_MARKER_FILE: /https-marker/setup-complete
    volumes:
      - ./scripts/init/tailscale-serve-https.sh:/init.sh:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - tailscale-https-marker:/https-marker
    network_mode: host
    depends_on:
      tailscale:
        condition: service_healthy
    restart: "no"
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M

  # Tailscale HTTPS Monitor - Periodically re-applies HTTPS serve rules
  tailscale-https-monitor:
    image: alpine:${ALPINE_TAG:-latest}
    container_name: tailscale-https-monitor
    logging: *default-logging
    entrypoint: ["/bin/sh", "/init.sh"]
    environment:
      <<: *common-env
      TAILSCALE_CONTAINER: tailscale
      # aria2 ports removed, pyload (8076) added, filestash (8095) added
      TAILSCALE_SERVE_PORTS: "7575,8088,3001,3002,8089,8096,5055,8989,7878,8686,9696,6767,8787,13378,4533,8945,9900,8282,8076,2234,8097,8008,2283,8090,8095,8080,8384,51515,3004,3006,9000,9443,8888,8288,9091,8093,5006,9090,3100,9093,10903,10902,8087,6060,8091,8001,8788,8060,8889,8081,3010,8085,9925,5984"
      # Special port mappings (external:internal)
      TAILSCALE_SERVE_SPECIAL: ""
      TAILSCALE_SERVE_LOOP: "true"
      TAILSCALE_SERVE_INTERVAL: "300"
      TAILSCALE_MARKER_FILE: /https-marker/monitor-alive
    volumes:
      - ./scripts/init/tailscale-serve-https.sh:/init.sh:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - tailscale-https-marker:/https-marker
    network_mode: host
    depends_on:
      tailscale:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M

  ################################################################################
  # CLOUD STORAGE & FILE SYNC
  ################################################################################

  # # Nextcloud AIO - DISABLED
  # nextcloud-aio:
  #   image: nextcloud/all-in-one:${NEXTCLOUD_AIO_TAG:-latest}
  #   container_name: nextcloud-aio-mastercontainer
  #   logging: *default-logging
  #   init: true
  #   ports:
  #     - "${HOST_BIND:-192.168.178.158}:8080:8080"
  #   environment:
  #     PUID: 1000
  #     PGID: 1000
  #     APACHE_PORT: 11000
  #     APACHE_IP_BINDING: 0.0.0.0
  #     NEXTCLOUD_DATADIR: /mnt/storage/nextcloud/data
  #     NEXTCLOUD_MOUNT: /mnt/storage/nextcloud
  #     NEXTCLOUD_UPLOAD_LIMIT: 10G
  #     NEXTCLOUD_MAX_TIME: 3600
  #     NEXTCLOUD_MEMORY_LIMIT: 512M
  #     SKIP_DOMAIN_VALIDATION: "true"
  #   dns:
  #     - 100.100.100.100
  #     - 1.1.1.1
  #     - 8.8.8.8
  #   volumes:
  #     - nextcloud_aio_mastercontainer:/mnt/docker-aio-config
  #     - /var/run/docker.sock:/var/run/docker.sock:ro
  #     - /mnt/storage/nextcloud:/mnt/storage/nextcloud:rw
  #   networks:
  #     - potatostack
  #   depends_on:
  #     storage-init:
  #       condition: service_completed_successfully
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "1.0"
  #         memory: 128M
  #   labels:
  #     - "traefik.enable=true"
  #     - "traefik.http.routers.nextcloud-aio.rule=Host(`nextcloud.${HOST_DOMAIN:-local.domain}`)"
  #     - "traefik.http.routers.nextcloud-aio.entrypoints=websecure"
  #     - "traefik.http.routers.nextcloud-aio.tls=true"
  #     - "traefik.http.services.nextcloud-aio.loadbalancer.server.port=11000"
  #     - "traefik.http.routers.nextcloud-aio.middlewares=security-headers@docker"

  # Syncthing - P2P file sync
  syncthing:
    image: lscr.io/linuxserver/syncthing:${SYNCTHING_TAG:-latest}
    container_name: syncthing
    logging: *default-logging
    entrypoint: bash /syncthing-init.sh
    ports:
      - "${HOST_BIND:-192.168.178.158}:8384:8384"
      - "${HOST_BIND:-192.168.178.158}:22000:22000/tcp"
      - "${HOST_BIND:-192.168.178.158}:22000:22000/udp"
      - "${HOST_BIND:-192.168.178.158}:21027:21027/udp"
    environment:
      <<: *common-env
    volumes:
      - syncthing-config:/config
      - /mnt/storage/syncthing:/data
      - /mnt/cachehdd/sync/syncthing-versions:/data/.stversions
      - shared-keys:/keys:ro
      - ./scripts/init/syncthing-init.sh:/syncthing-init.sh:ro
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8384/rest/noauth/health"]
      interval: 120s
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.syncthing.rule=Host(`syncthing.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.syncthing.entrypoints=websecure"
      - "traefik.http.routers.syncthing.tls=true"
      - "traefik.http.services.syncthing.loadbalancer.server.port=8384"

  # Filebrowser - Web-based file manager with full RW access
  filebrowser:
    image: filebrowser/filebrowser:${FILEBROWSER_TAG:-latest}
    container_name: filebrowser
    logging: *default-logging
    user: "${PUID:-1000}:${PGID:-1000}"
    entrypoint: sh /filebrowser-init.sh
    ports:
      - "${HOST_BIND:-192.168.178.158}:8090:80"
    environment:
      <<: *common-env
      FILEBROWSER_USER: ${FILEBROWSER_USER:-admin}
      FILEBROWSER_PASSWORD: ${FILEBROWSER_PASSWORD:-}
    volumes:
      - /mnt/storage:/srv/storage
      - /mnt/cachehdd:/srv/cachehdd
      - /mnt/ssd/docker-data:/srv/docker-data
      - /mnt/ssd/docker-data/filebrowser:/config
      - ./scripts/init/filebrowser-init.sh:/filebrowser-init.sh:ro
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:80/health || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.filebrowser.rule=Host(`filebrowser.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.filebrowser.entrypoints=websecure"
      - "traefik.http.routers.filebrowser.tls=true"
      - "traefik.http.services.filebrowser.loadbalancer.server.port=80"
      - "traefik.http.routers.filebrowser.middlewares=sso-chain@docker"

  # Filestash - Advanced web file manager with multi-protocol support (SOTA 2025)
  # Supports: Local, SFTP, FTP, S3, WebDAV, Git, LDAP, Dropbox, Google Drive
  # Features: Image/video/music previews, office doc viewer, text editor
  # Docs: https://www.filestash.app/docs/
  filestash:
    image: machines/filestash:${FILESTASH_TAG:-latest}
    container_name: filestash
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.158}:8095:8334"
    environment:
      <<: *common-env
      # Disable cloud integrations
      GDRIVE_CLIENT_ID: ""
      DROPBOX_CLIENT_ID: ""
    volumes:
      # Full storage access (same as filebrowser)
      - /mnt/storage:/mnt/storage
      - /mnt/cachehdd:/mnt/cachehdd
      - /mnt/ssd/docker-data:/mnt/docker-data:ro
      # Config persistence
      - /mnt/ssd/docker-data/filestash:/app/data/state
      # Plugins directory
      - /mnt/ssd/docker-data/filestash/plugins:/app/data/state/plugins
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://127.0.0.1:8334/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.1"
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.filestash.rule=Host(`filestash.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.filestash.entrypoints=websecure"
      - "traefik.http.routers.filestash.tls=true"
      - "traefik.http.services.filestash.loadbalancer.server.port=8334"
      - "traefik.http.routers.filestash.middlewares=sso-chain@docker"

   ################################################################################
   # KNOWLEDGE MANAGEMENT
   ################################################################################

  # Obsidian LiveSync - CouchDB backend for Obsidian note synchronization
  obsidian-livesync:
    image: oleduc/docker-obsidian-livesync-couchdb:${OBSIDIAN_LIVESYNC_TAG:-latest}
    container_name: obsidian-livesync
    logging: *default-logging
    ports:
      - "127.0.0.1:5984:5984"
    environment:
      <<: *common-env
      COUCHDB_USER: ${COUCHDB_USER:-obsidian}
      COUCHDB_PASSWORD: ${COUCHDB_PASSWORD}
      COUCHDB_DATABASE: ${COUCHDB_DATABASE:-obsidian-vault}
      COUCHDB_CORS_ORIGINS: https://obsidian.${HOST_DOMAIN:-local.domain},app://obsidian.md
    volumes:
      - /mnt/storage/obsidian-couchdb:/opt/couchdb/data
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f -u $$COUCHDB_USER:$$COUCHDB_PASSWORD http://127.0.0.1:5984/_up || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.1"
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.obsidian.rule=Host(`obsidian.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.obsidian.entrypoints=websecure"
      - "traefik.http.routers.obsidian.tls=true"
      - "traefik.http.services.obsidian.loadbalancer.server.port=5984"


  # Obsidian LiveSync Init - Single-node CouchDB bootstrap
  obsidian-livesync-init:
    image: alpine:${ALPINE_TAG:-latest}
    container_name: obsidian-livesync-init
    logging: *default-logging
    entrypoint:
      - "/bin/sh"
      - "-c"
      - "apk add --no-cache bash curl >/dev/null 2>&1 && /bin/bash /init-obsidian-livesync.sh"
    environment:
      <<: *common-env
      COUCHDB_HOST: "obsidian-livesync"
      COUCHDB_PORT: "5984"
      COUCHDB_USER: ${COUCHDB_USER:-obsidian}
      COUCHDB_PASSWORD: ${COUCHDB_PASSWORD}
      COUCHDB_DATABASE: ${COUCHDB_DATABASE:-obsidian-vault}
    volumes:
      - ./scripts/init/obsidian-livesync-init.sh:/init-obsidian-livesync.sh:ro
    networks:
      - potatostack
    depends_on:
      - obsidian-livesync
    restart: "no"

   ################################################################################
   # RSS & NEWS AGGREGATION
   ################################################################################

  # Miniflux - Minimalist RSS reader with Postgres backend (SOTA 2025)
  miniflux:
    image: miniflux/miniflux:${MINIFLUX_TAG:-latest}
    container_name: miniflux
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.158}:8093:8080"
    environment:
      <<: *common-env
      DATABASE_URL: postgres://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/miniflux?sslmode=disable
      RUN_MIGRATIONS: "1"
      CREATE_ADMIN: "1"
      ADMIN_USERNAME: ${MINIFLUX_ADMIN_USER:-admin}
      ADMIN_PASSWORD: ${MINIFLUX_ADMIN_PASSWORD}
      BASE_URL: https://rss.${HOST_DOMAIN:-local.domain}
      POLLING_FREQUENCY: 60
      BATCH_SIZE: 100
      WORKER_POOL_SIZE: 5
      METRICS_COLLECTOR: "1"
      METRICS_ALLOWED_NETWORKS: 172.22.0.0/16
    networks:
      - potatostack
    depends_on:
      - postgres
      - pgbouncer
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "/usr/bin/miniflux", "-healthcheck", "auto"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 64M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.miniflux.rule=Host(`rss.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.miniflux.entrypoints=websecure"
      - "traefik.http.routers.miniflux.tls=true"
      - "traefik.http.routers.miniflux.middlewares=sso-chain@docker"
      - "traefik.http.services.miniflux.loadbalancer.server.port=8080"

  ################################################################################
  # RECIPE MANAGEMENT
  ################################################################################

  # Mealie - Recipe management and meal planning (SOTA 2025)
  mealie:
    image: ghcr.io/mealie-recipes/mealie:${MEALIE_TAG:-latest}
    container_name: mealie
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.158}:9925:9000"
    environment:
      <<: *common-env
      # Core settings
      BASE_URL: https://mealie.${HOST_DOMAIN:-local.domain}
      API_PORT: 9000
      SECRET_KEY: ${MEALIE_SECRET_KEY}
      ALLOW_SIGNUP: "false"                    # Use Authentik SSO instead
      LOG_LEVEL: info
      # PostgreSQL (your primary DB)
      POSTGRES_SERVER: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: mealie
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
      # Redis (your shared redis-cache)
      REDIS_URL: redis://redis-cache:6379/10   # DB 10 - avoid conflicts
      # Optional nice-to-haves
      MEALIE_LOG_LEVEL: info
      MEALIE_THEME: light                      # or system, dark
      MEALIE_DEFAULT_GROUP: Home
      MEALIE_DEFAULT_HOUSEHOLD: Main
      # Disable built-in auth (Traefik + Authentik will handle it)
      OIDC_ENABLED: "false"
    volumes:
      - /mnt/storage/mealie-data:/app/data
    networks:
      - potatostack
    depends_on:
      postgres:
        condition: service_healthy
      redis-cache:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "/opt/mealie/bin/python -c \"import urllib.request; urllib.request.urlopen('http://127.0.0.1:9000/api/app/about', timeout=5)\""]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 45s
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 512M
        reservations:
          cpus: "0.5"
          memory: 256M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.mealie.rule=Host(`mealie.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.mealie.entrypoints=websecure"
      - "traefik.http.routers.mealie.tls=true"
      - "traefik.http.routers.mealie.middlewares=sso-chain@docker"
      - "traefik.http.services.mealie.loadbalancer.server.port=9000"

  ################################################################################
  # FINANCE
  ################################################################################

  # Actual Budget - Modern budgeting with bank sync (SOTA 2025)
  actual-budget:
    image: actualbudget/actual-server:${ACTUAL_TAG:-latest}
    container_name: actual-budget
    logging: *default-logging
    # NOTE: Access via Traefik HTTPS (https://budget.danielhomelab.local) for SharedArrayBuffer support
    # Direct port access won't work as browsers require COOP/COEP headers only sent via secure context
    ports:
      - "${HOST_BIND:-192.168.178.158}:5006:5006"
    environment:
      <<: *common-env
    volumes:
      - actual-data:/data
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "timeout 5 bash -c 'cat < /dev/null > /dev/tcp/127.0.0.1/5006' || exit 1",
        ]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.actual.rule=Host(`budget.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.actual.entrypoints=websecure"
      - "traefik.http.routers.actual.tls=true"
      # Use shared-array-chain for SharedArrayBuffer support (COOP/COEP headers)
      - "traefik.http.routers.actual.middlewares=shared-array-chain@file"
      - "traefik.http.services.actual.loadbalancer.server.port=5006"

  ################################################################################
  # MEDIA MANAGEMENT - *ARR STACK
  ################################################################################

  # Prowlarr - Indexer manager
  prowlarr:
    image: lscr.io/linuxserver/prowlarr:${PROWLARR_TAG:-latest}
    container_name: prowlarr
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
    volumes:
      - prowlarr-config:/config
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:9696/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 192M
        reservations:
          cpus: "0.1"
          memory: 128M
    labels:
      - "autoheal=true"

  # Sonarr - TV show management
  sonarr:
    image: lscr.io/linuxserver/sonarr:${SONARR_TAG:-latest}
    container_name: sonarr
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
    volumes:
      - sonarr-config:/config
      - /mnt/storage/media/tv:/tv
      - /mnt/storage/downloads:/downloads
      - /mnt/cachehdd/downloads/torrent:/downloads/incomplete
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://127.0.0.1:8989/ping || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 256M
        reservations:
          cpus: "0.25"
          memory: 128M
    labels:
      - "autoheal=true"

  # Radarr - Movie management
  radarr:
    image: lscr.io/linuxserver/radarr:${RADARR_TAG:-latest}
    container_name: radarr
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
    volumes:
      - radarr-config:/config
      - /mnt/storage/media/movies:/movies
      - /mnt/storage/downloads:/downloads
      - /mnt/cachehdd/downloads/torrent:/downloads/incomplete
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:7878/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 256M
        reservations:
          cpus: "0.25"
          memory: 128M
    labels:
      - "autoheal=true"

  # Lidarr - Music management
  lidarr:
    image: lscr.io/linuxserver/lidarr:${LIDARR_TAG:-latest}
    container_name: lidarr
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
    volumes:
      - lidarr-config:/config
      - /mnt/storage/media/music:/music
      - /mnt/storage/downloads:/downloads
      - /mnt/cachehdd/downloads/torrent:/downloads/incomplete
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:8686/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 256M
        reservations:
          cpus: "0.25"
          memory: 128M
    labels:
      - "autoheal=true"

  # Bookshelf - Ebook management (Readarr revival with working metadata)
  bookshelf:
    image: ghcr.io/pennydreadful/bookshelf:${BOOKSHELF_TAG:-hardcover}
    container_name: bookshelf
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
    volumes:
      - bookshelf-config:/config
      - /mnt/storage/media/books:/books
      - /mnt/storage/downloads:/downloads
      - /mnt/cachehdd/downloads/torrent:/downloads/incomplete
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:8787/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 256M
        reservations:
          cpus: "0.25"
          memory: 128M
    labels:
      - "autoheal=true"

  # Bazarr - Subtitle management
  bazarr:
    image: lscr.io/linuxserver/bazarr:${BAZARR_TAG:-latest}
    container_name: bazarr
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
    volumes:
      - bazarr-config:/config
      - /mnt/storage/media/movies:/movies
      - /mnt/storage/media/tv:/tv
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      sonarr:
        condition: service_started
      radarr:
        condition: service_started
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:6767/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 256M
        reservations:
          cpus: "0.1"
          memory: 128M
    labels:
      - "autoheal=true"

  # Maintainerr - DISABLED: Media library cleanup
  # maintainerr:
  #   image: jorenn92/maintainerr:${MAINTAINERR_TAG:-latest}
  #   container_name: maintainerr
  #   logging: *default-logging
  #   ports:
  #     - "${HOST_BIND:-192.168.178.158}:6246:6246"
  #   environment:
  #     <<: *common-env
  #   volumes:
  #     - maintainerr-data:/opt/data
  #   networks:
  #     - potatostack
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.25"
  #         memory: 192M
  #   labels:
  #     - "traefik.enable=true"
  #     - "traefik.http.routers.maintainerr.rule=Host(`maintainerr.${HOST_DOMAIN:-local.domain}`)"
  #     - "traefik.http.routers.maintainerr.entrypoints=websecure"
  #     - "traefik.http.routers.maintainerr.tls=true"
  #     - "traefik.http.services.maintainerr.loadbalancer.server.port=6246"
  #     - "traefik.http.routers.maintainerr.middlewares=sso-chain@docker"

  ################################################################################
  # MEDIA SERVERS & REQUEST MANAGEMENT
  ################################################################################

  # Jellyfin - Media server (SOTA 2025 with HW acceleration support)
  jellyfin:
    image: lscr.io/linuxserver/jellyfin:${JELLYFIN_TAG:-latest}
    container_name: jellyfin
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.158}:8096:8096"
      - "${HOST_BIND:-192.168.178.158}:8920:8920"
      - "${HOST_BIND:-192.168.178.158}:7359:7359/udp"
      - "${HOST_BIND:-192.168.178.158}:1900:1900/udp"
    environment:
      <<: *common-env
    # Uncomment group_add and devices for Intel/AMD hardware acceleration
    # Get render group ID with: getent group render | cut -d: -f3
    # group_add:
    #   - "107"  # render group - adjust to match your system
    # devices:
    #   - /dev/dri/renderD128:/dev/dri/renderD128
    devices:
      - /dev/dri/renderD128:/dev/dri/renderD128
      - /dev/dri/card0:/dev/dri/card0
    group_add:
      - "109"
    volumes:
      - jellyfin-config:/config
      - /mnt/storage/media/tv:/data/tvshows
      - /mnt/storage/media/movies:/data/movies
      - /mnt/storage/media/music:/data/music
      - /mnt/storage/media/audiobooks:/data/audiobooks
      - /mnt/cachehdd/media/jellyfin:/cache
    tmpfs:
      - /tmp:exec,mode=1777,size=500m
      - /transcode:exec,mode=1777,size=2g
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:8096/health || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: "2.5"
          memory: 2G
        reservations:
          cpus: "1.0"
          memory: 1G
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.jellyfin.rule=Host(`jellyfin.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.jellyfin.entrypoints=websecure"
      - "traefik.http.routers.jellyfin.tls=true"
      - "traefik.http.services.jellyfin.loadbalancer.server.port=8096"
      - "traefik.http.routers.jellyfin.middlewares=sso-chain@docker"

  # Jellyseerr - Media request management for Jellyfin
  jellyseerr:
    image: fallenbagel/jellyseerr:${JELLYSEERR_TAG:-latest}
    container_name: jellyseerr
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.158}:5055:5055"
    environment:
      <<: *common-env
    volumes:
      - /mnt/ssd/docker-data/jellyseerr:/app/config
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 192M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.jellyseerr.rule=Host(`jellyseerr.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.jellyseerr.entrypoints=websecure"
      - "traefik.http.routers.jellyseerr.tls=true"
      - "traefik.http.services.jellyseerr.loadbalancer.server.port=5055"
      - "traefik.http.routers.jellyseerr.middlewares=sso-chain@docker"

  # Audiobookshelf - Audiobook and podcast server
  audiobookshelf:
    image: ghcr.io/advplyr/audiobookshelf:${AUDIOBOOKSHELF_TAG:-latest}
    container_name: audiobookshelf
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.158}:13378:80"
    environment:
      <<: *common-env
    volumes:
      - audiobookshelf-config:/config
      - /mnt/cachehdd/media/audiobookshelf/metadata:/metadata
      - /mnt/storage/media/audiobooks:/audiobooks
      - /mnt/storage/media/podcasts:/podcasts
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.audiobookshelf.rule=Host(`audiobooks.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.audiobookshelf.entrypoints=websecure"
      - "traefik.http.routers.audiobookshelf.tls=true"
      - "traefik.http.services.audiobookshelf.loadbalancer.server.port=80"
      - "traefik.http.routers.audiobookshelf.middlewares=sso-chain@docker"

  # Navidrome - Music streaming server (Subsonic/Airsonic compatible)
  navidrome:
    image: deluan/navidrome:${NAVIDROME_TAG:-latest}
    container_name: navidrome
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.158}:4533:4533"
    environment:
      <<: *common-env
      ND_SCANSCHEDULE: 1h
      ND_LOGLEVEL: info
      ND_SESSIONTIMEOUT: 24h
      ND_BASEURL: ""
      ND_ENABLETRANSCODINGCONFIG: "true"
      ND_ENABLESHARING: "true"
    volumes:
      - navidrome-data:/data
      - /mnt/storage/media/music:/music:ro
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:4533/ping || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 256M
        reservations:
          cpus: "0.25"
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.navidrome.rule=Host(`music.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.navidrome.entrypoints=websecure"
      - "traefik.http.routers.navidrome.tls=true"
      - "traefik.http.services.navidrome.loadbalancer.server.port=4533"
      - "traefik.http.routers.navidrome.middlewares=sso-chain@docker"

  # SpotiFLAC - Spotify to FLAC downloader (Tidal/Qobuz/Amazon sources)
  spotiflac:
    image: neiht/spotiflac:latest@sha256:65de2009536579cda0311b2d9606faa70bfc5bc9962f96f8ad150e5e82b3da23
    container_name: spotiflac
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
      SERVER_PORT: 8080
      SERVER_HOST: 0.0.0.0
      DOWNLOAD_DIR: /downloads
      STATIC_DIR: /app/dist
    volumes:
      - /mnt/storage/downloads:/downloads
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 128M
    labels:
      - "potatostack.alerts=critical"
      - "autoheal=true"

  ################################################################################
  # DOWNLOAD CLIENTS (behind VPN)
  ################################################################################

  # qBittorrent - Torrent client
  qbittorrent:
    image: lscr.io/linuxserver/qbittorrent:${QBITTORRENT_TAG:-latest}
    container_name: qbittorrent
    logging: *default-logging
    network_mode: "service:gluetun"
    entrypoint: /qb-init.sh
    environment:
      <<: *common-env
      WEBUI_PORT: 8282
      QBITTORRENT_USER: ${QBITTORRENT_USER:-admin}
      QBITTORRENT_PASSWORD: ${QBITTORRENT_PASSWORD:-}
    volumes:
      - qbittorrent-config:/config
      - /mnt/storage/downloads:/downloads
      - /mnt/cachehdd/downloads/torrent:/incomplete
      - ./scripts/init/qbittorrent-init.sh:/qb-init.sh:ro
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:8282/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 128M
        reservations:
          cpus: "0.25"
          memory: 128M
    labels:
      - "autoheal=true"

  # Aria2 - REMOVED (using qBittorrent instead)
  # aria2:
  #   image: p3terx/aria2-pro:${ARIA2_TAG:-latest}
  #   container_name: aria2
  #   logging: *default-logging
  #   network_mode: "service:gluetun"
  #   entrypoint: sh /aria2-init.sh
  #   environment:
  #     <<: *common-env
  #     RPC_PORT: 6800
  #     LISTEN_PORT: 6888
  #     DISK_CACHE: 64M
  #     IPV6_MODE: "false"
  #   volumes:
  #     - aria2-config:/config
  #     - /mnt/storage/downloads/aria2:/downloads
  #     - /mnt/cachehdd/downloads/aria2:/incomplete
  #     - ./scripts/init/aria2-init.sh:/aria2-init.sh:ro
  #     - shared-keys:/keys:ro
  #   depends_on:
  #     gluetun:
  #       condition: service_healthy
  #       restart: true
  #     storage-init:
  #       condition: service_completed_successfully
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD-SHELL", "pgrep -x aria2c || exit 1"]
  #     interval: 60s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 30s
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.25"
  #         memory: 128M
  #       reservations:
  #         cpus: "0.1"
  #         memory: 128M
  #   labels:
  #     - "autoheal=true"

  # AriaNg - REMOVED
  # ariang:
  #   image: p3terx/ariang:${ARIANG_TAG:-latest}
  #   container_name: ariang
  #   logging: *default-logging
  #   network_mode: "service:gluetun"
  #   command: --port 80
  #   depends_on:
  #     - aria2
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.1"
  #         memory: 64M

  # slskd - Soulseek P2P client for music sharing (SOTA 2025)
  slskd:
    image: ghcr.io/slskd/slskd:${SLSKD_TAG:-latest}
    container_name: slskd
    logging: *default-logging
    network_mode: "service:gluetun"
    entrypoint: sh /init.sh
    environment:
      <<: *common-env
      SLSKD_HTTP_PORT: 2234
      SLSKD_SLSK_LISTEN_PORT: 50000
      SLSKD_USERNAME: ${SLSKD_USER:-admin}
      SLSKD_PASSWORD: ${SLSKD_PASSWORD}
      SLSKD_SOULSEEK_USERNAME: ${SLSKD_SOULSEEK_USERNAME}
      SLSKD_SOULSEEK_PASSWORD: ${SLSKD_SOULSEEK_PASSWORD}
      SLSKD_METRICS: ${SLSKD_METRICS_ENABLED:-true}
      SLSKD_API_KEY: ${SLSKD_API_KEY}
      SLSKD_SHARED_DIR: /var/slskd/shared
      SLSKD_UPLOAD_SLOTS: ${SLSKD_UPLOAD_SLOTS:-4}
      SLSKD_UPLOAD_SPEED_LIMIT: ${SLSKD_UPLOAD_SPEED_LIMIT:-25}
      SLSKD_DOWNLOAD_SLOTS: ${SLSKD_DOWNLOAD_SLOTS:-500}
      SLSKD_DOWNLOAD_SPEED_LIMIT: ${SLSKD_DOWNLOAD_SPEED_LIMIT:-1000}
      SLSKD_QUEUE_FILES: ${SLSKD_QUEUE_FILES:-500}
      SLSKD_QUEUE_MEGABYTES: ${SLSKD_QUEUE_MEGABYTES:-5000}
      SLSKD_GROUP_UPLOAD_SLOTS: ${SLSKD_GROUP_UPLOAD_SLOTS:-4}
      SLSKD_GROUP_UPLOAD_SPEED_LIMIT: ${SLSKD_GROUP_UPLOAD_SPEED_LIMIT:-25}
      SLSKD_GROUP_QUEUE_FILES: ${SLSKD_GROUP_QUEUE_FILES:-150}
      SLSKD_GROUP_QUEUE_MEGABYTES: ${SLSKD_GROUP_QUEUE_MEGABYTES:-1500}
      SLSKD_LOGGER_DISK: ${SLSKD_LOGGER_DISK:-true}
      SLSKD_LOGGER_NO_COLOR: ${SLSKD_LOGGER_NO_COLOR:-true}
      SLSKD_LOGGER_LOKI: ${SLSKD_LOGGER_LOKI:-null}
      SLSKD_METRICS_ENABLED: ${SLSKD_METRICS_ENABLED:-true}
      SLSKD_METRICS_URL: ${SLSKD_METRICS_URL:-/metrics}
      SLSKD_METRICS_AUTH_DISABLED: ${SLSKD_METRICS_AUTH_DISABLED:-true}
      SLSKD_METRICS_USERNAME: ${SLSKD_METRICS_USERNAME:-slskd}
      SLSKD_METRICS_PASSWORD: ${SLSKD_METRICS_PASSWORD:-slskd}
    volumes:
      - slskd-config:/app
      - /mnt/cachehdd/slskd/logs:/app/logs
      - /mnt/storage/slskd-shared:/var/slskd/shared
      - /mnt/cachehdd/downloads/slskd:/var/slskd/incomplete
      - shared-keys:/keys:ro
      - ./scripts/init/slskd-init.sh:/init.sh:ro
      # Share music and audiobook libraries
      - /mnt/storage/media/music:/music:ro
      - /mnt/storage/media/audiobooks:/audiobooks:ro
    tmpfs:
      - /tmp:exec,mode=1777,size=256m
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://127.0.0.1:2234/ || exit 1",
        ]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: "0.75"
          memory: 256M
        reservations:
          cpus: "0.25"
          memory: 128M
    labels:
      - "autoheal=true"
      - "potatostack.alerts=critical"

  # pyLoad-ng - Web-based download manager (HTTP/FTP/etc)
  # WebUI: 8076 (external) -> 8000 (internal), Click'n'Load: 9666
  pyload:
    image: lscr.io/linuxserver/pyload-ng:latest
    container_name: pyload
    logging: *default-logging
    network_mode: "service:gluetun"
    entrypoint: /pyload-init.sh
    environment:
      <<: *common-env
      PYLOAD_USER: ${PYLOAD_USER:-pyload}
      PYLOAD_PASSWORD: ${PYLOAD_PASSWORD:-}
      PYLOAD_ENABLE_NTFY_HOOKS: ${PYLOAD_ENABLE_NTFY_HOOKS:-true}
      NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
      NTFY_TOKEN: ${NTFY_TOKEN:-}
    volumes:
      - pyload-config:/config
      - /mnt/storage/downloads/pyload:/downloads
      - /mnt/cachehdd/downloads/pyload:/incomplete
      - ./scripts/init/pyload-init.sh:/pyload-init.sh:ro
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:8000/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.1"
          memory: 128M
    labels:
      - "autoheal=true"

  ################################################################################
  # PHOTO MANAGEMENT
  ################################################################################

  # Immich - Photo management and AI tagging
  immich-server:
    image: ghcr.io/immich-app/immich-server:${IMMICH_TAG:-release}
    container_name: immich-server
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.158}:2283:2283"
    environment:
      <<: *common-env
      DB_HOSTNAME: postgres
      DB_USERNAME: postgres
      DB_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
      DB_DATABASE_NAME: immich
      REDIS_HOSTNAME: redis-cache
      REDIS_DBINDEX: 0
      UPLOAD_LOCATION: /usr/src/app/upload
    volumes:
      - /mnt/storage/photos:/usr/src/app/upload
      - /etc/localtime:/etc/localtime:ro
    networks:
      - potatostack
    depends_on:
      - postgres
      - redis-cache
      - storage-init
    restart: unless-stopped
    healthcheck:
      test:
        ["CMD-SHELL", "curl -f http://127.0.0.1:2283/api/server/ping || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 2G
        reservations:
          cpus: "0.5"
          memory: 512M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.immich.rule=Host(`immich.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.immich.entrypoints=websecure"
      - "traefik.http.routers.immich.tls=true"
      - "traefik.http.services.immich.loadbalancer.server.port=2283"
      # - "traefik.http.routers.immich.middlewares=sso-chain@docker"  # Disabled for native app auth

  immich-machine-learning:
    image: ghcr.io/immich-app/immich-machine-learning:${IMMICH_TAG:-release}
    container_name: immich-ml
    logging: *default-logging
    environment:
      <<: *common-env
    volumes:
      - /mnt/cachehdd/media/immich-ml:/cache
    networks:
      - potatostack
    depends_on:
      immich-server:
        condition: service_healthy
    tmpfs:
      - /tmp:exec,mode=1777,size=500m
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'python3 -c ''import requests; requests.get("http://127.0.0.1:3003/ping")''',
        ]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 120s
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 1G
        reservations:
          cpus: "0.5"
          memory: 512M

  # Immich Log Monitor - Restart on error patterns
  immich-log-monitor:
    image: docker:${DOCKER_TAG:-cli}
    container_name: immich-log-monitor
    logging: *default-logging
    entrypoint: sh /immich-log-monitor.sh
    environment:
      IMMICH_CONTAINER: immich-server
      IMMICH_ML_CONTAINER: immich-ml
      CHECK_INTERVAL: ${IMMICH_LOG_CHECK_INTERVAL:-60}
      RESTART_COOLDOWN: ${IMMICH_RESTART_COOLDOWN:-300}
      REACHABILITY_TIMEOUT: ${IMMICH_REACHABILITY_TIMEOUT:-120}
      REACHABILITY_RETRIES: ${IMMICH_REACHABILITY_RETRIES:-6}
      IMMICH_LOG_PATTERNS: ${IMMICH_LOG_PATTERNS:-redis|Redis|ECONNREFUSED|Connection refused|connect ECONNREFUSED|socket hang up}
      IMMICH_NOTIFY_COOLDOWN: ${IMMICH_NOTIFY_COOLDOWN:-300}
      NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
      NTFY_TOKEN: ${NTFY_TOKEN:-}
      NTFY_DEFAULT_TAGS: ${NTFY_DEFAULT_TAGS:-potatostack,monitor}
      NTFY_DEFAULT_PRIORITY: ${NTFY_DEFAULT_PRIORITY:-default}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./scripts/monitor/immich-log-monitor.sh:/immich-log-monitor.sh:ro
      - ./scripts/monitor/notify.sh:/notify.sh:ro
    depends_on:
      - immich-server
    networks:
      - potatostack
    restart: unless-stopped

  ################################################################################
  # MONITORING & OBSERVABILITY
  ################################################################################

  # Prometheus - Metrics collection and time-series database (SOTA 2025)
  prometheus:
    image: prom/prometheus:${PROMETHEUS_TAG:-latest}
    container_name: prometheus
    logging: *default-logging
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=7d"
      - "--storage.tsdb.retention.size=5GB"
      - "--storage.tsdb.min-block-duration=2h"
      - "--storage.tsdb.max-block-duration=2h"
      - "--web.console.libraries=/usr/share/prometheus/console_libraries"
      - "--web.console.templates=/usr/share/prometheus/consoles"
      - "--web.enable-lifecycle"
      - "--web.enable-admin-api"
    ports:
      - "${HOST_BIND:-192.168.178.158}:9090:9090"
    environment:
      <<: *common-env
    volumes:
      - ./config/prometheus:/etc/prometheus:ro
      - /mnt/cachehdd/observability/prometheus:/prometheus
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://127.0.0.1:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.25"
          memory: 128M
    labels:
      - "potatostack.alerts=critical"
      - "traefik.enable=true"
      - "traefik.http.routers.prometheus.rule=Host(`prometheus.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.prometheus.entrypoints=websecure"
      - "traefik.http.routers.prometheus.tls=true"
      - "traefik.http.routers.prometheus.middlewares=sso-chain@docker"
      - "traefik.http.services.prometheus.loadbalancer.server.port=9090"

  # Thanos Sidecar - Long-term metrics storage (SOTA 2025)
  thanos-sidecar:
    image: quay.io/thanos/thanos:${THANOS_TAG:-v0.37.2}
    container_name: thanos-sidecar
    logging: *default-logging
    command:
      - "sidecar"
      - "--tsdb.path=/prometheus"
      - "--prometheus.url=http://prometheus:9090"
      - "--grpc-address=0.0.0.0:10901"
      - "--http-address=0.0.0.0:10902"
      - "--objstore.config-file=/etc/thanos/bucket.yml"
    ports:
      - "${HOST_BIND:-192.168.178.158}:10902:10902"
    environment:
      <<: *common-env
    volumes:
      - /mnt/cachehdd/observability/prometheus:/prometheus
      - ./config/thanos:/etc/thanos:ro
    networks:
      - potatostack
    depends_on:
      - prometheus
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 128M

  # Thanos Store - Long-term storage gateway
  thanos-store:
    image: quay.io/thanos/thanos:${THANOS_TAG:-v0.37.2}
    container_name: thanos-store
    logging: *default-logging
    command:
      - "store"
      - "--data-dir=/var/thanos/store"
      - "--grpc-address=0.0.0.0:10901"
      - "--http-address=0.0.0.0:10902"
      - "--objstore.config-file=/etc/thanos/bucket.yml"
    volumes:
      - thanos-store-data:/var/thanos/store
      - ./config/thanos:/etc/thanos:ro
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 128M

  # Thanos Query - Unified query interface
  thanos-query:
    image: quay.io/thanos/thanos:${THANOS_TAG:-v0.37.2}
    container_name: thanos-query
    logging: *default-logging
    command:
      - "query"
      - "--grpc-address=0.0.0.0:10901"
      - "--http-address=0.0.0.0:10902"
      - "--store=thanos-sidecar:10901"
      - "--store=thanos-store:10901"
      - "--query.replica-label=replica"
    ports:
      - "${HOST_BIND:-192.168.178.158}:10903:10902"
    networks:
      - potatostack
    depends_on:
      - thanos-sidecar
      - thanos-store
    restart: unless-stopped
    healthcheck:
      test:
        ["CMD", "wget", "--spider", "-q", "http://127.0.0.1:10902/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.thanos.rule=Host(`thanos.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.thanos.entrypoints=websecure"
      - "traefik.http.routers.thanos.tls=true"
      - "traefik.http.routers.thanos.middlewares=sso-chain@docker"
      - "traefik.http.services.thanos.loadbalancer.server.port=10902"

  # Thanos Compactor - Metrics compaction and downsampling
  thanos-compactor:
    image: quay.io/thanos/thanos:${THANOS_TAG:-v0.37.2}
    container_name: thanos-compactor
    logging: *default-logging
    command:
      - "compact"
      - "--data-dir=/var/thanos/compact"
      - "--objstore.config-file=/etc/thanos/bucket.yml"
      - "--http-address=0.0.0.0:10902"
      - "--wait"
      - "--retention.resolution-raw=30d"
      - "--retention.resolution-5m=90d"
      - "--retention.resolution-1h=365d"
    volumes:
      - thanos-compact-data:/var/thanos/compact
      - ./config/thanos:/etc/thanos:ro
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 128M

  # Grafana - Metrics visualization and dashboards (SOTA 2025)
  grafana:
    image: grafana/grafana:${GRAFANA_TAG:-latest}
    container_name: grafana
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.158}:3002:3000"
    environment:
      <<: *common-env
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_INSTALL_PLUGINS: grafana-clock-panel,grafana-simple-json-datasource,grafana-piechart-panel
      GF_SERVER_ROOT_URL: https://grafana.${HOST_DOMAIN:-local.domain}
      GF_DATABASE_TYPE: postgres
      GF_DATABASE_HOST: postgres:5432
      GF_DATABASE_NAME: grafana
      GF_DATABASE_USER: postgres
      GF_DATABASE_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
      GF_DATABASE_SSL_MODE: disable
    volumes:
      - grafana-data:/var/lib/grafana
      - ./config/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./config/grafana/dashboards:/var/lib/grafana/dashboards:ro
    networks:
      - potatostack
    depends_on:
      - postgres
      - pgbouncer
      - prometheus
      - loki
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --spider -q http://127.0.0.1:3000/api/health || exit 1",
        ]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.25"
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.grafana.rule=Host(`grafana.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.grafana.entrypoints=websecure"
      - "traefik.http.routers.grafana.tls=true"
      - "traefik.http.routers.grafana.middlewares=sso-chain@docker"
      - "traefik.http.services.grafana.loadbalancer.server.port=3000"

  # cAdvisor - Container metrics for Prometheus and Grafana dashboards
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:${CADVISOR_TAG:-latest}
    container_name: cadvisor
    logging: *default-logging
    privileged: true
    devices:
      - /dev/kmsg
    ports:
      - "${HOST_BIND:-192.168.178.158}:8089:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    command:
      - "--housekeeping_interval=60s"
      - "--docker_only=true"
      - "--store_container_labels=false"
      - "--disable_metrics=tcp,udp,disk,diskIO,hugetlb,referenced_memory,cpu_topology,resctrl,percpu,sched,process"
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:8080/healthz || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.2"
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.cadvisor.rule=Host(`cadvisor.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.cadvisor.entrypoints=websecure"
      - "traefik.http.routers.cadvisor.tls=true"
      - "traefik.http.services.cadvisor.loadbalancer.server.port=8080"
      - "traefik.http.routers.cadvisor.middlewares=sso-chain@docker"

  # Node Exporter - Host metrics for Prometheus (SOTA 2025)
  node-exporter:
    image: prom/node-exporter:${NODE_EXPORTER_TAG:-latest}
    container_name: node-exporter
    logging: *default-logging
    command:
      - "--path.rootfs=/host"
      - "--path.procfs=/host/proc"
      - "--path.sysfs=/host/sys"
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|run|var/lib/docker/.+)($|/)"
    volumes:
      - /:/host:ro,rslave
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 64M

  # Postgres Exporter - PostgreSQL metrics
  postgres-exporter:
    image: quay.io/prometheuscommunity/postgres-exporter:${POSTGRES_EXPORTER_TAG:-latest}
    container_name: postgres-exporter
    logging: *default-logging
    environment:
      DATA_SOURCE_NAME: postgresql://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/postgres?sslmode=disable
    networks:
      - potatostack
    depends_on:
      - postgres
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M
        reservations:
          cpus: "0.05"
          memory: 32M

  # Redis Exporter - Redis metrics
  redis-exporter:
    image: oliver006/redis_exporter:${REDIS_EXPORTER_TAG:-latest}
    container_name: redis-exporter
    logging: *default-logging
    environment:
      REDIS_ADDR: redis-cache:6379
    networks:
      - potatostack
    depends_on:
      - redis-cache
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M
        reservations:
          cpus: "0.05"
          memory: 32M

  # MongoDB Exporter - MongoDB metrics
  mongodb-exporter:
    image: percona/mongodb_exporter:${MONGODB_EXPORTER_TAG:-0.43}
    container_name: mongodb-exporter
    logging: *default-logging
    environment:
      MONGODB_URI: mongodb://root:${MONGO_ROOT_PASSWORD}@mongo:27017/admin
    networks:
      - potatostack
    depends_on:
      - mongo
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M
        reservations:
          cpus: "0.05"
          memory: 32M

  # Smartctl Exporter - SMART metrics for disk health alerting
  smartctl-exporter:
    image: prometheuscommunity/smartctl-exporter:${SMARTCTL_EXPORTER_TAG:-latest}
    container_name: smartctl-exporter
    logging: *default-logging
    privileged: true
    command:
      - "--smartctl.path=/usr/sbin/smartctl"
    volumes:
      - /dev:/dev
      - /run/udev:/run/udev:ro
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M
        reservations:
          cpus: "0.05"
          memory: 32M

  # Netdata - DISABLED: Use Prometheus + Grafana instead for monitoring
  # netdata:
  #   image: netdata/netdata:${NETDATA_TAG:-latest}
  #   container_name: netdata
  #   logging: *default-logging
  #   hostname: potatostack-main
  #   ports:
  #     - "${HOST_BIND:-192.168.178.158}:19999:19999"
  #   cap_add:
  #     - SYS_PTRACE
  #     - SYS_ADMIN
  #   security_opt:
  #     - apparmor:unconfined
  #   environment:
  #     <<: *common-env
  #     NETDATA_CLAIM_TOKEN: ${NETDATA_CLAIM_TOKEN:-}
  #     NETDATA_CLAIM_ROOMS: ${NETDATA_CLAIM_ROOMS:-}
  #     NETDATA_CLAIM_URL: https://app.netdata.cloud
  #     DOCKER_HOST: /var/run/docker.sock
  #   volumes:
  #     - netdata-config:/etc/netdata
  #     - netdata-lib:/var/lib/netdata
  #     - netdata-cache:/var/cache/netdata
  #     - /etc/passwd:/host/etc/passwd:ro
  #     - /etc/group:/host/etc/group:ro
  #     - /proc:/host/proc:ro
  #     - /sys:/host/sys:ro
  #     - /etc/os-release:/host/etc/os-release:ro
  #     - /var/run/docker.sock:/var/run/docker.sock:ro
  #   networks:
  #     - potatostack
  #   restart: unless-stopped
  #   healthcheck:
  #     test:
  #       ["CMD-SHELL", "curl -f http://127.0.0.1:19999/api/v1/info || exit 1"]
  #     interval: 60s
  #     timeout: 10s
  #     retries: 3
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.5"
  #         memory: 384M
  #       reservations:
  #         cpus: "0.1"
  #         memory: 64M
  #   labels:
  #     - "traefik.enable=true"
  #     - "traefik.http.routers.netdata.rule=Host(`netdata.${HOST_DOMAIN:-local.domain}`)"
  #     - "traefik.http.routers.netdata.entrypoints=websecure"
  #     - "traefik.http.routers.netdata.tls=true"
  #     - "traefik.http.routers.netdata.middlewares=sso-chain@docker"
  #     - "traefik.http.services.netdata.loadbalancer.server.port=19999"

  # Loki - Log aggregation
  loki:
    image: grafana/loki:${LOKI_TAG:-latest}
    container_name: loki
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.158}:3100:3100"
    command: -config.file=/etc/loki/loki.yml -target=all
    volumes:
      - ./config/loki:/etc/loki
      - /mnt/cachehdd/observability/loki/data:/loki
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 384M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.loki.rule=Host(`loki.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.loki.entrypoints=websecure"
      - "traefik.http.routers.loki.tls=true"
      - "traefik.http.services.loki.loadbalancer.server.port=3100"
      - "traefik.http.routers.loki.middlewares=sso-chain@docker"

  # Grafana Alloy - Unified collector (Promtail replacement)
  alloy:
    image: grafana/alloy:${ALLOY_TAG:-latest}
    container_name: alloy
    logging: *default-logging
    command: run /etc/alloy/config.alloy --storage.path=/var/lib/alloy
    volumes:
      - ./config/alloy:/etc/alloy
      - /var/log:/var/log:ro
      - traefik-logs:/logs:ro
      - /mnt/cachehdd/slskd/logs:/slskd-logs:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - alloy-data:/var/lib/alloy
    networks:
      - potatostack
    depends_on:
      - loki
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 192M

  # Parseable - REMOVED (using Loki instead)
  # parseable:
  #   image: parseable/parseable:${PARSEABLE_TAG:-latest}
  #   container_name: parseable
  #   logging: *default-logging
  #   command: ["parseable", "local-store"]
  #   ports:
  #     - "${HOST_BIND:-192.168.178.158}:8094:8000"
  #   environment:
  #     <<: *common-env
  #     P_USERNAME: ${PARSEABLE_USERNAME:-admin}
  #     P_PASSWORD: ${PARSEABLE_PASSWORD:-admin}
  #     P_ADDR: ${PARSEABLE_ADDR:-0.0.0.0:8000}
  #     P_FS_DIR: ${PARSEABLE_FS_DIR:-/data}
  #   volumes:
  #     - /mnt/ssd/docker-data/parseable:/data
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - storage-init
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.5"
  #         memory: 256M
  #   labels:
  #     - "traefik.enable=true"
  #     - "traefik.http.routers.parseable.rule=Host(`parseable.${HOST_DOMAIN:-local.domain}`)"
  #     - "traefik.http.routers.parseable.entrypoints=websecure"
  #     - "traefik.http.routers.parseable.tls=true"
  #     - "traefik.http.services.parseable.loadbalancer.server.port=8000"
  #     - "traefik.http.routers.parseable.middlewares=sso-chain@docker"

  # Scrutiny - HDD SMART monitoring
  scrutiny:
    image: ghcr.io/analogj/scrutiny:${SCRUTINY_TAG:-master-omnibus}
    container_name: scrutiny
    logging: *default-logging
    cap_add:
      - SYS_RAWIO
      - SYS_ADMIN
    ports:
      - "${HOST_BIND:-192.168.178.158}:8087:8080"
    volumes:
      - /run/udev:/run/udev:ro
      - /mnt/ssd/docker-data/scrutiny/config:/opt/scrutiny/config
      - /mnt/ssd/docker-data/scrutiny/influxdb:/opt/scrutiny/influxdb
    devices:
      - "${SCRUTINY_DEVICE_1:-/dev/sda}"
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 192M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.scrutiny.rule=Host(`scrutiny.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.scrutiny.entrypoints=websecure"
      - "traefik.http.routers.scrutiny.tls=true"
      - "traefik.http.routers.scrutiny.middlewares=sso-chain@docker"
      - "traefik.http.services.scrutiny.loadbalancer.server.port=8080"

  # Uptime Kuma - Uptime monitoring with Docker container support
  uptime-kuma:
    image: louislam/uptime-kuma:${UPTIME_KUMA_TAG:-latest}
    container_name: uptime-kuma
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.158}:3001:3001"
    environment:
      <<: *common-env
      # Docker socket access via socket-proxy for container monitoring
      # NOTE: When creating Docker monitors in UI, set Docker Host to: tcp://socket-proxy:2375
      DOCKER_HOST: tcp://socket-proxy:2375
    volumes:
      - uptime-kuma-data:/app/data
      # Mount docker.sock for Docker monitors (socket-proxy env var may not work for all features)
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - potatostack
    depends_on:
      - socket-proxy
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:3001 || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.uptime.rule=Host(`uptime.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.uptime.entrypoints=websecure"
      - "traefik.http.routers.uptime.tls=true"
      - "traefik.http.routers.uptime.middlewares=sso-chain@docker"
      - "traefik.http.services.uptime.loadbalancer.server.port=3001"
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 192M

  ################################################################################
  # AUTOMATION & WORKFLOWS
  ################################################################################

  # n8n - Workflow automation
  # n8n:
  #   image: n8nio/n8n:${N8N_TAG:-latest}
  #   container_name: n8n
  #   logging: *default-logging
  #   ports:
  #     - "${HOST_BIND:-192.168.178.158}:5678:5678"
  #   environment:
  #     <<: *common-env
  #     N8N_BASIC_AUTH_ACTIVE: "true"
  #     N8N_BASIC_AUTH_USER: ${N8N_USER:-admin}
  #     N8N_BASIC_AUTH_PASSWORD: ${N8N_PASSWORD}
  #     N8N_HOST: n8n.${HOST_DOMAIN:-local.domain}
  #     N8N_PORT: 5678
  #     N8N_PROTOCOL: https
  #     WEBHOOK_URL: https://n8n.${HOST_DOMAIN:-local.domain}/
  #     GENERIC_TIMEZONE: Europe/Berlin
  #     # Allow access without HTTPS (for direct port access via Tailscale)
  #     N8N_SECURE_COOKIE: false
  #     DB_TYPE: postgresdb
  #     DB_POSTGRESDB_HOST: postgres
  #     DB_POSTGRESDB_PORT: 5432
  #     DB_POSTGRESDB_DATABASE: n8n
  #     DB_POSTGRESDB_USER: postgres
  #     DB_POSTGRESDB_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
  #     EXECUTIONS_MODE: queue
  #     QUEUE_BULL_REDIS_HOST: redis-cache
  #     QUEUE_BULL_REDIS_PORT: 6379
  #   volumes:
  #     - /mnt/ssd/docker-data/n8n:/home/node/.n8n
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - postgres
  #     - redis-cache
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "1.0"
  #         memory: 384M
  #   labels:
  #     - "traefik.enable=true"
  #     - "traefik.http.routers.n8n.rule=Host(`n8n.${HOST_DOMAIN:-local.domain}`)"
  #     - "traefik.http.routers.n8n.entrypoints=websecure"
  #     - "traefik.http.routers.n8n.tls=true"
  #     - "traefik.http.services.n8n.loadbalancer.server.port=5678"
  #     - "traefik.http.routers.n8n.middlewares=sso-chain@docker"

  # Healthchecks - Cron monitoring
  healthchecks:
    image: lscr.io/linuxserver/healthchecks:${HEALTHCHECKS_TAG:-latest}
    container_name: healthchecks
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.158}:8001:8000"
    environment:
      <<: *common-env
      SITE_ROOT: https://healthchecks.${HOST_DOMAIN:-local.domain}
      SITE_NAME: PotatoStack Healthchecks
      SUPERUSER_EMAIL: ${HEALTHCHECKS_ADMIN_EMAIL}
      SUPERUSER_PASSWORD: ${HEALTHCHECKS_ADMIN_PASSWORD}
      SECRET_KEY: ${HEALTHCHECKS_SECRET_KEY}
      DB: postgres
      DB_HOST: postgres
      DB_PORT: 5432
      DB_NAME: healthchecks
      DB_USER: postgres
      DB_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
    volumes:
      - healthchecks-data:/config
    networks:
      - potatostack
    depends_on:
      - postgres
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 192M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.healthchecks.rule=Host(`healthchecks.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.healthchecks.entrypoints=websecure"
      - "traefik.http.routers.healthchecks.tls=true"
      - "traefik.http.services.healthchecks.loadbalancer.server.port=8000"
      - "traefik.http.routers.healthchecks.middlewares=sso-chain@docker"

  ################################################################################
  # UTILITIES & TOOLS
  ################################################################################

  # Rustypaste - Pastebin
  rustypaste:
    image: orhunp/rustypaste:${RUSTYPASTE_TAG:-latest}
    container_name: rustypaste
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.158}:8788:8000"
    environment:
      <<: *common-env
      CONFIG: /config/config.toml
    volumes:
      - /mnt/storage/rustypaste:/data
      - ./config/rustypaste/config.toml:/config/config.toml:ro
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    healthcheck:
      disable: true
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 64M
    labels:
      - "autoheal=true"
      - "traefik.enable=true"
      - "traefik.http.routers.rustypaste.rule=PathPrefix(`/paste`)"
      - "traefik.http.routers.rustypaste.entrypoints=websecure"
      - "traefik.http.routers.rustypaste.tls=true"
      - "traefik.http.routers.rustypaste.middlewares=sso-chain@docker"
      - "traefik.http.services.rustypaste.loadbalancer.server.port=8000"

  # Stirling-PDF - PDF tools
  # stirling-pdf:
  #   image: frooodle/s-pdf:${STIRLING_PDF_TAG:-latest}
  #   container_name: stirling-pdf
  #   logging: *default-logging
  #   ports:
  #     - "${HOST_BIND:-192.168.178.158}:8086:8080"
  #   environment:
  #     <<: *common-env
  #     DOCKER_ENABLE_SECURITY: "false"
  #     INSTALL_BOOK_AND_ADVANCED_HTML_OPS: "true"
  #     LANGS: de_DE,en_US
  #   volumes:
  #     - stirling-pdf-data:/usr/share/tessdata
  #     - stirling-pdf-configs:/configs
  #   networks:
  #     - potatostack
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.75"
  #         memory: 512M
  #       reservations:
  #         cpus: "0.25"
  #         memory: 256M
  #   labels:
  #     - "traefik.enable=true"
  #     - "traefik.http.routers.stirling.rule=Host(`stirling.${HOST_DOMAIN:-local.domain}`)"
  #     - "traefik.http.routers.stirling.entrypoints=websecure"
  #     - "traefik.http.routers.stirling.tls=true"
  #     - "traefik.http.services.stirling.loadbalancer.server.port=8080"
  #     - "traefik.http.routers.stirling.middlewares=sso-chain@docker"

  # Linkding - Bookmark manager
  linkding:
    image: sissbruecker/linkding:${LINKDING_TAG:-latest}
    container_name: linkding
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.158}:9091:9090"
    environment:
      <<: *common-env
      LD_DB_ENGINE: postgres
      LD_DB_HOST: postgres
      LD_DB_PORT: 5432
      LD_DB_DATABASE: linkding
      LD_DB_USER: postgres
      LD_DB_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
      LD_SUPERUSER_NAME: ${LINKDING_ADMIN_USER:-admin}
      LD_SUPERUSER_PASSWORD: ${LINKDING_ADMIN_PASSWORD}
    volumes:
      - linkding-data:/etc/linkding/data
    networks:
      - potatostack
    depends_on:
      - postgres
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 192M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.linkding.rule=Host(`linkding.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.linkding.entrypoints=websecure"
      - "traefik.http.routers.linkding.tls=true"
      - "traefik.http.services.linkding.loadbalancer.server.port=9090"
      - "traefik.http.routers.linkding.middlewares=sso-chain@docker"

  # Code-server - DISABLED: VS Code in browser
  # code-server:
  #   image: lscr.io/linuxserver/code-server:${CODE_SERVER_TAG:-latest}
  #   container_name: code-server
  #   logging: *default-logging
  #   ports:
  #     - "${HOST_BIND:-192.168.178.158}:8444:8443"
  #   environment:
  #     <<: *common-env
  #     PASSWORD: ${CODE_SERVER_PASSWORD}
  #     SUDO_PASSWORD: ${CODE_SERVER_SUDO_PASSWORD}
  #   volumes:
  #     - code-server-config:/config
  #     - /mnt/storage/projects:/config/workspace
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - storage-init
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "2.0"
  #         memory: 384M
  #   labels:
  #     - "traefik.enable=true"
  #     - "traefik.http.routers.code-server.rule=Host(`code.${HOST_DOMAIN:-local.domain}`)"
  #     - "traefik.http.routers.code-server.entrypoints=websecure"
  #     - "traefik.http.routers.code-server.tls=true"
  #     - "traefik.http.services.code-server.loadbalancer.server.port=8443"
  #     - "traefik.http.services.code-server.loadbalancer.server.scheme=https"
  #     - "traefik.http.routers.code-server.middlewares=sso-chain@docker"

  # Atuin - Shell history sync
  atuin:
    image: ghcr.io/atuinsh/atuin:${ATUIN_TAG:-latest}
    container_name: atuin
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.158}:8889:8888"
    environment:
      <<: *common-env
      ATUIN_DB_URI: postgres://postgres:${POSTGRES_SUPER_PASSWORD}@postgres/atuin
      # Bind to all interfaces for external access (Tailscale/LAN)
      ATUIN_HOST: "0.0.0.0"
      ATUIN_PORT: "8888"
      ATUIN_OPEN_REGISTRATION: ${ATUIN_OPEN_REGISTRATION:-false}
    command: server start
    volumes:
      - atuin-config:/config
    networks:
      - potatostack
    depends_on:
      - postgres
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.atuin.rule=Host(`atuin.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.atuin.entrypoints=websecure"
      - "traefik.http.routers.atuin.tls=true"
      - "traefik.http.services.atuin.loadbalancer.server.port=8888"
      - "traefik.http.routers.atuin.middlewares=sso-chain@docker"

  # IT-Tools - REMOVED: User requested removal
  # it-tools:
  #   image: corentinth/it-tools:${IT_TOOLS_TAG:-latest}
  #   container_name: it-tools
  #   logging: *default-logging
  #   ports:
  #     - "${HOST_BIND:-192.168.178.158}:8091:80"
  #   environment:
  #     <<: *common-env
  #   networks:
  #     - potatostack
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:80/ || exit 1"]
  #     interval: 60s
  #     timeout: 10s
  #     retries: 3
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.25"
  #         memory: 128M
  #   labels:
  #     - "traefik.enable=true"
  #     - "traefik.http.routers.ittools.rule=Host(`tools.${HOST_DOMAIN:-local.domain}`)"
  #     - "traefik.http.routers.ittools.entrypoints=websecure"
  #     - "traefik.http.routers.ittools.tls=true"
  #     - "traefik.http.routers.ittools.middlewares=sso-chain@docker"
  #     - "traefik.http.services.ittools.loadbalancer.server.port=80"

  # DuckDB - Ad-hoc analytics container
  duckdb:
    image: alpine:latest
    container_name: duckdb
    logging: *default-logging
    entrypoint: ["/bin/sh", "-c", "tail -f /dev/null"]
    volumes:
      - /mnt/storage/duckdb:/data
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M

  # Paperless-ngx - DISABLED (OOM issues with Celery workers)
  # Uncomment to re-enable document management
  # paperless-ngx:
  #   image: ghcr.io/paperless-ngx/paperless-ngx:${PAPERLESS_TAG:-latest}
  #   container_name: paperless-ngx
  #   logging: *default-logging
  #   ports:
  #     - "${HOST_BIND:-192.168.178.158}:8092:8000"
  #   environment:
  #     <<: *common-env
  #     PAPERLESS_REDIS: redis://redis-cache:6379/1
  #     PAPERLESS_DBHOST: postgres
  #     PAPERLESS_DBNAME: paperless
  #     PAPERLESS_DBUSER: postgres
  #     PAPERLESS_DBPASS: ${POSTGRES_SUPER_PASSWORD}
  #     PAPERLESS_URL: https://docs.${HOST_DOMAIN:-local.domain}
  #     PAPERLESS_SECRET_KEY: ${PAPERLESS_SECRET_KEY}
  #     PAPERLESS_ADMIN_USER: ${PAPERLESS_ADMIN_USER:-admin}
  #     PAPERLESS_ADMIN_PASSWORD: ${PAPERLESS_ADMIN_PASSWORD}
  #     PAPERLESS_OCR_LANGUAGE: deu+eng
  #     PAPERLESS_TIME_ZONE: Europe/Berlin
  #   volumes:
  #     - /mnt/ssd/docker-data/paperless-data:/usr/src/paperless/data
  #     - /mnt/storage/paperless/media:/usr/src/paperless/media
  #     - /mnt/storage/paperless/consume:/usr/src/paperless/consume
  #     - /mnt/storage/paperless/export:/usr/src/paperless/export
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - postgres
  #     - redis-cache
  #     - storage-init
  #   tmpfs:
  #     - /tmp:exec,mode=1777,size=1g
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -f http://127.0.0.1:8000/ || exit 1"]
  #     interval: 60s
  #     timeout: 10s
  #     retries: 3
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "2.0"
  #         memory: 2G
  #       reservations:
  #         cpus: "0.5"
  #         memory: 768M
  #   labels:
  #     - "traefik.enable=true"
  #     - "traefik.http.routers.paperless.rule=Host(`docs.${HOST_DOMAIN:-local.domain}`)"
  #     - "traefik.http.routers.paperless.entrypoints=websecure"
  #     - "traefik.http.routers.paperless.tls=true"
  #     - "traefik.http.routers.paperless.middlewares=sso-chain@docker"
  #     - "traefik.http.services.paperless.loadbalancer.server.port=8000"

  # Pingvin Share - Secure file sharing (SOTA 2025)
  ################################################################################
  # DEVELOPMENT & GIT
  ################################################################################

  # Gitea - Git hosting
  gitea:
    image: gitea/gitea:${GITEA_TAG:-latest}
    container_name: gitea
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.158}:3004:3000"
      - "${HOST_BIND:-192.168.178.158}:${GITEA_SSH_PORT:-2223}:22"
    environment:
      <<: *common-env
      GITEA__database__DB_TYPE: postgres
      GITEA__database__HOST: postgres:5432
      GITEA__database__NAME: gitea
      GITEA__database__USER: postgres
      GITEA__database__PASSWD: ${POSTGRES_SUPER_PASSWORD}
      GITEA__server__DOMAIN: git.${HOST_DOMAIN:-local.domain}
      # For HTTPS via Traefik - access via https://git.danielhomelab.local
      GITEA__server__ROOT_URL: https://git.${HOST_DOMAIN:-local.domain}/
      GITEA__server__SSH_DOMAIN: git.${HOST_DOMAIN:-local.domain}
      GITEA__server__SSH_PORT: ${GITEA_SSH_PORT:-2223}
      # Allow HTTP access for local development (WebAuthn requires HTTPS for full functionality)
      GITEA__server__LOCAL_ROOT_URL: http://localhost:3000/
      GITEA__security__INSTALL_LOCK: "true"
      GITEA__cache__ENABLED: "true"
      GITEA__cache__ADAPTER: redis
      GITEA__cache__HOST: network=tcp,addr=redis-cache:6379,db=0
      GITEA__session__PROVIDER: redis
      GITEA__session__PROVIDER_CONFIG: network=tcp,addr=redis-cache:6379,db=2
      GITEA__queue__TYPE: redis
      GITEA__queue__CONN_STR: redis://redis-cache:6379/1
    volumes:
      - /mnt/ssd/docker-data/gitea:/data
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    networks:
      - potatostack
    dns:
      - 127.0.0.11
      - 1.1.1.1
    depends_on:
      postgres:
        condition: service_healthy
      redis-cache:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 384M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.gitea.rule=Host(`git.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.gitea.entrypoints=websecure"
      - "traefik.http.routers.gitea.tls=true"
      - "traefik.http.services.gitea.loadbalancer.server.port=3000"
      - "traefik.http.routers.gitea.middlewares=sso-chain@docker"

  # Gitea Runner - CI/CD for Gitea
  gitea-runner:
    image: gitea/act_runner:${GITEA_RUNNER_TAG:-latest}
    container_name: gitea-runner
    logging: *default-logging
    environment:
      <<: *common-env
      GITEA_INSTANCE_URL: http://gitea:3000
      GITEA_RUNNER_REGISTRATION_TOKEN: ${GITEA_RUNNER_TOKEN}
      GITEA_RUNNER_NAME: docker-runner
    volumes:
      - gitea-runner-data:/data
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - potatostack
    depends_on:
      - gitea
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M

  # Woodpecker CI - Gitea integrated CI/CD (SOTA 2025)
  woodpecker-server:
    image: woodpeckerci/woodpecker-server:${WOODPECKER_TAG:-latest}
    container_name: woodpecker-server
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.158}:3006:8000"
    environment:
      <<: *common-env
      WOODPECKER_HOST: https://ci.${HOST_DOMAIN:-local.domain}
      WOODPECKER_SERVER_ADDR: :8000
      WOODPECKER_GRPC_ADDR: :9000
      WOODPECKER_AGENT_SECRET: ${WOODPECKER_AGENT_SECRET}
      WOODPECKER_DATABASE_DRIVER: postgres
      WOODPECKER_DATABASE_DATASOURCE: postgres://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/woodpecker?sslmode=disable
      WOODPECKER_OPEN: "false"
      WOODPECKER_ADMIN: ${WOODPECKER_ADMIN}
      WOODPECKER_GITEA: "true"
      WOODPECKER_GITEA_URL: http://gitea:3000
      WOODPECKER_GITEA_CLIENT: ${WOODPECKER_GITEA_CLIENT}
      WOODPECKER_GITEA_SECRET: ${WOODPECKER_GITEA_SECRET}
    networks:
      - potatostack
    depends_on:
      - postgres
      - gitea
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.woodpecker.rule=Host(`ci.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.woodpecker.entrypoints=websecure"
      - "traefik.http.routers.woodpecker.tls=true"
      - "traefik.http.routers.woodpecker.middlewares=sso-chain@docker"
      - "traefik.http.services.woodpecker.loadbalancer.server.port=8000"

  woodpecker-agent:
    image: woodpeckerci/woodpecker-agent:${WOODPECKER_AGENT_TAG:-latest}
    container_name: woodpecker-agent
    logging: *default-logging
    environment:
      <<: *common-env
      WOODPECKER_SERVER: woodpecker-server:9000
      WOODPECKER_AGENT_SECRET: ${WOODPECKER_AGENT_SECRET}
      WOODPECKER_BACKEND: docker
      WOODPECKER_BACKEND_DOCKER_HOST: unix:///var/run/docker.sock
      DOCKER_API_VERSION: "1.44"
      WOODPECKER_BACKEND_DOCKER_NETWORK: potatostack
      WOODPECKER_MAX_WORKFLOWS: ${WOODPECKER_MAX_WORKFLOWS:-2}
      WOODPECKER_GRPC_SECURE: "false"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - potatostack
    depends_on:
      - woodpecker-server
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M

  # Sentry - Error tracking
  ################################################################################
  # AI & SPECIAL APPLICATIONS
  ################################################################################

  # Open WebUI - LLM interface
  # open-webui:
  #   image: ghcr.io/open-webui/open-webui:${OPEN_WEBUI_TAG:-main}
  #   container_name: open-webui
  #   logging: *default-logging
  #   ports:
  #     - "${HOST_BIND:-192.168.178.158}:3005:8080"
  #   environment:
  #     <<: *common-env
  #     OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
  #     WEBUI_SECRET_KEY: ${OPEN_WEBUI_SECRET_KEY}
  #     DATABASE_URL: postgresql://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/openwebui
  #     VECTOR_DB: pgvector
  #   volumes:
  #     - open-webui-data:/app/backend/data
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - postgres
  #   extra_hosts:
  #     - "host.docker.internal:host-gateway"
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "2.0"
  #         memory: 384M
  #   labels:
  #     - "traefik.enable=true"
  #     - "traefik.http.routers.open-webui.rule=Host(`openwebui.${HOST_DOMAIN:-local.domain}`)"
  #     - "traefik.http.routers.open-webui.entrypoints=websecure"
  #     - "traefik.http.routers.open-webui.tls=true"
  #     - "traefik.http.services.open-webui.loadbalancer.server.port=8080"
  #     - "traefik.http.routers.open-webui.middlewares=sso-chain@docker"

  # Pinchflat - YouTube downloader
  pinchflat:
    image: ghcr.io/kieraneglin/pinchflat:${PINCHFLAT_TAG:-latest}
    container_name: pinchflat
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
    volumes:
      - pinchflat-config:/config
      - /mnt/storage/media/youtube:/downloads
      - /mnt/cachehdd/downloads/pinchflat:/incomplete
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 384M
    labels:
      - "autoheal=true"

  # Stash - Adult media organizer (behind VPN)
  stash:
    image: stashapp/stash:${STASH_TAG:-latest}
    container_name: stash
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
      STASH_PORT: "9900"
      STASH_STASH: /data
      STASH_GENERATED: /generated
      STASH_METADATA: /metadata
      STASH_CACHE: /cache
    volumes:
      - stash-config:/root/.stash
      - stash-generated:/generated
      - stash-metadata:/metadata
      - stash-cache:/cache
      - /mnt/storage/media/adult:/data
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 1024M
        reservations:
          cpus: "0.5"
          memory: 256M
    labels:
      - "autoheal=true"

  ################################################################################
  # DASHBOARD & CONTAINER MANAGEMENT
  ################################################################################

  # Docker Socket Proxy - Restricted Docker API for Homarr (SOTA 2025)
  socket-proxy:
    image: lscr.io/linuxserver/socket-proxy:${SOCKET_PROXY_TAG:-latest}
    container_name: socket-proxy
    logging: *default-logging
    security_opt:
      - no-new-privileges:true
    environment:
      <<: *common-env
      ALLOW_START: 1
      ALLOW_STOP: 1
      ALLOW_RESTARTS: 1
      CONTAINERS: 1
      EVENTS: 1
      INFO: 1
      IMAGES: 1
      NETWORKS: 1
      PING: 1
      POST: 0
      VERSION: 1
      LOG_LEVEL: info
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - potatostack
    restart: unless-stopped
    read_only: true
    tmpfs:
      - /run
    healthcheck:
      test:
        ["CMD-SHELL", "wget --spider -q http://127.0.0.1:2375/_ping || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M

  # Homarr - Modern drag-and-drop dashboard with 30+ integrations (homarr-labs v1.x)
  homarr:
    image: ghcr.io/homarr-labs/homarr:${HOMARR_TAG:-latest}
    container_name: homarr
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.158}:7575:7575"
    environment:
      <<: *common-env
      # Encryption key (generate with: openssl rand -hex 32)
      SECRET_ENCRYPTION_KEY: ${HOMARR_SECRET_KEY:-f95cefcebd962e7b56b1cb4e1a166c756cf7b821c91e759cfb7fd2995be4517a}
      # Database - PostgreSQL for persistence
      DRIVER: nodePostgres
      DB_HOST: postgres
      DB_PORT: 5432
      DB_NAME: homarr
      DB_USER: postgres
      DB_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
      # Docker integration via socket-proxy (v1.x format)
      DOCKER_HOSTNAMES: socket-proxy
      DOCKER_PORTS: 2375
      # Use external Redis (shared redis-cache)
      REDIS_IS_EXTERNAL: "true"
      REDIS_HOST: redis-cache
      REDIS_PORT: 6379
      REDIS_DATABASE_INDEX: 7
    volumes:
      - /mnt/ssd/docker-data/homarr:/appdata
    networks:
      - potatostack
    depends_on:
      postgres:
        condition: service_healthy
      redis-cache:
        condition: service_healthy
      socket-proxy:
        condition: service_started
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:7575/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 768M
        reservations:
          cpus: "0.25"
          memory: 256M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.homarr.rule=Host(`home.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.homarr.entrypoints=websecure"
      - "traefik.http.routers.homarr.tls=true"
      - "traefik.http.routers.homarr.middlewares=sso-chain@docker"
      - "traefik.http.services.homarr.loadbalancer.server.port=7575"

  # Dockge - Modern Docker Compose stack manager (SOTA 2025)
  ################################################################################
  # ENTERPRISE SECURITY & SECRETS MANAGEMENT
  ################################################################################

  # Infisical - Secrets management
  # infisical:
  #   image: infisical/infisical:${INFISICAL_TAG:-latest}
  #   container_name: infisical
  #   logging: *default-logging
  #   user: "0:0"
  #   ports:
  #     - "${HOST_BIND:-192.168.178.158}:8288:8080"
  #   environment:
  #     <<: *common-env
  #     NODE_ENV: production
  #     SITE_URL: https://secrets.${HOST_DOMAIN:-local.domain}
  #     ENCRYPTION_KEY: ${INFISICAL_ENCRYPTION_KEY}
  #     AUTH_SECRET: ${INFISICAL_AUTH_SECRET}
  #     DB_CONNECTION_URI: postgres://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/infisical?sslmode=disable
  #     REDIS_URL: redis://redis-cache:6379
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - postgres
  #     - redis-cache
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.5"
  #         memory: 256M
  #       reservations:
  #         cpus: "0.25"
  #         memory: 128M
  #   labels:
  #     - "traefik.enable=true"
  #     - "traefik.http.routers.infisical.rule=Host(`secrets.${HOST_DOMAIN:-local.domain}`)"
  #     - "traefik.http.routers.infisical.entrypoints=websecure"
  #     - "traefik.http.routers.infisical.tls=true"
  #     - "traefik.http.services.infisical.loadbalancer.server.port=8080"
  #     - "traefik.http.routers.infisical.middlewares=security-headers@docker"

  # fail2ban - REMOVED (using CrowdSec instead)
  # fail2ban:
  #   image: crazymax/fail2ban:${FAIL2BAN_TAG:-latest}
  #   container_name: fail2ban
  #   logging: *default-logging
  #   network_mode: host
  #   cap_add:
  #     - NET_ADMIN
  #     - NET_RAW
  #   environment:
  #     <<: *common-env
  #     F2B_DB_PURGE_AGE: 30d
  #     F2B_LOG_LEVEL: INFO
  #     F2B_IPTABLES_CHAIN: DOCKER-USER
  #   volumes:
  #     - fail2ban-data:/data
  #     - /var/log:/var/log:ro
  #     - traefik-logs:/logs:ro
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.25"
  #         memory: 128M
  #       reservations:
  #         cpus: "0.1"
  #         memory: 64M

  # Trivy - Vulnerability scanner
  trivy:
    image: aquasec/trivy:${TRIVY_TAG:-latest}
    container_name: trivy
    logging: *default-logging
    command: ["server", "--listen", "0.0.0.0:8081"]
    ports:
      - "${HOST_BIND:-192.168.178.158}:8081:8081"
    environment:
      <<: *common-env
      TRIVY_LISTEN: "0.0.0.0:8081"
      TRIVY_CACHE_DIR: /root/.cache/trivy
    volumes:
      - trivy-cache:/root/.cache/
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 192M
        reservations:
          cpus: "0.25"
          memory: 128M

  # Alertmanager - Alert routing and management
  alertmanager:
    image: prom/alertmanager:${ALERTMANAGER_TAG:-latest}
    container_name: alertmanager
    logging: *default-logging
    command:
      - "--config.file=/etc/alertmanager/config.yml"
      - "--storage.path=/alertmanager"
      - "--web.external-url=https://alerts.${HOST_DOMAIN:-local.domain}"
      - "--cluster.listen-address="
    environment:
      <<: *common-env
    volumes:
      - ./config/alertmanager:/etc/alertmanager:ro
      - alertmanager-data:/alertmanager
    ports:
      - "${HOST_BIND:-192.168.178.158}:9093:9093"
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://127.0.0.1:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 64M
    labels:
      - "potatostack.alerts=critical"
      - "traefik.enable=true"
      - "traefik.http.routers.alertmanager.rule=Host(`alerts.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.alertmanager.entrypoints=websecure"
      - "traefik.http.routers.alertmanager.tls=true"
      - "traefik.http.routers.alertmanager.middlewares=sso-chain@docker"
      - "traefik.http.services.alertmanager.loadbalancer.server.port=9093"

  # Alertmanager ntfy Formatter - Enterprise alert formatting -> ntfy
  alertmanager-ntfy:
    image: python:3.12-alpine
    container_name: alertmanager-ntfy
    logging: *default-logging
    command: ["python", "/app/alertmanager-ntfy.py"]
    environment:
      NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
      NTFY_TOPIC_CRITICAL: ${NTFY_TOPIC_CRITICAL:-potatostack-critical}
      NTFY_TOPIC_WARNING: ${NTFY_TOPIC_WARNING:-potatostack-warning}
      NTFY_TOPIC_INFO: ${NTFY_TOPIC_INFO:-potatostack-info}
      NTFY_TOKEN: ${NTFY_TOKEN:-}
    volumes:
      - ./scripts/alertmanager/alertmanager-ntfy.py:/app/alertmanager-ntfy.py:ro
    networks:
      - potatostack
    depends_on:
      - ntfy
    restart: unless-stopped

  ################################################################################
  # SYSTEM UTILITIES
  ################################################################################

  # ntfy - Notification hub (push/webhook)
  ntfy:
    image: binwiederhier/ntfy:${NTFY_TAG:-latest}
    container_name: ntfy
    logging: *default-logging
    command: serve
    ports:
      - "${HOST_BIND:-192.168.178.158}:8060:80"
    environment:
      <<: *common-env
      NTFY_BASE_URL: https://ntfy.${HOST_DOMAIN:-local.domain}
      NTFY_BEHIND_PROXY: "true"
      NTFY_AUTH_DEFAULT_ACCESS: ${NTFY_AUTH_DEFAULT_ACCESS:-read-write}
      NTFY_ENABLE_LOGIN: ${NTFY_ENABLE_LOGIN:-false}
      NTFY_ENABLE_METRICS: ${NTFY_ENABLE_METRICS:-true}
      NTFY_CACHE_FILE: /var/cache/ntfy/cache.db
      NTFY_AUTH_FILE: /var/lib/ntfy/user.db
    volumes:
      - ntfy-cache:/var/cache/ntfy
      - ntfy-data:/var/lib/ntfy
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:80/v1/health || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.2"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 64M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.ntfy.rule=Host(`ntfy.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.ntfy.entrypoints=websecure"
      - "traefik.http.routers.ntfy.tls=true"
      - "traefik.http.services.ntfy.loadbalancer.server.port=80"
      - "traefik.http.routers.ntfy.middlewares=security-headers@docker"

  # Diun - Docker Image Update Notifier (SOTA 2025 best practice)
  # Monitors for updates but doesn't auto-update, giving full control
  diun:
    image: crazymax/diun:${DIUN_TAG:-latest}
    container_name: diun
    logging: *default-logging
    command: serve
    environment:
      <<: *common-env
      LOG_LEVEL: info
      LOG_JSON: "false"
      DIUN_WATCH_WORKERS: 20
      DIUN_WATCH_SCHEDULE: "0 */6 * * *"
      DIUN_WATCH_JITTER: 30s
      DIUN_PROVIDERS_DOCKER: "true"
      DIUN_PROVIDERS_DOCKER_WATCHBYDEFAULT: "true"
      DIUN_NOTIF_NTFY_ENDPOINT: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      DIUN_NOTIF_NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
      DIUN_NOTIF_NTFY_TOKEN: ${NTFY_TOKEN:-}
      # DIUN_NOTIF_GOTIFY_ENDPOINT: ${DIUN_GOTIFY_ENDPOINT:-}
      # DIUN_NOTIF_GOTIFY_TOKEN: ${DIUN_GOTIFY_TOKEN:-}
      DIUN_NOTIF_DISCORD_WEBHOOKURL: ${DIUN_DISCORD_WEBHOOK:-}
      DIUN_NOTIF_TELEGRAM_TOKEN: ${DIUN_TELEGRAM_TOKEN:-}
      DIUN_NOTIF_TELEGRAM_CHATIDS: ${DIUN_TELEGRAM_CHATIDS:-}
    volumes:
      - diun-data:/data
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M

  # Autoheal - Container health recovery
  autoheal:
    image: willfarrell/autoheal:${AUTOHEAL_TAG:-latest}
    container_name: autoheal
    logging: *default-logging
    environment:
      AUTOHEAL_CONTAINER_LABEL: autoheal
      AUTOHEAL_INTERVAL: 60
      AUTOHEAL_START_PERIOD: 300
      AUTOHEAL_DEFAULT_STOP_TIMEOUT: 10
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M
        reservations:
          cpus: "0.05"
          memory: 32M

  # OpenSSH Server - Secure remote shell and SFTP access
  openssh-server:
    image: lscr.io/linuxserver/openssh-server:${OPENSSH_SERVER_TAG:-latest}
    container_name: openssh-server
    logging: *default-logging
    hostname: openssh-server
    security_opt:
      - no-new-privileges:true
    ports:
      - "${HOST_BIND:-192.168.178.158}:${OPENSSH_PORT:-2222}:2222"
    environment:
      <<: *common-env
      USER_NAME: ${OPENSSH_USER:-sshuser}
      USER_PASSWORD: ${OPENSSH_PASSWORD}
      PASSWORD_ACCESS: ${OPENSSH_PASSWORD_ACCESS:-true}
      SUDO_ACCESS: ${OPENSSH_SUDO_ACCESS:-false}
      PUBLIC_KEY: ${OPENSSH_PUBLIC_KEY:-}
      PUBLIC_KEY_FILE: ${OPENSSH_PUBLIC_KEY_FILE:-}
      PUBLIC_KEY_DIR: ${OPENSSH_PUBLIC_KEY_DIR:-}
      PUBLIC_KEY_URL: ${OPENSSH_PUBLIC_KEY_URL:-}
      LOG_STDOUT: ${OPENSSH_LOG_STDOUT:-true}
    volumes:
      - openssh-config:/config
      - /mnt/storage:/data/storage
      - /mnt/cachehdd:/data/cachehdd
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 64M

  # Samba - Windows file sharing for HDD access
  samba:
    image: ghcr.io/servercontainers/samba:${SAMBA_TAG:-latest}
    container_name: samba
    logging: *default-logging
    network_mode: host
    cap_add:
      - NET_ADMIN
    environment:
      <<: *common-env
      SAMBA_GLOBAL_STANZA: |
        workgroup = WORKGROUP
        server string = PotatoStack Samba Server
        server role = standalone server
        log file = /var/log/samba/log.%m
        max log size = 50
        dns proxy = no
        security = user
        passdb backend = tdbsam
        map to guest = Never
      # User configuration - use quoted password for special characters
      ACCOUNT_daniel: "${SAMBA_PASSWORD:-changeme}"
      UID_daniel: "${PUID:-1000}"
      GROUP_daniel: "${PGID:-1000}"
      SAMBA_VOLUME_CONFIG_storage: |
        [storage]
        path = /mnt/storage
        browsable = yes
        read only = no
        guest ok = no
        valid users = daniel
        create mask = 0755
        directory mask = 0755
      SAMBA_VOLUME_CONFIG_cachehdd: |
        [cachehdd]
        path = /mnt/cachehdd
        browsable = yes
        read only = no
        guest ok = no
        valid users = daniel
        create mask = 0755
        directory mask = 0755
      SAMBA_VOLUME_CONFIG_media: |
        [media]
        path = /mnt/storage/media
        browsable = yes
        read only = no
        guest ok = no
        valid users = daniel
        create mask = 0755
        directory mask = 0755
    volumes:
      - /mnt/storage:/mnt/storage
      - /mnt/cachehdd:/mnt/cachehdd
      - samba-lib:/var/lib/samba
      - samba-cache:/var/cache/samba
      - samba-run:/run/samba
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "smbclient -L localhost -U% || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.1"
          memory: 64M

  ################################################################################
  # Gluetun Monitor - VPN Connection Monitor & Auto-Restart
  ################################################################################
  gluetun-monitor:
    image: docker:${DOCKER_TAG:-cli}
    container_name: gluetun-monitor
    logging: *default-logging
    user: "0:0"
    entrypoint: sh /monitor.sh
    environment:
      - GLUETUN_URL=http://gluetun:8008
      - CHECK_INTERVAL=${GLUETUN_CHECK_INTERVAL:-10}
      - RESTART_CONTAINERS=prowlarr sonarr radarr lidarr bookshelf bazarr qbittorrent pyload slskd pinchflat spotiflac stash
      - RESTART_ON_STOP=${GLUETUN_RESTART_ON_STOP:-true}
      - RESTART_ON_FAILURE=${GLUETUN_RESTART_ON_FAILURE:-true}
      - RESTART_COOLDOWN=${GLUETUN_RESTART_COOLDOWN:-120}
      - NTFY_INTERNAL_URL=${NTFY_INTERNAL_URL:-http://ntfy:80}
      - NTFY_TOPIC=${NTFY_TOPIC:-potatostack}
      - NTFY_TOKEN=${NTFY_TOKEN:-}
      - NTFY_DEFAULT_TAGS=${NTFY_DEFAULT_TAGS:-potatostack,monitor}
      - NTFY_DEFAULT_PRIORITY=${NTFY_DEFAULT_PRIORITY:-default}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./scripts/monitor/gluetun-monitor.sh:/monitor.sh:ro
      - ./scripts/monitor/notify.sh:/notify.sh:ro
      - .:/compose:ro
    working_dir: /compose
    networks:
      - potatostack
    depends_on:
      gluetun:
        condition: service_started
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.05"
          memory: 32M
        reservations:
          cpus: "0.025"
          memory: 16M

  # Disk Space Monitor
  disk-space-monitor:
    image: alpine:${ALPINE_TAG:-3.21}
    container_name: disk-space-monitor
    logging: *default-logging
    entrypoint: sh /disk-space-monitor.sh
    environment:
      DISK_MONITOR_PATHS: ${DISK_MONITOR_PATHS:-/mnt/storage /mnt/ssd /mnt/cachehdd}
      DISK_MONITOR_INTERVAL: ${DISK_MONITOR_INTERVAL:-300}
      DISK_MONITOR_WARN: ${DISK_MONITOR_WARN:-80}
      DISK_MONITOR_CRIT: ${DISK_MONITOR_CRIT:-90}
      NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
      NTFY_TOKEN: ${NTFY_TOKEN:-}
      NTFY_DEFAULT_TAGS: ${NTFY_DEFAULT_TAGS:-potatostack,monitor}
      NTFY_DEFAULT_PRIORITY: ${NTFY_DEFAULT_PRIORITY:-default}
    volumes:
      - /mnt/storage:/mnt/storage:ro
      - /mnt/cachehdd:/mnt/cachehdd:ro
      - /mnt/ssd:/mnt/ssd:ro
      - ./scripts/monitor/disk-space-monitor.sh:/disk-space-monitor.sh:ro
      - ./scripts/monitor/notify.sh:/notify.sh:ro
    networks:
      - potatostack
    restart: unless-stopped

  # Traefik Log Monitor
  traefik-log-monitor:
    image: docker:${DOCKER_TAG:-cli}
    container_name: traefik-log-monitor
    logging: *default-logging
    entrypoint: sh /traefik-log-monitor.sh
    environment:
      TRAEFIK_CONTAINER: traefik
      TRAEFIK_LOG_CHECK_INTERVAL: ${TRAEFIK_LOG_CHECK_INTERVAL:-60}
      TRAEFIK_LOG_PATTERNS: ${TRAEFIK_LOG_PATTERNS:-acme|certificate|x509|tls}
      TRAEFIK_LOG_LEVEL_PATTERN: ${TRAEFIK_LOG_LEVEL_PATTERN:-level=error}
      TRAEFIK_RESTART_ON_ERROR: ${TRAEFIK_RESTART_ON_ERROR:-false}
      TRAEFIK_RESTART_COOLDOWN: ${TRAEFIK_RESTART_COOLDOWN:-300}
      TRAEFIK_NOTIFY_COOLDOWN: ${TRAEFIK_NOTIFY_COOLDOWN:-300}
      NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
      NTFY_TOKEN: ${NTFY_TOKEN:-}
      NTFY_DEFAULT_TAGS: ${NTFY_DEFAULT_TAGS:-potatostack,monitor}
      NTFY_DEFAULT_PRIORITY: ${NTFY_DEFAULT_PRIORITY:-default}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./scripts/monitor/traefik-log-monitor.sh:/traefik-log-monitor.sh:ro
      - ./scripts/monitor/notify.sh:/notify.sh:ro
    depends_on:
      - traefik
    networks:
      - potatostack
    restart: unless-stopped

  # Backup Monitor
  backup-monitor:
    image: alpine:${ALPINE_TAG:-3.21}
    container_name: backup-monitor
    logging: *default-logging
    entrypoint: sh /backup-monitor.sh
    environment:
      BACKUP_MONITOR_PATHS: ${BACKUP_MONITOR_PATHS:-/mnt/storage/kopia/stack-snapshot.log /mnt/storage/velld/backups}
      BACKUP_MAX_AGE_HOURS: ${BACKUP_MAX_AGE_HOURS:-48}
      BACKUP_MONITOR_INTERVAL: ${BACKUP_MONITOR_INTERVAL:-3600}
      NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
      NTFY_TOKEN: ${NTFY_TOKEN:-}
      NTFY_DEFAULT_TAGS: ${NTFY_DEFAULT_TAGS:-potatostack,monitor}
      NTFY_DEFAULT_PRIORITY: ${NTFY_DEFAULT_PRIORITY:-default}
    volumes:
      - /mnt/storage:/mnt/storage:ro
      - ./scripts/monitor/backup-monitor.sh:/backup-monitor.sh:ro
      - ./scripts/monitor/notify.sh:/notify.sh:ro
    networks:
      - potatostack
    restart: unless-stopped

  # DB Health Monitor
  db-health-monitor:
    image: docker:${DOCKER_TAG:-cli}
    container_name: db-health-monitor
    logging: *default-logging
    entrypoint: sh /db-health-monitor.sh
    environment:
      DB_MONITOR_INTERVAL: ${DB_MONITOR_INTERVAL:-30}
      DB_FAIL_THRESHOLD: ${DB_FAIL_THRESHOLD:-3}
      DB_RESTART_COOLDOWN: ${DB_RESTART_COOLDOWN:-180}
      DB_RESTART_ON_FAILURE: ${DB_RESTART_ON_FAILURE:-true}
      DB_CHECK_POSTGRES: ${DB_CHECK_POSTGRES:-true}
      DB_CHECK_REDIS: ${DB_CHECK_REDIS:-true}
      DB_CHECK_MONGO: ${DB_CHECK_MONGO:-false}
      POSTGRES_CONTAINER: postgres
      REDIS_CONTAINER: redis-cache
      MONGO_CONTAINER: mongo
      NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
      NTFY_TOKEN: ${NTFY_TOKEN:-}
      NTFY_DEFAULT_TAGS: ${NTFY_DEFAULT_TAGS:-potatostack,monitor}
      NTFY_DEFAULT_PRIORITY: ${NTFY_DEFAULT_PRIORITY:-default}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./scripts/monitor/db-health-monitor.sh:/db-health-monitor.sh:ro
      - ./scripts/monitor/notify.sh:/notify.sh:ro
    depends_on:
      - postgres
      - redis-cache
    networks:
      - potatostack
    restart: unless-stopped

  # Tailscale Connectivity Monitor
  tailscale-connectivity-monitor:
    image: docker:${DOCKER_TAG:-cli}
    container_name: tailscale-connectivity-monitor
    logging: *default-logging
    entrypoint: sh /tailscale-connectivity-monitor.sh
    environment:
      TAILSCALE_CONTAINER: tailscale
      TAILSCALE_PING_TARGET: ${TAILSCALE_PING_TARGET:-}
      TAILSCALE_PING_INTERVAL: ${TAILSCALE_PING_INTERVAL:-60}
      TAILSCALE_PING_FAIL_THRESHOLD: ${TAILSCALE_PING_FAIL_THRESHOLD:-3}
      TAILSCALE_RESTART_COOLDOWN: ${TAILSCALE_RESTART_COOLDOWN:-300}
      NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
      NTFY_TOKEN: ${NTFY_TOKEN:-}
      NTFY_DEFAULT_TAGS: ${NTFY_DEFAULT_TAGS:-potatostack,monitor}
      NTFY_DEFAULT_PRIORITY: ${NTFY_DEFAULT_PRIORITY:-default}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./scripts/monitor/tailscale-connectivity-monitor.sh:/tailscale-connectivity-monitor.sh:ro
      - ./scripts/monitor/notify.sh:/notify.sh:ro
    depends_on:
      - tailscale
    networks:
      - potatostack
    restart: unless-stopped

  # Internet Connectivity Monitor
  internet-connectivity-monitor:
    image: alpine:${ALPINE_TAG:-3.21}
    container_name: internet-connectivity-monitor
    logging: *default-logging
    entrypoint: sh /internet-connectivity-monitor.sh
    environment:
      INTERNET_CHECK_INTERVAL: ${INTERNET_CHECK_INTERVAL:-30}
      INTERNET_FAIL_THRESHOLD: ${INTERNET_FAIL_THRESHOLD:-3}
      INTERNET_CHECK_TIMEOUT: ${INTERNET_CHECK_TIMEOUT:-5}
      INTERNET_CHECK_URLS: ${INTERNET_CHECK_URLS:-https://1.1.1.1 https://www.google.com/generate_204 https://cloudflare.com/cdn-cgi/trace}
      NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
      NTFY_TOKEN: ${NTFY_TOKEN:-}
      NTFY_DEFAULT_TAGS: ${NTFY_DEFAULT_TAGS:-potatostack,monitor}
      NTFY_DEFAULT_PRIORITY: ${NTFY_DEFAULT_PRIORITY:-default}
    volumes:
      - ./scripts/monitor/internet-connectivity-monitor.sh:/internet-connectivity-monitor.sh:ro
      - ./scripts/monitor/notify.sh:/notify.sh:ro
    networks:
      - potatostack
    restart: unless-stopped

  # slskd Queue & Download Monitor
  slskd-queue-monitor:
    image: alpine:${ALPINE_TAG:-3.21}
    container_name: slskd-queue-monitor
    logging: *default-logging
    entrypoint: sh /slskd-queue-monitor.sh
    environment:
      SLSKD_DB_PATH: /slskd/data/transfers.db
      SLSKD_QUEUE_CHECK_INTERVAL: ${SLSKD_QUEUE_CHECK_INTERVAL:-60}
      SLSKD_QUEUE_WARN_PERCENT: ${SLSKD_QUEUE_WARN_PERCENT:-80}
      SLSKD_QUEUE_FILES: ${SLSKD_QUEUE_FILES:-500}
      SLSKD_NOTIFY_LIMIT: ${SLSKD_NOTIFY_LIMIT:-5}
      NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
      NTFY_TOKEN: ${NTFY_TOKEN:-}
      NTFY_DEFAULT_TAGS: ${NTFY_DEFAULT_TAGS:-potatostack,monitor}
      NTFY_DEFAULT_PRIORITY: ${NTFY_DEFAULT_PRIORITY:-default}
    volumes:
      - slskd-config:/slskd:ro
      - ./scripts/monitor/slskd-queue-monitor.sh:/slskd-queue-monitor.sh:ro
      - ./scripts/monitor/notify.sh:/notify.sh:ro
    networks:
      - potatostack
    depends_on:
      - slskd
    restart: unless-stopped

  ################################################################################
  # Velld - Database Backup Scheduler (Postgres, Mongo, Redis)
  ################################################################################
  velld-api:
    image: ghcr.io/dendianugerah/velld/api:${VELLD_API_TAG:-latest}
    container_name: velld-api
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.158}:8085:8080"
    environment:
      <<: *common-env
      JWT_SECRET: ${VELLD_JWT_SECRET}
      ENCRYPTION_KEY: ${VELLD_ENCRYPTION_KEY}
      ADMIN_USERNAME_CREDENTIAL: ${VELLD_ADMIN_USERNAME}
      ADMIN_PASSWORD_CREDENTIAL: ${VELLD_ADMIN_PASSWORD}
      ALLOW_REGISTER: ${VELLD_ALLOW_REGISTER:-false}
    volumes:
      - /mnt/ssd/docker-data/velld:/app/data
      - /mnt/storage/velld/backups:/app/backups
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.velld-api.rule=Host(`velld-api.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.velld-api.entrypoints=websecure"
      - "traefik.http.routers.velld-api.tls=true"
      - "traefik.http.services.velld-api.loadbalancer.server.port=8080"
      - "traefik.http.routers.velld-api.middlewares=sso-chain@docker"

  velld-web:
    image: ghcr.io/dendianugerah/velld/web:${VELLD_WEB_TAG:-latest}
    container_name: velld-web
    logging: *default-logging
    ports:
      - "${HOST_BIND:-192.168.178.158}:3010:3000"
    environment:
      <<: *common-env
      NEXT_PUBLIC_API_URL: ${VELLD_API_URL}
      ALLOW_REGISTER: ${VELLD_ALLOW_REGISTER:-false}
    networks:
      - potatostack
    depends_on:
      - velld-api
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.velld.rule=Host(`velld.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.velld.entrypoints=websecure"
      - "traefik.http.routers.velld.tls=true"
      - "traefik.http.services.velld.loadbalancer.server.port=3000"
      - "traefik.http.routers.velld.middlewares=sso-chain@docker"

  # Snapshot Scheduler - Cron-based Kopia snapshots
  snapshot-scheduler:
    image: docker:${DOCKER_CLI_TAG:-27.2.1}-cli
    container_name: snapshot-scheduler
    logging: *default-logging
    entrypoint: sh -c "chmod 600 /etc/crontabs/root 2>/dev/null || true; crond -f -l 8"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /mnt/ssd/docker-data/cron:/etc/crontabs
      - /mnt/storage/kopia:/mnt/storage/kopia
      - ./scripts/backup/stack-snapshot.sh:/stack-snapshot.sh:ro
    networks:
      - potatostack
    depends_on:
      - kopia
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M
  # Kopia - Fast and secure backup server
  kopia:
    image: kopia/kopia:${KOPIA_TAG:-latest}
    container_name: kopia
    logging: *default-logging
    hostname: kopia-server
    entrypoint: sh /kopia-init.sh
    ports:
      - "${HOST_BIND:-192.168.178.158}:51515:51515"
    environment:
      <<: *common-env
      KOPIA_PASSWORD: ${KOPIA_PASSWORD}
      KOPIA_SERVER_USER: ${KOPIA_SERVER_USER:-admin}
      KOPIA_SERVER_PASSWORD: ${KOPIA_SERVER_PASSWORD}
      KOPIA_HOSTNAME: ${KOPIA_HOSTNAME:-potatostack}
      KOPIA_INSECURE_HTTP: "true"  # HTTP mode for Tailscale proxy
      KOPIA_CONFIG_PATH: /app/config/repository.config
      KOPIA_CACHE_DIRECTORY: /app/cache
      KOPIA_LOG_DIR: /app/logs
    volumes:
      - /mnt/storage/kopia/repository:/repository
      - kopia-config:/app/config
      - /mnt/cachehdd/sync/kopia-cache:/app/cache
      - kopia-logs:/app/logs
      - ./scripts/init/kopia-init.sh:/kopia-init.sh:ro
      # Backup targets
      - vaultwarden-data:/data/vaultwarden:ro
      - /mnt/storage/syncthing:/data/syncthing:ro
      - /mnt/storage/downloads:/data/downloads:ro
      - /mnt/storage/slskd-shared:/data/slskd-shared:ro
      - /mnt/storage/photos:/data/photos:ro
      - /mnt/storage/media:/data/media:ro
      # Host system backup (Debian 13) - includes /etc, /home, /root, /var
      - /etc:/data/host-etc:ro
      - /home:/data/host-home:ro
      - /root:/data/host-root:ro
      - /var/log:/data/host-var-log:ro
      - /mnt/ssd/docker-data:/data/docker-data:ro
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 384M
    healthcheck:
      test: ["CMD", "kopia", "repository", "status"]
      interval: 120s
      timeout: 20s
      retries: 3
      start_period: 60s
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.kopia.rule=Host(`kopia.${HOST_DOMAIN:-local.domain}`)"
      - "traefik.http.routers.kopia.entrypoints=websecure"
      - "traefik.http.routers.kopia.tls=true"
      - "traefik.http.services.kopia.loadbalancer.server.port=51515"
      - "traefik.http.services.kopia.loadbalancer.server.scheme=https"
      - "traefik.http.routers.kopia.middlewares=sso-chain@docker"

################################################################################
# NETWORKS
################################################################################
networks:
  potatostack:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: br-potato-main
    ipam:
      config:
        - subnet: 172.22.0.0/16

################################################################################
# VOLUMES
################################################################################
volumes:
  # Databases use bind mounts to /mnt/ssd/docker-data/

  # Authentication
  authentik-media:
  authentik-custom-templates:
  authentik-certs:
  vaultwarden-data:

  # Reverse Proxy
  traefik-certs:

  # VPN
  gluetun-config:
  tailscale-data:
  tailscale-https-marker:

  # Cloud Storage
  # nextcloud_aio_mastercontainer:  # DISABLED
  syncthing-config:

  # Media Management
  prowlarr-config:
  sonarr-config:
  radarr-config:
  lidarr-config:
  bookshelf-config:
  bazarr-config:
  maintainerr-data:
  jellyfin-config:
  jellyseerr-config:
  audiobookshelf-config:
  navidrome-data:
  stash-config:
  stash-generated:
  stash-metadata:
  stash-cache:

  # Downloads
  qbittorrent-config:
  # aria2-config:  # REMOVED
  pyload-config:
  slskd-config:

  # Shared secrets
  shared-keys:

  # Monitoring
  uptime-kuma-data:
  # prometheus uses bind mount to /mnt/cachehdd/observability/prometheus
  grafana-data:
  netdata-config:
  netdata-lib:
  netdata-cache:
  alloy-data:
  thanos-store-data:
  thanos-compact-data:

  # Automation
  # n8n-data:
  healthchecks-data:

  # Utilities
  # stirling-pdf-data:
  # stirling-pdf-configs:
  linkding-data:
  code-server-config:
  atuin-config:

  # Development
  gitea-data:
  gitea-runner-data:
  sentry-data:

  # AI & Special
  # open-webui-data:
  octobot-data:
  octobot-tentacles:
  octobot-logs:
  pinchflat-config:

  # Dashboard & Container Management
  homarr-data:        # Legacy v0.x - keep for migration backup
  homarr-icons:       # Legacy v0.x - keep for migration backup
  homarr-appdata:     # New v1.x volume

  # Security
  crowdsec-db:
  crowdsec-config:
  traefik-logs:

  # DNS & Ad Blocking (DISABLED)
  # adguard-work:
  # adguard-conf:

  # Monitoring

  # Finance
  actual-data:

  # Utilities (DISABLED - paperless commented out)
  # paperless-data:
  # paperless-media:

  # System
  diun-data:
  ntfy-cache:
  ntfy-data:
  openssh-config:
  samba-lib:
  samba-cache:
  samba-run:

  # Backups
  kopia-config:
  kopia-logs:

  # Enterprise Security & Secrets
  fail2ban-data:
  trivy-cache:

  # Enterprise Observability
  alertmanager-data:
