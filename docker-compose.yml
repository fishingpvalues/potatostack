################################################################################
# POTATOSTACK - Fully Integrated Stack for Le Potato SBC
# Optimized for ARM64 with 2GB RAM
# Includes: VPN (Gluetun), P2P (qBittorrent, slskd/Soulseek), Backups (Kopia),
#           Storage (Filebrowser/Samba/Seafile), Monitoring (Prometheus, Grafana, Loki),
#           Management (Portainer, Diun, Uptime Kuma, Dozzle),
#           Git (Gitea), Reverse Proxy (Nginx Proxy Manager), Dashboard (Homepage),
#           Security (Vaultwarden, Authelia SSO),
#           Photos (Immich - self-hosted Google Photos alternative)
################################################################################

# Global defaults for all services
x-default-env: &default-env
  TZ: Europe/Berlin

x-default-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"

x-default-security: &default-security
  no-new-privileges: true

networks:
  vpn:
    driver: bridge
    name: potatostack_vpn
  backend:
    # Unified network for monitoring + proxy + default services
    driver: bridge
    name: potatostack_backend
  default:
    driver: bridge
    name: potatostack_default

volumes:
  portainer_data:
  prometheus_data:
  grafana_data:
  loki_data:
  alertmanager_data:
  gitea_data:
  postgres_data:
  npm_data:
  npm_ssl:
  npm_db:
  vaultwarden_data:
  immich_model_cache:

services:
  ############################################################################
  # VPN STACK - Gluetun (Universal VPN Client) with Surfshark + Killswitch
  # Replaces ilteoood/docker-surfshark for better reliability, metrics, and firewall
  ############################################################################

  gluetun:
    image: qmcgaw/gluetun:${GLUETUN_TAG:-latest}
    container_name: gluetun
    hostname: gluetun-vpn
    restart: unless-stopped
    cap_add:
      - NET_ADMIN
    devices:
      - /dev/net/tun:/dev/net/tun
    environment:
      <<: *default-env
      # VPN Provider Configuration
      VPN_SERVICE_PROVIDER: surfshark
      VPN_TYPE: ${SURFSHARK_CONNECTION_TYPE:-openvpn}

      # OpenVPN Credentials (from .env)
      OPENVPN_USER: ${SURFSHARK_USER}
      OPENVPN_PASSWORD: ${SURFSHARK_PASSWORD}

      # Server Selection
      SERVER_COUNTRIES: ${SURFSHARK_COUNTRY:-Netherlands}
      SERVER_CITIES: ${SURFSHARK_CITY:-Amsterdam}

      # Firewall & Killswitch (CRITICAL - enforces VPN-only traffic)
      FIREWALL_OUTBOUND_SUBNETS: ${LAN_NETWORK:-192.168.178.0/24}
      FIREWALL_VPN_INPUT_PORTS: 6881
      FIREWALL_INPUT_PORTS: ""
      FIREWALL: "on"

      # DNS Configuration
      DNS_ADDRESS: ${VPN_DNS:-1.1.1.1}
      DOT: "off"

      # Health Check Server (HTTP API on port 8000)
      HTTPPROXY: "off"
      SHADOWSOCKS: "off"
      HTTP_CONTROL_SERVER_ADDRESS: ":8000"
      HTTP_CONTROL_SERVER_LOG: "on"

      # Advanced Settings
      HEALTH_VPN_DURATION_INITIAL: 20s
      HEALTH_VPN_DURATION_ADDITION: 5s
      UPDATER_PERIOD: 24h
      PUBLICIP_PERIOD: 12h
      LOG_LEVEL: info

      # IPv6 (disable for compatibility)
      IPV6: "off"

    networks:
      - vpn
      - backend

    ports:
      # qBittorrent WebUI
      - "${HOST_BIND:-0.0.0.0}:8080:8080"
      # slskd WebUI
      - "${HOST_BIND:-0.0.0.0}:2234:2234"
      # P2P ports (BitTorrent, Soulseek)
      - "${HOST_BIND:-0.0.0.0}:6881:6881"
      - "${HOST_BIND:-0.0.0.0}:6881:6881/udp"
      # Gluetun HTTP Control Server (health checks, metrics)
      - "${HOST_BIND:-0.0.0.0}:8000:8000"

    # Security: Mark source packets for VPN routing
    sysctls:
      - net.ipv4.conf.all.src_valid_mark=1
      - net.ipv6.conf.all.disable_ipv6=1

    # Healthcheck: Use Gluetun's built-in HTTP control server
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--spider",
          "--quiet",
          "http://localhost:8000/v1/publicip/ip",
        ]
      interval: 1m
      timeout: 10s
      retries: 3
      start_period: 30s

    # Resource limits (Le Potato 2GB optimized)
    mem_limit: 128m
    mem_reservation: 96m
    cpus: 1

    logging:
      <<: *default-logging

    labels:
      - "homepage.group=Infrastructure"
      - "homepage.name=Gluetun VPN"
      - "homepage.icon=si-wireguard"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:8000"
      - "homepage.description=VPN killswitch (Surfshark)"

  qbittorrent:
    image: lscr.io/linuxserver/qbittorrent:${QBITTORRENT_TAG:-latest}
    container_name: qbittorrent
    # hostname removed - conflicts with network_mode: service:gluetun
    restart: unless-stopped
    network_mode: service:gluetun
    depends_on:
      gluetun:
        condition: service_healthy
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Berlin
      - WEBUI_PORT=8080
      - TORRENTING_PORT=6881
      # Resource tuning for 2GB RAM
      - BT_MAX_OPEN_FILES=100
      - BT_MAX_CONNECTIONS_GLOBAL=200
      - BT_MAX_UPLOADS_GLOBAL=20
    volumes:
      - /mnt/seconddrive/qbittorrent/config:/config
      - /mnt/cachehdd/torrents:/downloads
      - /mnt/cachehdd/torrents/incomplete:/incomplete
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    labels:
      - "homepage.group=Media & Downloads"
      - "homepage.name=qBittorrent"
      - "homepage.icon=si-qbittorrent"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:8080"
      - "homepage.description=Torrent client via VPN"
      - "homepage.widget.type=qbittorrent"
      - "homepage.widget.url=http://gluetun:8080"
      - "homepage.widget.username=admin"
      - "homepage.widget.password={{HOMEPAGE_VAR_QBITTORRENT_PASSWORD}}"
    mem_limit: 384m
    mem_reservation: 256m
    cpus: 1.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"

  slskd:
    image: ghcr.io/slskd/slskd:${SLSKD_TAG:-latest} # slskd - Modern Soulseek daemon with web UI
    container_name: slskd
    # hostname removed - conflicts with network_mode: service:gluetun
    restart: unless-stopped
    network_mode: service:gluetun
    depends_on:
      gluetun:
        condition: service_healthy
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Berlin
      - SLSKD_HTTP_PORT=2234
      - SLSKD_SLSK_LISTEN_PORT=50000
      - SLSKD_USERNAME=${SLSKD_USER:-admin}
      - SLSKD_PASSWORD=${SLSKD_PASSWORD}
      - SLSKD_METRICS=true
      - SLSKD_METRICS_URL=http://localhost:5031/metrics
    volumes:
      - /mnt/seconddrive/slskd/config:/app
      - /mnt/seconddrive/slskd/logs:/app/logs
      - /mnt/cachehdd/soulseek:/var/slskd/shared
      - /mnt/cachehdd/soulseek/incomplete:/var/slskd/incomplete
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:2234 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    labels:
      - "homepage.group=Media & Downloads"
      - "homepage.name=slskd (Soulseek)"
      - "homepage.icon=slskd.png"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:2234"
      - "homepage.description=Soulseek P2P via VPN (slskd client)"
    mem_limit: 256m
    mem_reservation: 128m
    cpus: 1
    logging:
      options:
        max-size: "10m"
        max-file: "3"

  ############################################################################
  # BACKUP - Kopia Server (from existing config)
  ############################################################################

  kopia:
    image: kopia/kopia:${KOPIA_TAG:-latest}
    container_name: kopia_server
    hostname: lepotato-backup
    restart: unless-stopped
    networks:
      - monitoring
      - backend
    ports:
      - "${HOST_BIND:-0.0.0.0}:51515:51515"
      - "${HOST_BIND:-0.0.0.0}:51516:51516"
    environment:
      - TZ=Europe/Berlin
      - KOPIA_PASSWORD=${KOPIA_PASSWORD}
      - GOGC=50
      - GOMAXPROCS=2
      - KOPIA_CONFIG_PATH=/app/config/repository.config
      - KOPIA_CACHE_DIRECTORY=/app/cache
      - KOPIA_LOG_DIR=/app/logs
      - KOPIA_PROMETHEUS_ENABLED=true
      - KOPIA_PROMETHEUS_LISTEN_ADDR=:51516
    volumes:
      - /mnt/seconddrive/kopia/repository:/repository
      - /mnt/seconddrive/kopia/config:/app/config
      - /mnt/seconddrive/kopia/cache:/app/cache
      - /mnt/seconddrive/kopia/logs:/app/logs
      - /mnt/seconddrive/kopia/tmp:/tmp
      - /:/host:ro
    cap_add:
      - SYS_ADMIN
    security_opt:
      - no-new-privileges:false
    cap_drop:
      - ALL
      - SYS_ADMIN  # Keep only the required cap_add
    devices:
      - /dev/fuse:/dev/fuse
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:51515/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    command:
      - server
      - start
      - --address=0.0.0.0:51515
      - --tls-generate-cert
      - --tls-generate-rsa-key-size=4096
      - --server-username=${KOPIA_SERVER_USER:-admin}
      - --server-password=${KOPIA_SERVER_PASSWORD}
      - --log-level=info
      - --file-log-level=debug
      - --metrics-listen-addr=:51516
    mem_limit: 384m
    mem_reservation: 256m
    cpus: 1.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "homepage.group=Storage & Backup"
      - "homepage.name=Kopia"
      - "homepage.icon=kopia.png"
      - "homepage.href=https://${HOST_ADDR:-192.168.178.40}:51515"
      - "homepage.description=Encrypted backups"
      - "homepage.widget.type=generic"
      - "homepage.widget.url=http://kopia:51515/health"

  ############################################################################
  # UNIFIED BACKUPS - PostgreSQL databases + Vaultwarden (single container)
  ############################################################################

  unified-backups:
    image: alpine:${ALPINE_TAG:-latest}
    container_name: unified_backups
    hostname: unified-backups
    restart: unless-stopped
    networks:
      - default
    environment:
      <<: *default-env
      GITEA_DB_PASSWORD: ${GITEA_DB_PASSWORD}
      IMMICH_DB_PASSWORD: ${IMMICH_DB_PASSWORD}
      SEAFILE_DB_PASSWORD: ${SEAFILE_DB_PASSWORD}
      BACKUP_VAULTWARDEN: ${BACKUP_VAULTWARDEN:-false}
    volumes:
      - /mnt/seconddrive/backups:/backups
      - vaultwarden_data:/vaultwarden:ro
    depends_on:
      postgres:
        condition: service_healthy
    security_opt:
      - *default-security
    command: >
      sh -c "
        set -o pipefail;
        apk add --no-cache postgresql-client sqlite gzip tar busybox-extras;
        while true; do
          BACKUP_DATE=$$(date +%Y-%m-%d-%H%M);

          if nc -z postgres 5432; then
            echo '[DB Backup] Starting PostgreSQL backups...';
            mkdir -p /backups/db;

            PGPASSWORD=$$GITEA_DB_PASSWORD pg_dump -h postgres -U gitea -d gitea | gzip > /backups/db/gitea-$$BACKUP_DATE.sql.gz \
              && echo '[DB Backup] Gitea: OK' || echo '[DB Backup] Gitea: FAILED';

            PGPASSWORD=$$IMMICH_DB_PASSWORD pg_dump -h postgres -U immich -d immich | gzip > /backups/db/immich-$$BACKUP_DATE.sql.gz \
              && echo '[DB Backup] Immich: OK' || echo '[DB Backup] Immich: FAILED';

            PGPASSWORD=$$SEAFILE_DB_PASSWORD pg_dump -h postgres -U seafile -d ccnet_db | gzip > /backups/db/seafile-ccnet-$$BACKUP_DATE.sql.gz \
              && echo '[DB Backup] Seafile ccnet: OK' || echo '[DB Backup] Seafile ccnet: FAILED';

            PGPASSWORD=$$SEAFILE_DB_PASSWORD pg_dump -h postgres -U seafile -d seafile_db | gzip > /backups/db/seafile-db-$$BACKUP_DATE.sql.gz \
              && echo '[DB Backup] Seafile db: OK' || echo '[DB Backup] Seafile db: FAILED';

            PGPASSWORD=$$SEAFILE_DB_PASSWORD pg_dump -h postgres -U seafile -d seahub_db | gzip > /backups/db/seafile-seahub-$$BACKUP_DATE.sql.gz \
              && echo '[DB Backup] Seafile seahub: OK' || echo '[DB Backup] Seafile seahub: FAILED';

            find /backups/db -name '*.sql.gz' -mtime +7 -delete && echo '[DB Backup] Cleanup: Removed backups older than 7 days';
          else
            echo '[DB Backup] Postgres not running, skipping database backups.';
          fi;

          if [ \"$$BACKUP_VAULTWARDEN\" = \"true\" ] && [ -f /vaultwarden/db.sqlite3 ]; then
            echo '[VW Backup] Starting Vaultwarden backup...';
            mkdir -p /backups/vaultwarden /tmp/vw-backup;

            sqlite3 /vaultwarden/db.sqlite3 '.backup /tmp/vw-backup/db.sqlite3' &&
            cp -r /vaultwarden/attachments /tmp/vw-backup/ 2>/dev/null || true;
            cp -r /vaultwarden/sends /tmp/vw-backup/ 2>/dev/null || true;
            cp -r /vaultwarden/icon_cache /tmp/vw-backup/ 2>/dev/null || true;
            cp /vaultwarden/rsa_key* /tmp/vw-backup/ 2>/dev/null || true;

            tar -czf /backups/vaultwarden/vaultwarden-$$BACKUP_DATE.tar.gz -C /tmp/vw-backup . \
              && echo '[VW Backup] OK' || echo '[VW Backup] FAILED';

            rm -rf /tmp/vw-backup;
            find /backups/vaultwarden -name '*.tar.gz' -mtime +7 -delete && echo '[VW Backup] Cleanup: Removed backups older than 7 days';
          fi;

          echo '[Backup] Cycle complete. Sleeping 24h...';
          sleep 1d;
        done
      "
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.25
    logging:
      <<: *default-logging
      options:
        max-size: "5m"
        max-file: "2"

  ############################################################################
  # MONITORING STACK - Prometheus, Grafana, Loki, Thanos, Alertmanager
  ############################################################################

  prometheus:
    image: prom/prometheus:${PROMETHEUS_TAG:-latest}
    container_name: prometheus
    hostname: prometheus
    restart: unless-stopped
    networks:
      - backend
    ports:
      - "${HOST_BIND:-0.0.0.0}:9090:9090"
    security_opt:
      - *default-security
    read_only: false
    environment:
      <<: *default-env
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=${PROMETHEUS_RETENTION_DAYS:-3d}"
      - "--web.console.libraries=/etc/prometheus/console_libraries"
      - "--web.console.templates=/etc/prometheus/consoles"
      - "--web.enable-lifecycle"
    volumes:
      - ./config/prometheus:/etc/prometheus
      - prometheus_data:/prometheus
      - /var/run/docker.sock:/var/run/docker.sock:ro
    depends_on:
      - unified-exporters
    mem_limit: 192m
    mem_reservation: 128m
    cpus: 0.75
    logging:
      <<: *default-logging
    labels:
      - "homepage.group=Monitoring"
      - "homepage.name=Prometheus"
      - "homepage.icon=si-prometheus"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:9090"
      - "homepage.description=Metrics collection"
      - "homepage.widget.type=generic"
      - "homepage.widget.url=http://prometheus:9090/-/healthy"

  grafana:
    image: grafana/grafana:${GRAFANA_TAG:-latest}
    container_name: grafana
    hostname: grafana
    restart: unless-stopped
    networks:
      - backend
    ports:
      - "${HOST_BIND:-0.0.0.0}:3000:3000"
    environment:
      <<: *default-env
      GF_SECURITY_ADMIN_USER: ${GRAFANA_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
      GF_INSTALL_PLUGINS: grafana-piechart-panel,grafana-clock-panel
      GF_SERVER_ROOT_URL: "%(protocol)s://%(domain)s:%(http_port)s/grafana/"
      GF_ANALYTICS_REPORTING_ENABLED: "false"
      # OAuth2 via Authelia
      GF_AUTH_GENERIC_OAUTH_ENABLED: "true"
      GF_AUTH_GENERIC_OAUTH_NAME: Authelia
      GF_AUTH_GENERIC_OAUTH_CLIENT_ID: grafana
      GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET: ${GRAFANA_OIDC_SECRET}
      GF_AUTH_GENERIC_OAUTH_SCOPES: openid profile email groups
      GF_AUTH_GENERIC_OAUTH_AUTH_URL: https://authelia.lepotato.local/api/oidc/authorization
      GF_AUTH_GENERIC_OAUTH_TOKEN_URL: https://authelia.lepotato.local/api/oidc/token
      GF_AUTH_GENERIC_OAUTH_API_URL: https://authelia.lepotato.local/api/oidc/userinfo
      GF_AUTH_GENERIC_OAUTH_ALLOW_SIGN_UP: "true"
      GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH: contains(groups[*], 'admins') && 'Admin' || 'Viewer'
    security_opt:
      - *default-security
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana/provisioning:/etc/grafana/provisioning
    depends_on:
      - prometheus
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --spider --quiet http://localhost:3000/api/health || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    mem_limit: 128m
    mem_reservation: 96m
    cpus: 0.75
    logging:
      <<: *default-logging
    labels:
      - "homepage.group=Monitoring"
      - "homepage.name=Grafana"
      - "homepage.icon=si-grafana"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:3000"
      - "homepage.description=Dashboards"
      - "homepage.widget.type=grafana"
      - "homepage.widget.url=http://grafana:3000"
      - "homepage.widget.username={{HOMEPAGE_VAR_GRAFANA_USER}}"
      - "homepage.widget.password={{HOMEPAGE_VAR_GRAFANA_PASSWORD}}"

  loki:
    image: grafana/loki:${LOKI_TAG:-latest}
    container_name: loki
    hostname: loki
    restart: unless-stopped
    networks:
      - backend
    ports:
      - "${HOST_BIND:-0.0.0.0}:3100:3100"
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    read_only: false
    command: -config.file=/etc/loki/local-config.yaml -config.expand-env=true
    volumes:
      - ./config/loki:/etc/loki
      - loki_data:/loki
      - /mnt/seconddrive:/mnt/seconddrive:ro
      - /mnt/cachehdd:/mnt/cachehdd:ro
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "homepage.group=Monitoring"
      - "homepage.name=Loki"
      - "homepage.icon=si-grafana"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:3100"
      - "homepage.description=Log store (Loki)"
      - "homepage.widget.type=generic"
      - "homepage.widget.url=http://loki:3100/ready"

  promtail:
    image: grafana/promtail:${PROMTAIL_TAG:-latest}
    container_name: promtail
    hostname: promtail
    restart: unless-stopped
    networks:
      - backend
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    volumes:
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /mnt/seconddrive/kopia/logs:/kopia-logs:ro
      - /mnt/seconddrive/qbittorrent/config/logs:/qbittorrent-logs:ro
      - /mnt/seconddrive/slskd/logs:/slskd-logs:ro
      - ./config/promtail:/etc/promtail
      - /var/lib/promtail:/var/lib/promtail
      - /mnt/seconddrive:/mnt/seconddrive:ro
      - /mnt/cachehdd:/mnt/cachehdd:ro
    command: -config.file=/etc/promtail/config.yml
    depends_on:
      - loki
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"

  alertmanager:
    image: prom/alertmanager:${ALERTMANAGER_TAG:-latest}
    container_name: alertmanager
    hostname: alertmanager
    restart: unless-stopped
    networks:
      - backend
    ports:
      - "${HOST_BIND:-0.0.0.0}:9093:9093"
    environment:
      - ALERT_EMAIL_USER=${ALERT_EMAIL_USER}
      - ALERT_EMAIL_PASSWORD=${ALERT_EMAIL_PASSWORD}
      - ALERT_EMAIL_TO=${ALERT_EMAIL_TO}
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    read_only: false
    volumes:
      - ./config/alertmanager:/etc/alertmanager
      - alertmanager_data:/alertmanager
      - /mnt/seconddrive:/mnt/seconddrive:ro
      - /mnt/cachehdd:/mnt/cachehdd:ro
    command:
      - "--config.file=/etc/alertmanager/config.yml"
      - "--storage.path=/alertmanager"
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "homepage.group=Monitoring"
      - "homepage.name=Alertmanager"
      - "homepage.icon=si-prometheus"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:9093"
      - "homepage.description=Alert routing"
      - "homepage.widget.type=generic"
      - "homepage.widget.url=http://alertmanager:9093/api/v2/status"

  ############################################################################
  # UNIFIED SYSTEM MONITORING - Node + SMART + Process metrics in one container
  ############################################################################

  unified-exporters:
    # Multi-exporter container running node-exporter + smartctl-exporter via supervisord
    image: alpine:${ALPINE_TAG:-latest}
    container_name: unified-exporters
    hostname: unified-exporters
    restart: unless-stopped
    networks:
      - backend
    ports:
      - "${HOST_BIND:-0.0.0.0}:9100:9100"  # node-exporter
      - "${HOST_BIND:-0.0.0.0}:9633:9633"  # smartctl-exporter
    privileged: true  # Required for SMART access
    environment:
      <<: *default-env
    command: >
      sh -c "
        apk add --no-cache wget curl smartmontools;
        wget -qO /usr/local/bin/node_exporter https://github.com/prometheus/node_exporter/releases/download/v1.7.0/node_exporter-1.7.0.linux-arm64.tar.gz &&
        tar -xzf /usr/local/bin/node_exporter -C /usr/local/bin --strip-components=1 node_exporter-1.7.0.linux-arm64/node_exporter 2>/dev/null || wget -qO- https://github.com/prometheus/node_exporter/releases/download/v1.7.0/node_exporter-1.7.0.linux-arm64.tar.gz | tar xzf - -C /tmp && mv /tmp/node_exporter*/node_exporter /usr/local/bin/;
        wget -qO /usr/local/bin/smartctl_exporter https://github.com/prometheus-community/smartctl_exporter/releases/download/v0.12.0/smartctl_exporter-0.12.0.linux-arm64.tar.gz &&
        tar -xzf /usr/local/bin/smartctl_exporter -C /usr/local/bin --strip-components=1 smartctl_exporter-0.12.0.linux-arm64/smartctl_exporter 2>/dev/null || wget -qO- https://github.com/prometheus-community/smartctl_exporter/releases/download/v0.12.0/smartctl_exporter-0.12.0.linux-arm64.tar.gz | tar xzf - -C /tmp && mv /tmp/smartctl_exporter*/smartctl_exporter /usr/local/bin/;
        chmod +x /usr/local/bin/node_exporter /usr/local/bin/smartctl_exporter;
        /usr/local/bin/node_exporter --path.procfs=/host/proc --path.sysfs=/host/sys --path.rootfs=/rootfs --collector.filesystem.mount-points-exclude='^/(sys|proc|dev|host|etc)($$|/)' &
        /usr/local/bin/smartctl_exporter --web.listen-address=:9633 &
        wait
      "
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    mem_limit: 64m
    mem_reservation: 32m
    cpus: 0.5
    logging:
      <<: *default-logging
      options:
        max-size: "5m"
        max-file: "2"
    labels:
      - "homepage.group=Monitoring"
      - "homepage.name=System Exporters"
      - "homepage.icon=si-prometheus"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:9100"
      - "homepage.description=Node + SMART metrics"

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:${CADVISOR_TAG:-latest}
    container_name: cadvisor
    hostname: cadvisor
    restart: unless-stopped
    networks:
      - monitoring
    ports:
      - "${HOST_BIND:-0.0.0.0}:8081:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker:/var/lib/docker:ro
      - /dev/disk:/dev/disk:ro
    devices:
      - /dev/kmsg
    privileged: true
    mem_limit: 64m
    mem_reservation: 32m
    cpus: 0.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "homepage.group=Monitoring"
      - "homepage.name=cAdvisor"
      - "homepage.icon=si-docker"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:8081"
      - "homepage.description=Container metrics"
      - "homepage.widget.type=generic"
      - "homepage.widget.url=http://cadvisor:8080/healthz"
    command:
      - "--docker_only=true"
      - "--housekeeping_interval=15s"
      - "--max_housekeeping_interval=1m"
      - "--store_container_labels=false"
      - "--disable_metrics=advtcp,sched,hugetlb,percpu,process"


  netdata:
    image: netdata/netdata:${NETDATA_TAG:-latest}
    container_name: netdata
    hostname: lepotato-netdata
    restart: unless-stopped
    networks:
      - monitoring
      - backend
    ports:
      - "${HOST_BIND:-0.0.0.0}:19999:19999"
    cap_add:
      - SYS_PTRACE
      - SYS_ADMIN
    security_opt:
      - no-new-privileges:false
      - apparmor:unconfined
    # cap_drop: ALL removed - was too restrictive
      - SYS_PTRACE  # Keep only the required cap_add
      - SYS_ADMIN
    environment:
      - NETDATA_CLAIM_TOKEN=${NETDATA_CLAIM_TOKEN}
      - NETDATA_CLAIM_URL=https://app.netdata.cloud
      - NETDATA_CLAIM_ROOMS=${NETDATA_CLAIM_ROOMS}
      - DOCKER_HOST=/var/run/docker.sock
    volumes:
      - /etc/passwd:/host/etc/passwd:ro
      - /etc/group:/host/etc/group:ro
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /mnt/seconddrive:/host/mnt/seconddrive:ro
      - /mnt/cachehdd:/host/mnt/cachehdd:ro
    labels:
      - "homepage.group=Monitoring"
      - "homepage.name=Netdata"
      - "homepage.icon=netdata.png"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:19999"
      - "homepage.description=Real-time system monitoring"
      - "homepage.widget.type=generic"
      - "homepage.widget.url=http://netdata:19999/api/v1/info"
    mem_limit: 256m
    mem_reservation: 128m
    cpus: 1
    logging:
      options:
        max-size: "10m"
        max-file: "3"

  blackbox-exporter:
    image: prom/blackbox-exporter:${BLACKBOX_EXPORTER_TAG:-latest}
    container_name: blackbox-exporter
    hostname: blackbox-exporter
    restart: unless-stopped
    networks:
      - monitoring
      - backend
      - default
    ports:
      - "${HOST_BIND:-0.0.0.0}:9115:9115"
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    volumes:
      - ./config/blackbox:/config
    command:
      - "--config.file=/config/blackbox.yml"
      - "--web.listen-address=:9115"
    mem_limit: 64m
    mem_reservation: 32m
    cpus: 0.25
    logging:
      options:
        max-size: "5m"
        max-file: "2"
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9115"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    depends_on:
      - prometheus
    labels:
      - "homepage.group=Monitoring"
      - "homepage.name=Blackbox Exporter"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:9115"
      - "homepage.description=HTTP/TCP service monitoring"

  ############################################################################
  # SPEEDTEST EXPORTER - Internet Speed & ISP Health Monitoring
  ############################################################################

  speedtest-exporter:
    image: miguelndecarvalho/speedtest-exporter:${SPEEDTEST_EXPORTER_TAG:-latest}
    container_name: speedtest-exporter
    hostname: speedtest-exporter
    restart: unless-stopped
    networks:
      - monitoring
    environment:
      - SPEEDTEST_INTERVAL=3600  # Run speedtest every hour
      - SPEEDTEST_HOST=  # Use closest server (default)
      - SPEEDTEST_PORT=8080
    ports:
      - "${HOST_BIND:-0.0.0.0}:9798:9798"
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "homepage.group=Monitoring"
      - "homepage.name=Speedtest Exporter"
      - "homepage.href=http://speedtest-exporter:9798"
      - "homepage.icon=speedtest.png"

  ############################################################################
  # FRITZBOX EXPORTER - Router Monitoring (Fritz!Box 7530 AX DSL)
  ############################################################################

  fritzbox-exporter:
    image: pdreker/fritz_exporter:${FRITZBOX_EXPORTER_TAG:-latest}
    container_name: fritzbox-exporter
    hostname: fritzbox-exporter
    restart: unless-stopped
    networks:
      - monitoring
      - default
    environment:
      - FRITZ_USERNAME=${FRITZ_USERNAME:-}  # Fritz!Box username (usually empty for default)
      - FRITZ_PASSWORD=${FRITZ_PASSWORD:-}  # Fritz!Box password
      - FRITZ_HOSTNAME=${FRITZ_HOSTNAME:-fritz.box}  # Fritz!Box hostname/IP
      - FRITZ_PORT=${FRITZ_PORT:-49000}  # TR-064 port (default)
      - FRITZ_PROTOCOL=${FRITZ_PROTOCOL:-https}  # Protocol (http/https)
      - FRITZ_DEVICE_NAME=${FRITZ_DEVICE_NAME:-fritzbox-7530}  # Device identifier
    ports:
      - "${HOST_BIND:-0.0.0.0}:9042:9042"
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.25
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "homepage.group=Monitoring"
      - "homepage.name=Fritz!Box Exporter"
      - "homepage.href=http://fritzbox-exporter:9042"
      - "homepage.icon=router.png"
      - "homepage.description=Router metrics"

  redis:
    image: redis:${REDIS_TAG:-7-alpine}
    container_name: redis
    hostname: redis
    restart: unless-stopped
    networks:
      - default
    environment:
      <<: *default-env
    security_opt:
      - *default-security
    read_only: false
    command:
      [
        "redis-server",
        "--save",
        "",
        "--appendonly",
        "no",
        "--maxmemory",
        "64mb",
        "--maxmemory-policy",
        "allkeys-lru",
        "--databases",
        "8",
      ]
    mem_limit: 64m
    mem_reservation: 48m
    cpus: 0.5
    logging:
      <<: *default-logging
      options:
        max-size: "5m"
        max-file: "2"
    labels:
      - "homepage.group=Infrastructure"
      - "homepage.name=Redis (Shared)"
      - "homepage.icon=si-redis"
      - "homepage.description=Shared cache (Gitea, Immich, Seafile, Authelia)"

  ############################################################################
  # UNIFIED FILE SERVER - Samba + SFTP + Filebrowser in one container
  ############################################################################

  unified-fileserver:
    image: alpine:${ALPINE_TAG:-latest}
    container_name: unified-fileserver
    hostname: fileserver
    restart: unless-stopped
    networks:
      - default
      - backend
    ports:
      - "${HOST_BIND:-0.0.0.0}:445:445"    # Samba
      - "${HOST_BIND:-0.0.0.0}:139:139"    # Samba
      - "${HOST_BIND:-0.0.0.0}:2223:22"    # SFTP
      - "${HOST_BIND:-0.0.0.0}:8087:8087"  # Filebrowser
    environment:
      <<: *default-env
      SAMBA_USER: ${SAMBA_USER:-files}
      SAMBA_PASSWORD: ${SAMBA_PASSWORD:-changeme}
      SFTP_USER: ${SFTP_USER:-files}
    volumes:
      - /mnt/seconddrive:/data
      - ./config/ssh:/config/ssh:ro
      - ./config/filebrowser:/filebrowser-db
    command: >
      sh -c "
        set -e;
        echo '[FileServer] Installing packages...';
        apk add --no-cache samba samba-common-tools openssh-server wget;

        echo '[FileServer] Configuring Samba...';
        mkdir -p /var/lib/samba /run/samba;
        (echo \"$$SAMBA_PASSWORD\"; echo \"$$SAMBA_PASSWORD\") | smbpasswd -a -s $$SAMBA_USER 2>/dev/null || adduser -D $$SAMBA_USER && (echo \"$$SAMBA_PASSWORD\"; echo \"$$SAMBA_PASSWORD\") | smbpasswd -a -s $$SAMBA_USER;
        cat > /etc/samba/smb.conf <<'SMBEOF'
        [global]
        workgroup = WORKGROUP
        server string = PotatoStack FileServer
        security = user
        map to guest = Bad User
        log level = 1

        [seconddrive]
        path = /data
        browseable = yes
        writable = yes
        valid users = files
        create mask = 0644
        directory mask = 0755
        SMBEOF

        echo '[FileServer] Configuring SFTP...';
        ssh-keygen -A;
        mkdir -p /config/ssh;
        if [ -f /config/ssh/authorized_keys ]; then
          mkdir -p /home/$$SFTP_USER/.ssh;
          cp /config/ssh/authorized_keys /home/$$SFTP_USER/.ssh/;
          chown -R $$SFTP_USER:$$SFTP_USER /home/$$SFTP_USER/.ssh;
          chmod 700 /home/$$SFTP_USER/.ssh;
          chmod 600 /home/$$SFTP_USER/.ssh/authorized_keys;
        fi;

        echo '[FileServer] Downloading Filebrowser...';
        wget -qO /tmp/filebrowser.tar.gz https://github.com/filebrowser/filebrowser/releases/download/v2.27.0/linux-arm64-filebrowser.tar.gz;
        tar -xzf /tmp/filebrowser.tar.gz -C /usr/local/bin;
        chmod +x /usr/local/bin/filebrowser;

        echo '[FileServer] Starting services...';
        smbd -D --no-process-group;
        nmbd -D;
        /usr/sbin/sshd -D -e -p 22 &
        /usr/local/bin/filebrowser --address 0.0.0.0 --port 8087 --database /filebrowser-db/filebrowser.db --root /data &

        echo '[FileServer] All services running';
        wait
      "
    mem_limit: 192m
    mem_reservation: 128m
    cpus: 0.75
    logging:
      <<: *default-logging
      options:
        max-size: "5m"
        max-file: "2"
    labels:
      - "homepage.group=Storage & Backup"
      - "homepage.name=File Server"
      - "homepage.icon=si-folder"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:8087"
      - "homepage.description=Unified SMB + SFTP + Web access"

  ############################################################################
  # SEAFILE - Lightweight File Sync & Share (Nextcloud alternative)
  # Now uses shared PostgreSQL + Redis (consolidated from MariaDB + memcached)
  ############################################################################

  seafile:
    image: seafileltd/seafile-mc:latest
    container_name: seafile
    hostname: seafile
    restart: unless-stopped
    networks:
      - default
      - backend
      - monitoring
    ports:
      - "${HOST_BIND:-0.0.0.0}:8001:80"
    environment:
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_USER=seafile
      - DB_PASSWORD=${SEAFILE_DB_PASSWORD}
      - CCNET_DB=ccnet_db
      - SEAFILE_DB=seafile_db
      - SEAHUB_DB=seahub_db
      - SEAFILE_ADMIN_EMAIL=${SEAFILE_ADMIN_EMAIL}
      - SEAFILE_ADMIN_PASSWORD=${SEAFILE_ADMIN_PASSWORD}
      - SEAFILE_SERVER_LETSENCRYPT=false
      - SEAFILE_SERVER_HOSTNAME=seafile.${HOST_DOMAIN:-lepotato.local}
      - TIME_ZONE=Europe/Berlin
      # Use Redis instead of memcached
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=6
    volumes:
      - /mnt/seconddrive/seafile:/shared
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
    healthcheck:
      test: ["CMD-SHELL", "wget --spider --quiet http://localhost/seafile || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s
    mem_limit: 384m
    mem_reservation: 256m
    cpus: 1
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "homepage.group=Storage & Backup"
      - "homepage.name=Seafile"
      - "homepage.icon=seafile.png"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:8001"
      - "homepage.description=Lightweight file sync & share"


  ############################################################################
  # GITEA - Git server
  ############################################################################

  postgres:
    # Unified Postgres for Gitea + Immich + Seafile (includes vector extension)
    image: tensorchord/pgvecto-rs:pg14-v0.2.0
    container_name: postgres
    hostname: postgres
    restart: unless-stopped
    networks:
      - default
    security_opt:
      - *default-security
    environment:
      <<: *default-env
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
      POSTGRES_DB: postgres
      # Pass app DB passwords for init script
      GITEA_DB_PASSWORD: ${GITEA_DB_PASSWORD}
      IMMICH_DB_PASSWORD: ${IMMICH_DB_PASSWORD}
      SEAFILE_DB_PASSWORD: ${SEAFILE_DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./config/postgres/init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d postgres"]
      interval: 30s
      timeout: 10s
      retries: 5
    # Optimized Postgres tuning for 2GB RAM SBC (3 apps: Gitea + Immich + Seafile)
    command: [
      "postgres",
      "-c", "shared_buffers=48MB",
      "-c", "work_mem=4MB",
      "-c", "maintenance_work_mem=48MB",
      "-c", "effective_cache_size=192MB",
      "-c", "max_connections=40",
      "-c", "random_page_cost=2.0",
      "-c", "jit=off",
      "-c", "max_wal_size=512MB",
      "-c", "min_wal_size=128MB"
    ]
    mem_limit: 192m
    mem_reservation: 96m
    cpus: 1
    logging:
      <<: *default-logging

  gitea:
    image: gitea/gitea:${GITEA_TAG:-latest}
    container_name: gitea
    hostname: gitea
    restart: unless-stopped
    networks:
      - default
      - backend
    ports:
      - "${HOST_BIND:-0.0.0.0}:3001:3000"
      - "${HOST_BIND:-0.0.0.0}:2222:22"
    security_opt:
      - *default-security
    environment:
      <<: *default-env
      USER_UID: 1000
      USER_GID: 1000
      GITEA__database__DB_TYPE: postgres
      GITEA__database__HOST: postgres:5432
      GITEA__database__NAME: gitea
      GITEA__database__USER: gitea
      GITEA__database__PASSWD: ${GITEA_DB_PASSWORD}
      GITEA__cache__ADAPTER: redis
      GITEA__cache__HOST: redis:6379/3
      GITEA__session__PROVIDER: redis
      GITEA__session__PROVIDER_CONFIG: redis:6379/4
      GITEA__queue__TYPE: redis
      GITEA__queue__CONN_STR: redis:6379/5
    volumes:
      - gitea_data:/data
      - /mnt/seconddrive/gitea:/data/gitea/repositories
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    depends_on:
      - redis
      - postgres
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.5
    logging:
      <<: *default-logging
    labels:
      - "homepage.group=Infrastructure"
      - "homepage.name=Gitea"
      - "homepage.icon=si-gitea"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:3001"
      - "homepage.description=Git server"
      - "homepage.widget.type=gitea"
      - "homepage.widget.url=http://gitea:3000"
      - "homepage.widget.token={{HOMEPAGE_VAR_GITEA_TOKEN}}"

  ############################################################################
  # MANAGEMENT TOOLS - Portainer, Diun, Uptime Kuma, Dozzle
  ############################################################################

  portainer:
    image: portainer/portainer-ce:${PORTAINER_TAG:-latest}
    container_name: portainer
    hostname: portainer
    restart: unless-stopped
    networks:
      - default
      - backend
    ports:
      - "${HOST_BIND:-0.0.0.0}:9000:9000"
      - "${HOST_BIND:-0.0.0.0}:9443:9443"
    environment:
      <<: *default-env
    security_opt:
      - *default-security
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - portainer_data:/data
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.5
    logging:
      <<: *default-logging
      options:
        max-size: "5m"
        max-file: "2"
    labels:
      - "homepage.group=Management"
      - "homepage.name=Portainer"
      - "homepage.icon=si-portainer"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:9000"
      - "homepage.description=Docker management"
      - "homepage.widget.type=portainer"
      - "homepage.widget.url=http://portainer:9000"
      - "homepage.widget.key={{HOMEPAGE_VAR_PORTAINER_KEY}}"

  unified-management:
    # Combined autoheal + diun - monitors container health and image updates
    image: alpine:${ALPINE_TAG:-latest}
    container_name: unified-management
    hostname: management
    restart: unless-stopped
    networks:
      - backend
    security_opt:
      - *default-security
    environment:
      <<: *default-env
      AUTOHEAL_INTERVAL: 30
      AUTOHEAL_START_PERIOD: 300
      DIUN_CHECK_INTERVAL: 12h
    volumes:
      - ./config/diun:/etc/diun:ro
      - ./data/diun:/data
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /etc/localtime:/etc/localtime:ro
    command: >
      sh -c "
        set -e;
        echo '[Management] Installing packages...';
        apk add --no-cache docker-cli curl jq wget;

        wget -qO /usr/local/bin/diun https://github.com/crazy-max/diun/releases/download/v4.28.0/diun_4.28.0_linux_arm64.tar.gz &&
        tar -xzf /usr/local/bin/diun -C /usr/local/bin diun 2>/dev/null || wget -qO- https://github.com/crazy-max/diun/releases/download/v4.28.0/diun_4.28.0_linux_arm64.tar.gz | tar xzf - -C /usr/local/bin diun;
        chmod +x /usr/local/bin/diun;

        echo '[Management] Starting autoheal monitoring...';
        (
          while true; do
            sleep $$AUTOHEAL_INTERVAL;
            docker ps --format '{{.ID}}:{{.Names}}' | while IFS=: read -r container_id container_name; do
              health=\$(docker inspect --format='{{.State.Health.Status}}' \"$$container_id\" 2>/dev/null || echo \"none\");
              if [ \"$$health\" = \"unhealthy\" ]; then
                echo \"[Autoheal] Restarting unhealthy container: $$container_name\";
                docker restart \"$$container_id\";
              fi;
            done;
          done
        ) &

        echo '[Management] Starting Diun image monitoring...';
        /usr/local/bin/diun serve --config /etc/diun/diun.yml &

        echo '[Management] All monitoring services running';
        wait
      "
    mem_limit: 96m
    mem_reservation: 48m
    cpus: 0.5
    logging:
      <<: *default-logging
      options:
        max-size: "5m"
        max-file: "2"
    labels:
      - "homepage.group=Management"
      - "homepage.name=Management Daemon"
      - "homepage.icon=mdi-cog-sync"
      - "homepage.description=Autoheal + image updates"
      - "diun.enable=true"

  uptime-kuma:
    image: louislam/uptime-kuma:${UPTIME_KUMA_TAG:-latest}
    container_name: uptime-kuma
    hostname: uptime-kuma
    restart: unless-stopped
    networks:
      - default
      - backend
      - monitoring
    ports:
      - "${HOST_BIND:-0.0.0.0}:3002:3001"
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    volumes:
      - /mnt/seconddrive/uptime-kuma:/app/data
      - /var/run/docker.sock:/var/run/docker.sock:ro
    mem_limit: 256m
    mem_reservation: 128m
    cpus: 0.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "homepage.group=Monitoring"
      - "homepage.name=Uptime Kuma"
      - "homepage.icon=si-uptimekuma"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:3002"
      - "homepage.description=Status monitoring"
      - "homepage.widget.type=uptimekuma"
      - "homepage.widget.url=http://uptime-kuma:3001"

  dozzle:
    image: amir20/dozzle:${DOZZLE_TAG:-latest}
    container_name: dozzle
    hostname: dozzle
    restart: unless-stopped
    networks:
      - backend
    ports:
      - "${HOST_BIND:-0.0.0.0}:8083:8080"
    security_opt:
      - *default-security
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      <<: *default-env
      DOZZLE_LEVEL: info
      DOZZLE_TAILSIZE: 300
      DOZZLE_FILTER: status=running
    mem_limit: 64m
    mem_reservation: 32m
    cpus: 0.25
    logging:
      <<: *default-logging
      options:
        max-size: "5m"
        max-file: "2"
    labels:
      - "homepage.group=Management"
      - "homepage.name=Dozzle"
      - "homepage.icon=si-docker"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:8083"
      - "homepage.description=Live container logs"
      - "homepage.widget.type=dozzle"
      - "homepage.widget.url=http://dozzle:8080"


  ############################################################################
  # PASSWORD MANAGER - Vaultwarden (Bitwarden-compatible)
  ############################################################################

  vaultwarden:
    image: vaultwarden/server:${VAULTWARDEN_TAG:-latest}
    container_name: vaultwarden
    hostname: vaultwarden
    restart: unless-stopped
    networks:
      - backend
      - default
      - monitoring
    ports:
      - "${HOST_BIND:-0.0.0.0}:8084:80"
      - "${HOST_BIND:-0.0.0.0}:3012:3012"  # WebSocket port
    security_opt:
      - no-new-privileges:false
    # cap_drop: ALL removed - was too restrictive
    environment:
      - TZ=Europe/Berlin
      - DOMAIN=https://vaultwarden.lepotato.local
      - ROCKET_PORT=80
      - WEBSOCKET_ENABLED=true
      - WEBSOCKET_PORT=3012
      - SIGNUPS_ALLOWED=${VAULTWARDEN_SIGNUPS_ALLOWED:-false}
      - INVITATIONS_ALLOWED=${VAULTWARDEN_INVITATIONS_ALLOWED:-true}
      - SHOW_PASSWORD_HINT=false
      - ADMIN_TOKEN=${VAULTWARDEN_ADMIN_TOKEN}
      - SMTP_HOST=smtp.gmail.com
      - SMTP_FROM=${ALERT_EMAIL_USER}
      - SMTP_PORT=587
      - SMTP_SECURITY=starttls
      - SMTP_USERNAME=${ALERT_EMAIL_USER}
      - SMTP_PASSWORD=${ALERT_EMAIL_PASSWORD}
      - LOG_LEVEL=info
      - EXTENDED_LOGGING=true
      - LOG_FILE=/data/vaultwarden.log
      # SSO via Authelia (OAuth2)
      - SSO_ENABLED=${VAULTWARDEN_SSO_ENABLED:-false}
      - SSO_ONLY=${VAULTWARDEN_SSO_ONLY:-false}
      - SSO_CLIENT_ID=vaultwarden
      - SSO_CLIENT_SECRET=${VAULTWARDEN_OIDC_SECRET}
      - SSO_AUTHORITY=https://authelia.lepotato.local
      # Database configuration (SQLite by default)
      - DATABASE_URL=/data/db.sqlite3
      # Icon cache
      - ICON_CACHE_TTL=2592000
      - ICON_CACHE_NEGTTL=259200
      # Security settings
      - PASSWORD_ITERATIONS=600000
      - REQUIRE_DEVICE_EMAIL=${VAULTWARDEN_REQUIRE_DEVICE_EMAIL:-false}
      - DISABLE_2FA_REMEMBER=${VAULTWARDEN_DISABLE_2FA_REMEMBER:-false}
    volumes:
      - vaultwarden_data:/data
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:80/alive || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.5
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "homepage.group=Security"
      - "homepage.name=Vaultwarden"
      - "homepage.icon=bitwarden.png"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:8084"
      - "homepage.description=Password manager (Bitwarden)"
      - "homepage.widget.type=generic"
      - "homepage.widget.url=http://vaultwarden:80/alive"

  ############################################################################
  # SINGLE SIGN-ON (SSO) - Authelia with OAuth2/OIDC
  ############################################################################

  authelia:
    # Always enabled per user preference
    image: authelia/authelia:${AUTHELIA_TAG:-latest}
    container_name: authelia
    hostname: authelia
    restart: unless-stopped
    networks:
      - backend
      - default
      - monitoring
    ports:
      - "${HOST_BIND:-0.0.0.0}:9091:9091"
    security_opt:
      - *default-security
    environment:
      <<: *default-env
      AUTHELIA_JWT_SECRET: ${AUTHELIA_JWT_SECRET}
      AUTHELIA_SESSION_SECRET: ${AUTHELIA_SESSION_SECRET}
      AUTHELIA_STORAGE_ENCRYPTION_KEY: ${AUTHELIA_STORAGE_ENCRYPTION_KEY}
      AUTHELIA_IDENTITY_PROVIDERS_OIDC_HMAC_SECRET: ${AUTHELIA_OIDC_HMAC_SECRET}
      AUTHELIA_IDENTITY_PROVIDERS_OIDC_ISSUER_PRIVATE_KEY: ${AUTHELIA_OIDC_PRIVATE_KEY}
      AUTHELIA_NOTIFIER_SMTP_USERNAME: ${ALERT_EMAIL_USER}
      AUTHELIA_NOTIFIER_SMTP_PASSWORD: ${ALERT_EMAIL_PASSWORD}
    volumes:
      - ./config/authelia:/config
    depends_on:
      - redis
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9091/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.5
    logging:
      <<: *default-logging
    labels:
      - "homepage.group=Infrastructure"
      - "homepage.name=Authelia SSO"
      - "homepage.icon=si-auth0"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:9091"
      - "homepage.description=Single Sign-On & 2FA"

  ############################################################################
  # REVERSE PROXY & DASHBOARD
  ############################################################################

  nginx-proxy-manager:
    image: jc21/nginx-proxy-manager:${NPM_TAG:-latest}
    container_name: nginx-proxy-manager
    hostname: npm
    restart: unless-stopped
    networks:
      - backend
      - monitoring
    ports:
      - "${HOST_BIND:-0.0.0.0}:80:80"
      - "${HOST_BIND:-0.0.0.0}:443:443"
      - "${HOST_BIND:-0.0.0.0}:81:81"
    security_opt:
      - *default-security
    environment:
      <<: *default-env
      DB_SQLITE_FILE: /data/database.sqlite
      DISABLE_IPV6: "true"
    volumes:
      - npm_data:/data
      - npm_ssl:/etc/letsencrypt
    healthcheck:
      test: ["CMD-SHELL", "wget --spider --quiet http://localhost:81 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    mem_limit: 128m
    mem_reservation: 64m
    cpus: 0.5
    logging:
      <<: *default-logging
    labels:
      - "homepage.group=Infrastructure"
      - "homepage.name=Nginx Proxy Manager"
      - "homepage.icon=si-nginx"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:81"
      - "homepage.description=Reverse proxy & SSL"
      - "homepage.widget.type=npm"
      - "homepage.widget.url=http://nginx-proxy-manager:81"
      - "homepage.widget.username={{HOMEPAGE_VAR_NPM_USER}}"
      - "homepage.widget.password={{HOMEPAGE_VAR_NPM_PASSWORD}}"

  homepage:
    image: ghcr.io/gethomepage/homepage:${HOMEPAGE_TAG:-latest}
    container_name: homepage
    hostname: homepage
    restart: unless-stopped
    networks:
      - backend
      - monitoring
      - default
    ports:
      - "${HOST_BIND:-0.0.0.0}:3003:3000"
    security_opt:
      - *default-security
    environment:
      <<: *default-env
      PUID: 1000
      PGID: 1000
      HOMEPAGE_VAR_QBITTORRENT_PASSWORD: ${HOMEPAGE_VAR_QBITTORRENT_PASSWORD}
      HOMEPAGE_VAR_PORTAINER_KEY: ${HOMEPAGE_VAR_PORTAINER_KEY}
      HOMEPAGE_VAR_GITEA_TOKEN: ${HOMEPAGE_VAR_GITEA_TOKEN}
      HOMEPAGE_VAR_GRAFANA_USER: ${HOMEPAGE_VAR_GRAFANA_USER}
      HOMEPAGE_VAR_GRAFANA_PASSWORD: ${HOMEPAGE_VAR_GRAFANA_PASSWORD}
      HOMEPAGE_VAR_NPM_USER: ${HOMEPAGE_VAR_NPM_USER}
      HOMEPAGE_VAR_NPM_PASSWORD: ${HOMEPAGE_VAR_NPM_PASSWORD}
    volumes:
      - ./config/homepage:/app/config
      - /var/run/docker.sock:/var/run/docker.sock:ro
    healthcheck:
      test: ["CMD-SHELL", "wget --spider --quiet http://localhost:3000 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    mem_limit: 192m
    mem_reservation: 96m
    cpus: 0.75
    logging:
      <<: *default-logging
      options:
        max-size: "5m"
        max-file: "2"

 

  ############################################################################
  # IMMICH - Self-hosted Google Photos Alternative
  # Mobile app support for Android and iOS/iPad
  ############################################################################

  # immich-db removed; unified under 'postgres'

  immich-server:
    # Always enabled per user preference
    image: ghcr.io/immich-app/immich-server:${IMMICH_TAG:-release}
    container_name: immich-server
    hostname: immich-server
    restart: unless-stopped
    networks:
      - default
      - backend
      - monitoring
    ports:
      - "${HOST_BIND:-0.0.0.0}:2283:3001"
    security_opt:
      - *default-security
    environment:
      <<: *default-env
      DB_HOSTNAME: postgres
      DB_DATABASE_NAME: immich
      DB_USERNAME: immich
      DB_PASSWORD: ${IMMICH_DB_PASSWORD}
      REDIS_HOSTNAME: redis
      REDIS_PORT: 6379
      REDIS_DBINDEX: 2
      # Upload size limits
      UPLOAD_LOCATION: /usr/src/app/upload
      # Machine learning (disabled on Le Potato - too heavy)
      IMMICH_MACHINE_LEARNING_ENABLED: "false"
      # Reverse proxy
      IMMICH_SERVER_URL: http://${HOST_ADDR:-192.168.178.40}:2283
    volumes:
      - /mnt/seconddrive/immich/upload:/usr/src/app/upload
      - /mnt/seconddrive/immich/library:/usr/src/app/library
      - /etc/localtime:/etc/localtime:ro
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
    healthcheck:
      test: ["CMD-SHELL", "wget --spider --quiet http://localhost:3001/api/server-info/ping || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.5
    logging:
      <<: *default-logging
    labels:
      - "homepage.group=Storage & Backup"
      - "homepage.name=Immich"
      - "homepage.icon=immich.png"
      - "homepage.href=http://${HOST_ADDR:-192.168.178.40}:2283"
      - "homepage.description=Photo & video management"

  immich-microservices:
    # Always enabled per user preference
    image: ghcr.io/immich-app/immich-server:${IMMICH_TAG:-release}
    container_name: immich-microservices
    hostname: immich-microservices
    restart: unless-stopped
    networks:
      - default
    security_opt:
      - *default-security
    environment:
      <<: *default-env
      DB_HOSTNAME: postgres
      DB_DATABASE_NAME: immich
      DB_USERNAME: immich
      DB_PASSWORD: ${IMMICH_DB_PASSWORD}
      REDIS_HOSTNAME: redis
      REDIS_PORT: 6379
      REDIS_DBINDEX: 2
      IMMICH_MACHINE_LEARNING_ENABLED: "false"
    volumes:
      - /mnt/seconddrive/immich/upload:/usr/src/app/upload
      - /mnt/seconddrive/immich/library:/usr/src/app/library
      - /etc/localtime:/etc/localtime:ro
    depends_on:
      - postgres
      - redis
    command: ["start.sh", "microservices"]
    mem_limit: 384m
    mem_reservation: 192m
    cpus: 1
    logging:
      <<: *default-logging
