################################################################################
# PotatoStack Main - Optimized for Mini PC (16GB RAM)
# Full-featured self-hosted stack with monitoring, automation, and media
# Target: Mini PC with 16GB RAM, 4+ core CPU, 1GB ethernet
################################################################################

x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"
    compress: "true"

x-common-env: &common-env
  TZ: Europe/Berlin
  PUID: 1000
  PGID: 1000

services:
  ################################################################################
  # Storage Init - Creates required directories on startup
  ################################################################################
  storage-init:
    image: alpine:${ALPINE_TAG:-latest}
    container_name: storage-init
    command: sh /init-storage.sh
    privileged: true
    environment:
      <<: *common-env
      SLSKD_API_KEY: ${SLSKD_API_KEY:-}
      SYNCTHING_API_KEY: ${SYNCTHING_API_KEY:-}
      ARIA2_RPC_SECRET: ${ARIA2_RPC_SECRET:-}
      SNAPSHOT_CRON_SCHEDULE: ${SNAPSHOT_CRON_SCHEDULE:-0 3 * * *}
      SNAPSHOT_PATHS: ${SNAPSHOT_PATHS:-/data}
      SNAPSHOT_LOG_FILE: ${SNAPSHOT_LOG_FILE:-/mnt/storage/stack-snapshot.log}
    volumes:
      - /mnt/storage:/mnt/storage
      - /mnt/cachehdd:/mnt/cachehdd
      - /mnt/ssd/docker-data:/mnt/ssd/docker-data
      - ./scripts/init/init-storage.sh:/init-storage.sh:ro
      - ./scripts/backup/stack-snapshot.sh:/stack-snapshot.sh:ro
      - shared-keys:/keys
    network_mode: none
    restart: "no"

  ################################################################################
  # CORE DATABASES
  ################################################################################

  # PostgreSQL - Primary database with pgvector for embeddings (SOTA 2025)
  postgres:
    image: pgvector/pgvector:${POSTGRES_TAG:-pg16}
    container_name: postgres
    logging: *default-logging
    shm_size: 1gb
    environment:
      <<: *common-env
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
      POSTGRES_MULTIPLE_DATABASES: ${POSTGRES_DATABASES:-gitea,immich,healthchecks,atuin,miniflux,grafana,bitmagnet}
    entrypoint: sh /postgres-wrapper.sh
    command:
      - "postgres"
      - "-c"
      - "shared_buffers=512MB"
      - "-c"
      - "effective_cache_size=768MB"
      - "-c"
      - "work_mem=16MB"
      - "-c"
      - "maintenance_work_mem=512MB"
      - "-c"
      - "max_connections=200"
      - "-c"
      - "checkpoint_completion_target=0.9"
      - "-c"
      - "wal_buffers=16MB"
      - "-c"
      - "default_statistics_target=100"
      - "-c"
      - "random_page_cost=1.1"
      - "-c"
      - "effective_io_concurrency=200"
      - "-c"
      - "max_worker_processes=8"
      - "-c"
      - "max_parallel_workers=4"
      - "-c"
      - "max_parallel_workers_per_gather=2"
    ports:
      - "${HOST_BIND:-127.0.0.1}:5432:5432"
    volumes:
      - /mnt/ssd/docker-data/postgres:/var/lib/postgresql/data
      - ./scripts/init/init-postgres-multiple-dbs.sh:/docker-entrypoint-initdb.d/init-postgres-multiple-dbs.sh:ro
      - ./scripts/init/postgres-wrapper.sh:/postgres-wrapper.sh:ro
    tmpfs:
      - /tmp:exec,mode=1777
      - /var/run/postgresql:exec,mode=1777
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: "1.5"
          memory: 1536M
        reservations:
          cpus: "0.5"
          memory: 1024M
    labels:
      - "potatostack.alerts=critical"
      - "wud.trigger.docker.update=false"

  # PgBouncer - PostgreSQL connection pooling (SOTA 2025)
  pgbouncer:
    image: edoburu/pgbouncer:${PGBOUNCER_TAG:-latest}
    container_name: pgbouncer
    logging: *default-logging
    environment:
      DATABASE_URL: postgres://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/postgres
      POOL_MODE: transaction
      DEFAULT_POOL_SIZE: 50
      MAX_CLIENT_CONN: 200
      MAX_DB_CONNECTIONS: 100
    networks:
      - potatostack
    depends_on:
      - postgres
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "pg_isready", "-h", "127.0.0.1", "-p", "5432"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 64M
    labels:
      - "potatostack.alerts=critical"

  # MongoDB - Document database (SOTA 2025)
  mongo:
    image: mongo:${MONGO_TAG:-7-jammy}
    container_name: mongo
    logging: *default-logging
    command:
      - "mongod"
      - "--wiredTigerCacheSizeGB=0.5"
      - "--wiredTigerJournalCompressor=snappy"
      - "--bind_ip_all"
    environment:
      <<: *common-env
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_ROOT_PASSWORD}
    volumes:
      - /mnt/ssd/docker-data/mongo:/data/db
      - /mnt/ssd/docker-data/mongo-config:/data/configdb
    tmpfs:
      - /tmp:size=64M # Prevents stale socket file on restart
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 512M
        reservations:
          cpus: "0.5"
          memory: 512M
    labels:
      - "potatostack.alerts=critical"
      - "wud.trigger.docker.update=false"

  # Redis Shared Cache - SOTA 2025 consolidated (N8n, Gitea, Immich, Paperless, Sentry)
  redis-cache:
    image: redis:${REDIS_TAG:-7-alpine}
    container_name: redis-cache
    logging: *default-logging
    command: redis-server --maxmemory 384mb --maxmemory-policy allkeys-lfu --databases 16 --activedefrag yes --lazyfree-lazy-eviction yes --lazyfree-lazy-expire yes --lazyfree-lazy-server-del yes --save 60 1000 --appendonly yes --appendfsync everysec
    volumes:
      - /mnt/ssd/docker-data/redis-cache:/data
    tmpfs:
      - /tmp:exec,mode=1777
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 512M
    labels:
      - "potatostack.alerts=critical"
      - "wud.trigger.docker.update=false"

  ################################################################################
  # SECURITY & INTRUSION PREVENTION
  ################################################################################

  # CrowdSec - Modern IPS/IDS with community threat intelligence (SOTA 2025)
  crowdsec:
    image: crowdsecurity/crowdsec:${CROWDSEC_TAG:-latest}
    container_name: crowdsec
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:6060:6060" # Prometheus metrics
    environment:
      <<: *common-env
      COLLECTIONS: "crowdsecurity/http-cve crowdsecurity/whitelist-good-actors crowdsecurity/nginx crowdsecurity/linux"
      GID: "1000"
      METRICS_ENABLED: "true"
    volumes:
      - /mnt/ssd/docker-data/crowdsec-db:/var/lib/crowdsec/data/
      - /mnt/ssd/docker-data/crowdsec-config:/etc/crowdsec/
      - /var/log:/var/log:ro
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:8080/health || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 128M
    labels:
      - "wud.trigger.docker.update=false"

  ################################################################################
  # AUTHENTICATION & SECURITY
  ################################################################################

  # Authentik - SSO and 2FA provider (DISABLED)
  # authentik-server:
  #   image: ghcr.io/goauthentik/server:${AUTHENTIK_TAG:-latest}
  #   container_name: authentik-server
  #   logging: *default-logging
  #   command: server
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:9000:9000"
  #     - "${HOST_BIND:-127.0.0.1}:9443:9443"
  #   environment:
  #     <<: *common-env
  #     AUTHENTIK_POSTGRESQL__HOST: postgres
  #     AUTHENTIK_POSTGRESQL__USER: postgres
  #     AUTHENTIK_POSTGRESQL__NAME: authentik
  #     AUTHENTIK_POSTGRESQL__PASSWORD: ${POSTGRES_SUPER_PASSWORD}
  #     AUTHENTIK_REDIS__HOST: redis-cache
  #     AUTHENTIK_SECRET_KEY: ${AUTHENTIK_SECRET_KEY}
  #     AUTHENTIK_ERROR_REPORTING__ENABLED: "false"
  #   volumes:
  #     - authentik-media:/media
  #     - authentik-custom-templates:/templates
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - postgres
  #     - redis-cache
  #   restart: unless-stopped
  #   healthcheck:
  #     test:
  #       [
  #         "CMD",
  #         "python3",
  #         "-c",
  #         "import requests; requests.get('http://127.0.0.1:9000/-/health/live/')",
  #       ]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 60s
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "2.0"
  #         memory: 512M
  #   labels:

  # authentik-worker:
  #   image: ghcr.io/goauthentik/server:${AUTHENTIK_TAG:-latest}
  #   container_name: authentik-worker
  #   logging: *default-logging
  #   command: worker
  #   environment:
  #     <<: *common-env
  #     AUTHENTIK_POSTGRESQL__HOST: postgres
  #     AUTHENTIK_POSTGRESQL__USER: postgres
  #     AUTHENTIK_POSTGRESQL__NAME: authentik
  #     AUTHENTIK_POSTGRESQL__PASSWORD: ${POSTGRES_SUPER_PASSWORD}
  #     AUTHENTIK_REDIS__HOST: redis-cache
  #     AUTHENTIK_SECRET_KEY: ${AUTHENTIK_SECRET_KEY}
  #   volumes:
  #     - authentik-media:/media
  #     - authentik-certs:/certs
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - postgres
  #     - redis-cache
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "1.0"
  #         memory: 512M

  # Vaultwarden - Password manager and 2FA aggregator
  vaultwarden:
    image: vaultwarden/server:${VAULTWARDEN_TAG:-latest}
    container_name: vaultwarden
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:8888:80"
      - "${HOST_BIND:-127.0.0.1}:8090:80"
      - "${HOST_BIND:-127.0.0.1}:3012:3012"
    environment:
      <<: *common-env
      DOMAIN: https://vault.${HOST_DOMAIN:-local.domain}
      ROCKET_PORT: 80
      WEBSOCKET_ENABLED: "true"
      WEBSOCKET_PORT: 3012
      SIGNUPS_ALLOWED: ${VAULTWARDEN_SIGNUPS_ALLOWED:-false}
      INVITATIONS_ALLOWED: ${VAULTWARDEN_INVITATIONS_ALLOWED:-true}
      ADMIN_TOKEN: ${VAULTWARDEN_ADMIN_TOKEN}
      DATABASE_URL: /data/db.sqlite3
      ICON_CACHE_TTL: 2592000
      LOG_LEVEL: warn
    volumes:
      - /mnt/ssd/docker-data/vaultwarden:/data
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://127.0.0.1:80/alive || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M

  ################################################################################
  # DNS & AD BLOCKING
  ################################################################################

  # AdGuard Home - DNS-level ad blocking with encrypted DNS (SOTA 2025)
  # DISABLED: Commented out per user request
  # adguardhome:
  #   image: adguard/adguardhome:${ADGUARD_TAG:-latest}
  #   container_name: adguardhome
  #   logging: *default-logging
  #   ports:
  #     - "53:53/tcp"
  #     - "53:53/udp"
  #     - "${HOST_BIND:-127.0.0.1}:3053:3000"
  #     - "${HOST_BIND:-127.0.0.1}:8053:80"
  #   environment:
  #     <<: *common-env
  #   volumes:
  #     - adguard-work:/opt/adguardhome/work
  #     - adguard-conf:/opt/adguardhome/conf
  #   networks:
  #     - potatostack
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:80/ || exit 1"]
  #     interval: 60s
  #     timeout: 10s
  #     retries: 3
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.25"
  #         memory: 128M
  #       reservations:
  #         cpus: "0.1"
  #         memory: 128M
  #   labels:

  ################################################################################
  # VPN & NETWORKING
  ################################################################################

  # Gluetun - VPN client with killswitch
  gluetun:
    image: qmcgaw/gluetun:${GLUETUN_TAG:-latest}
    container_name: gluetun
    logging: *default-logging
    cap_add:
      - NET_ADMIN
    devices:
      - /dev/net/tun:/dev/net/tun
    extra_hosts:
      - "host.docker.internal:host-gateway"
    ports:
      - "${HOST_BIND:-127.0.0.1}:8008:8008" # Gluetun control (moved from 8000)
      # - "${HOST_BIND:-127.0.0.1}:8787:8787" # Bookshelf (ebook manager) - DISABLED
      - "${HOST_BIND:-127.0.0.1}:8282:8282" # qBittorrent WebUI
      - "${HOST_BIND:-127.0.0.1}:51413:51413" # qBittorrent peer
      - "${HOST_BIND:-127.0.0.1}:51413:51413/udp"
      - "${HOST_BIND:-127.0.0.1}:6800:6800" # aria2 RPC
      - "${HOST_BIND:-127.0.0.1}:6888:6888" # aria2 BT peer
      - "${HOST_BIND:-127.0.0.1}:6888:6888/udp"
      - "${HOST_BIND:-127.0.0.1}:2234:2234" # slskd WebUI
      - "${HOST_BIND:-127.0.0.1}:50000:50000" # slskd peer
      - "${HOST_BIND:-127.0.0.1}:8097:8080" # SpotiFLAC
      # - "${HOST_BIND:-127.0.0.1}:8945:8945" # Pinchflat
      - "${HOST_BIND:-127.0.0.1}:8076:8000" # pyLoad WebUI (internal 8000)
      - "${HOST_BIND:-127.0.0.1}:9666:9666" # pyLoad Click'n'Load
      - "${HOST_BIND:-127.0.0.1}:9900:9900" # Stash
      - "${HOST_BIND:-127.0.0.1}:6500:6500" # rdt-client WebUI
      - "${HOST_BIND:-127.0.0.1}:3333:3333" # Bitmagnet WebUI
      - "${HOST_BIND:-127.0.0.1}:3334:3334" # Bitmagnet DHT
      - "${HOST_BIND:-127.0.0.1}:3334:3334/udp"
    environment:
      <<: *common-env
      VPN_SERVICE_PROVIDER: ${VPN_PROVIDER:-surfshark}
      VPN_TYPE: ${VPN_TYPE:-wireguard}
      WIREGUARD_PRIVATE_KEY: ${WIREGUARD_PRIVATE_KEY}
      WIREGUARD_ADDRESSES: ${WIREGUARD_ADDRESSES}
      SERVER_COUNTRIES: ${VPN_COUNTRY:-Germany}
      LOG_LEVEL: ${VPN_LOG_LEVEL:-info}
      FIREWALL_OUTBOUND_SUBNETS: ${LAN_NETWORK:-192.168.178.0/24},172.16.0.0/12
      FIREWALL_VPN_INPUT_PORTS: ${VPN_INPUT_PORTS:-51413,50000,6888,3333,3334}
      FIREWALL: "on"
      DNS_ADDRESS: ${VPN_DNS:-1.1.1.1}
      DOT: "off"
      HTTPPROXY: "off"
      SHADOWSOCKS: "off"
      HTTP_CONTROL_SERVER_ADDRESS: :8008 # Changed from 8000 to avoid pyload conflict
      HTTP_CONTROL_SERVER_LOG: "off"
      HTTP_CONTROL_SERVER_AUTH: "off"
      IPV6: "off"
      UPDATER_PERIOD: 24h
      HEALTH_VPN_DURATION_INITIAL: 60s
      HEALTH_VPN_DURATION_ADDITION: 10s
    volumes:
      - gluetun-config:/gluetun
      - ./config/gluetun/post-rules.txt:/iptables/post-rules.txt:ro
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "/gluetun-entrypoint", "healthcheck"]
      interval: 120s
      timeout: 10s
      retries: 3
      start_period: 120s
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
        reservations:
          cpus: "0.1"
          memory: 256M
    labels:
      - "autoheal=true"
      - "potatostack.alerts=critical"
      # aria2 has no web UI - RPC only on port 6800
      - "wud.trigger.docker.update=false"

  # WireGuard - REMOVED (using Gluetun + Tailscale instead)
  # wireguard:
  #   image: lscr.io/linuxserver/wireguard:${WIREGUARD_TAG:-latest}
  #   container_name: wireguard
  #   logging: *default-logging
  #   cap_add:
  #     - NET_ADMIN
  #     - SYS_MODULE
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:${WIREGUARD_SERVERPORT:-51820}:51820/udp"
  #   environment:
  #     <<: *common-env
  #     SERVERURL: ${WIREGUARD_SERVERURL:-auto}
  #     SERVERPORT: ${WIREGUARD_SERVERPORT:-51820}
  #     PEERS: ${WIREGUARD_PEERS:-vps,android,laptop,tablet,raspberry}
  #     PEERDNS: ${WIREGUARD_PEERDNS:-auto}
  #     INTERNAL_SUBNET: ${WIREGUARD_INTERNAL_SUBNET:-10.13.13.0/24}
  #     ALLOWEDIPS: ${WIREGUARD_ALLOWEDIPS:-0.0.0.0/0}
  #     LOG_CONFS: ${WIREGUARD_LOG_CONFS:-true}
  #   volumes:
  #     - /mnt/ssd/docker-data/wireguard:/config
  #     - /lib/modules:/lib/modules:ro
  #   sysctls:
  #     - net.ipv4.conf.all.src_valid_mark=1
  #   networks:
  #     - potatostack
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.25"
  #         memory: 128M

  # Tailscale - Mesh VPN for remote access (SOTA 2025 - easiest option)
  tailscale:
    image: tailscale/tailscale:${TAILSCALE_TAG:-latest}
    container_name: tailscale
    logging: *default-logging
    hostname: potatostack
    environment:
      <<: *common-env
      TS_AUTHKEY: ${TAILSCALE_AUTHKEY}
      TS_STATE_DIR: /var/lib/tailscale
      TS_USERSPACE: "false"
    volumes:
      - tailscale-data:/var/lib/tailscale
      - /dev/net/tun:/dev/net/tun
      - tailscale-https-marker:/https-marker
    cap_add:
      - NET_ADMIN
      - SYS_MODULE
    network_mode: host
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", 'tailscale status --json 2>/dev/null | grep -q ''"Self"'' || exit 1']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 384M
        reservations:
          cpus: "0.25"
          memory: 256M

  # Tailscale HTTPS Setup - Wraps HTTP ports with Tailscale TLS certificates
  tailscale-https-setup:
    image: alpine:${ALPINE_TAG:-latest}
    container_name: tailscale-https-setup
    logging: *default-logging
    entrypoint: ["/bin/sh", "/init.sh"]
    environment:
      <<: *common-env
      TAILSCALE_CONTAINER: tailscale
      # Homer (7575), WUD (3000), home-assistant (8123), flaresolverr (8191) added; removed: karakeep (9091), homarr
      TAILSCALE_SERVE_PORTS: "7575,8088,3000,3001,3002,8086,8096,5055,8989,7878,8686,9696,6767,13378,4533,9900,8282,8076,2234,8097,8008,2283,8090,8095,8384,3004,3006,8888,8093,5006,9090,3100,9093,10903,10902,8087,6060,8001,8788,8060,8889,8081,3010,3013,8180,9925,5984,9898,8123,8191,6500,3333, 5656"
      # Special port mappings (external:internal)
      TAILSCALE_SERVE_SPECIAL: ""
      TAILSCALE_MARKER_FILE: /https-marker/setup-complete
    volumes:
      - ./scripts/init/tailscale-serve-https.sh:/init.sh:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - tailscale-https-marker:/https-marker
    network_mode: host
    depends_on:
      tailscale:
        condition: service_healthy
    restart: "no"
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M

  # Tailscale HTTPS Monitor - Periodically re-applies HTTPS serve rules
  tailscale-https-monitor:
    image: alpine:${ALPINE_TAG:-latest}
    container_name: tailscale-https-monitor
    logging: *default-logging
    entrypoint: ["/bin/sh", "/init.sh"]
    environment:
      <<: *common-env
      TAILSCALE_CONTAINER: tailscale
      # Homer (7575), WUD (3000), home-assistant (8123), flaresolverr (8191) added; removed: karakeep (9091), homarr
      TAILSCALE_SERVE_PORTS: 7575,8088,3000,3001,3002,8086,8096,5055,8989,7878,8686,9696,6767,13378,4533,9900,8282,8076,2234,8097,8008,2283,8090,8095,8384,3004,3006,8888,8093,5006,9090,3100,9093,10903,10902,8087,6060,8001,8788,8060,8889,8081,3010,3013,8180,9925,5984,9898,8123,8191,6500,3333, 5656"
      # Special port mappings (external:internal)
      TAILSCALE_SERVE_SPECIAL: ""
      TAILSCALE_SERVE_LOOP: "true"
      TAILSCALE_SERVE_INTERVAL: "300"
      TAILSCALE_MARKER_FILE: /https-marker/monitor-alive
    volumes:
      - ./scripts/init/tailscale-serve-https.sh:/init.sh:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - tailscale-https-marker:/https-marker
    network_mode: host
    depends_on:
      tailscale:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M

  ################################################################################
  # CLOUD STORAGE & FILE SYNC
  ################################################################################

  # # Nextcloud AIO - DISABLED
  # nextcloud-aio:
  #   image: nextcloud/all-in-one:${NEXTCLOUD_AIO_TAG:-latest}
  #   container_name: nextcloud-aio-mastercontainer
  #   logging: *default-logging
  #   init: true
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:8080:8080"
  #   environment:
  #     PUID: 1000
  #     PGID: 1000
  #     APACHE_PORT: 11000
  #     APACHE_IP_BINDING: 0.0.0.0
  #     NEXTCLOUD_DATADIR: /mnt/storage/nextcloud/data
  #     NEXTCLOUD_MOUNT: /mnt/storage/nextcloud
  #     NEXTCLOUD_UPLOAD_LIMIT: 10G
  #     NEXTCLOUD_MAX_TIME: 3600
  #     NEXTCLOUD_MEMORY_LIMIT: 512M
  #     SKIP_DOMAIN_VALIDATION: "true"
  #   dns:
  #     - 100.100.100.100
  #     - 1.1.1.1
  #     - 8.8.8.8
  #   volumes:
  #     - nextcloud_aio_mastercontainer:/mnt/docker-aio-config
  #     - /var/run/docker.sock:/var/run/docker.sock:ro
  #     - /mnt/storage/nextcloud:/mnt/storage/nextcloud:rw
  #   networks:
  #     - potatostack
  #   depends_on:
  #     storage-init:
  #       condition: service_completed_successfully
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "1.0"
  #         memory: 128M
  #   labels:

  # Syncthing - P2P file sync
  syncthing:
    image: lscr.io/linuxserver/syncthing:${SYNCTHING_TAG:-latest}
    container_name: syncthing
    logging: *default-logging
    entrypoint: bash /syncthing-init.sh
    ports:
      - "${HOST_BIND:-127.0.0.1}:8384:8384"
      - "${HOST_BIND:-127.0.0.1}:22000:22000/tcp"
      - "${HOST_BIND:-127.0.0.1}:22000:22000/udp"
      - "${HOST_BIND:-127.0.0.1}:21027:21027/udp"
    environment:
      <<: *common-env
      # Disable NAT traversal to prevent NAT-PMP errors in Docker
      STNATPMPENABLED: "false"
      STGLOBALFOLDERS: "false"
    volumes:
      - syncthing-config:/config
      - /mnt/storage/syncthing:/data
      - /mnt/cachehdd/sync/syncthing-versions:/data/.stversions
      - shared-keys:/keys:ro
      - ./scripts/init/syncthing-init.sh:/syncthing-init.sh:ro
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8384/rest/noauth/health"]
      interval: 120s
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 128M
    labels:
      - "wud.trigger.docker.update=true"

  # Filebrowser - Web-based file manager with full RW access
  filebrowser:
    image: filebrowser/filebrowser:${FILEBROWSER_TAG:-latest}
    container_name: filebrowser
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:8086:80"
    environment:
      <<: *common-env
    entrypoint: sh /filebrowser-init.sh
    volumes:
      - /mnt/storage:/srv/storage
      - /mnt/cachehdd:/srv/cachehdd
      - /mnt/ssd/docker-data:/srv/docker-data
      - /mnt/ssd/docker-data/filebrowser:/config
      - ./scripts/init/filebrowser-init.sh:/filebrowser-init.sh:ro
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:80/health || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 64M
    labels:
      - "wud.trigger.docker.update=true"

  # Filestash - Advanced web file manager with multi-protocol support (SOTA 2025)
  # Supports: Local, SFTP, FTP, S3, WebDAV, Git, LDAP, Dropbox, Google Drive
  # Features: Image/video/music previews, office doc viewer, text editor
  # Docs: https://www.filestash.app/docs/
  filestash:
    image: machines/filestash:${FILESTASH_TAG:-latest}
    container_name: filestash
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:8095:8334"
    environment:
      <<: *common-env
      # Disable cloud integrations
      GDRIVE_CLIENT_ID: ""
      DROPBOX_CLIENT_ID: ""
    # Hardware acceleration for video playback
    devices:
      - /dev/dri/renderD128:/dev/dri/renderD128
      - /dev/dri/card0:/dev/dri/card0
    group_add:
      - "109"
    volumes:
      # Full storage access (same as filebrowser)
      - /mnt/storage:/mnt/storage
      - /mnt/cachehdd:/mnt/cachehdd
      - /mnt/ssd/docker-data:/mnt/docker-data:ro
      # Config persistence
      - /mnt/ssd/docker-data/filestash:/app/data/state
      # Plugins directory
      - /mnt/ssd/docker-data/filestash/plugins:/app/data/state/plugins
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://127.0.0.1:8334/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.1"
          memory: 128M

    ################################################################################
    # KNOWLEDGE MANAGEMENT
    ################################################################################

  # Obsidian LiveSync - CouchDB backend for Obsidian note synchronization
  obsidian-livesync:
    image: oleduc/docker-obsidian-livesync-couchdb:${OBSIDIAN_LIVESYNC_TAG:-latest}
    container_name: obsidian-livesync
    logging: *default-logging
    ports:
      - "127.0.0.1:5984:5984"
    environment:
      <<: *common-env
      COUCHDB_USER: ${COUCHDB_USER:-obsidian}
      COUCHDB_PASSWORD: ${COUCHDB_PASSWORD}
      COUCHDB_DATABASE: ${COUCHDB_DATABASE:-obsidian-vault}
      COUCHDB_CORS_ORIGINS: https://obsidian.${HOST_DOMAIN:-local.domain},app://obsidian.md
    volumes:
      - /mnt/ssd/docker-data/obsidian-couchdb:/opt/couchdb/data
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f -u $$COUCHDB_USER:$$COUCHDB_PASSWORD http://127.0.0.1:5984/_up || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.1"
          memory: 128M

  # Obsidian LiveSync Init - Single-node CouchDB bootstrap
  obsidian-livesync-init:
    image: alpine:${ALPINE_TAG:-latest}
    container_name: obsidian-livesync-init
    logging: *default-logging
    entrypoint:
      - "/bin/sh"
      - "-c"
      - "apk add --no-cache bash curl >/dev/null 2>&1 && /bin/bash /init-obsidian-livesync.sh"
    environment:
      <<: *common-env
      COUCHDB_HOST: "obsidian-livesync"
      COUCHDB_PORT: "5984"
      COUCHDB_USER: ${COUCHDB_USER:-obsidian}
      COUCHDB_PASSWORD: ${COUCHDB_PASSWORD}
      COUCHDB_DATABASE: ${COUCHDB_DATABASE:-obsidian-vault}
    volumes:
      - ./scripts/init/obsidian-livesync-init.sh:/init-obsidian-livesync.sh:ro
    networks:
      - potatostack
    depends_on:
      - obsidian-livesync
    restart: "no"

    ################################################################################
    # RSS & NEWS AGGREGATION
    ################################################################################

  # Miniflux - Minimalist RSS reader with Postgres backend (SOTA 2025)
  miniflux:
    image: miniflux/miniflux:${MINIFLUX_TAG:-latest}
    container_name: miniflux
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:8093:8080"
    environment:
      <<: *common-env
      DATABASE_URL: postgres://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/miniflux?sslmode=disable
      RUN_MIGRATIONS: "1"
      CREATE_ADMIN: "1"
      ADMIN_USERNAME: ${MINIFLUX_ADMIN_USER:-admin}
      ADMIN_PASSWORD: ${MINIFLUX_ADMIN_PASSWORD}
      BASE_URL: https://rss.${HOST_DOMAIN:-local.domain}
      POLLING_FREQUENCY: 60
      BATCH_SIZE: 100
      WORKER_POOL_SIZE: 5
      METRICS_COLLECTOR: "1"
      METRICS_ALLOWED_NETWORKS: 172.22.0.0/16
      CLEANUP_FREQUENCY_HOURS: "1"
      CLEANUP_ARCHIVE_READ_DAYS: "7"
      CLEANUP_ARCHIVE_UNREAD_DAYS: "30"
    networks:
      - potatostack
    depends_on:
      - postgres
      - pgbouncer
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "/usr/bin/miniflux", "-healthcheck", "auto"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 256M
        reservations:
          cpus: "0.1"
          memory: 128M
    labels:
      - "wud.trigger.docker.update=true"

  # Miniflux webhook to ntfy bridge
  miniflux-webhook:
    image: python:3.12-alpine
    container_name: miniflux-webhook
    logging: *default-logging
    command: ["python", "/app/miniflux-webhook.py"]
    environment:
      <<: *common-env
      MINIFLUX_NTFY_PORT: 8083
      NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      NTFY_TOPIC_INFO: ${NTFY_TOPIC_INFO:-potatostack-info}
      NTFY_TOKEN: ${NTFY_TOKEN:-}
    volumes:
      - ./scripts/webhooks/miniflux-webhook.py:/app/miniflux-webhook.py:ro
    networks:
      - potatostack
    depends_on:
      - ntfy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M
        reservations:
          cpus: "0.05"
          memory: 32M

  ################################################################################
  # RECIPE MANAGEMENT
  ################################################################################
  # RECIPE MANAGEMENT
  ################################################################################

  ################################################################################
  # FINANCE
  ################################################################################

  # Actual Budget - Modern budgeting with bank sync (SOTA 2025)
  actual-budget:
    image: actualbudget/actual-server:${ACTUAL_TAG:-latest}
    container_name: actual-budget
    logging: *default-logging
    # NOTE: Access via Traefik HTTPS (https://budget.danielhomelab.local) for SharedArrayBuffer support
    # Direct port access won't work as browsers require COOP/COEP headers only sent via secure context
    ports:
      - "${HOST_BIND:-127.0.0.1}:5006:5006"
    environment:
      <<: *common-env
    volumes:
      - /mnt/ssd/docker-data/actual-budget:/data
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "timeout 5 bash -c 'cat < /dev/null > /dev/tcp/127.0.0.1/5006' || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 128M
      # Use shared-array-chain for SharedArrayBuffer support (COOP/COEP headers)

  ################################################################################
  # MEDIA MANAGEMENT - *ARR STACK
  ################################################################################

  # Prowlarr - Indexer manager (DISABLED)
  # prowlarr:
  #   image: lscr.io/linuxserver/prowlarr:${PROWLARR_TAG:-latest}
  #   container_name: prowlarr
  #   logging: *default-logging
  #   network_mode: "service:gluetun"
  #   environment:
  #     <<: *common-env
  #   volumes:
  #     - prowlarr-config:/config
  #   depends_on:
  #     gluetun:
  #       condition: service_healthy
  #       restart: true
  #     storage-init:
  #       condition: service_completed_successfully
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -f http://127.0.0.1:9696/ || exit 1"]
  #     interval: 60s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 30s
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.25"
  #         memory: 512M
  #       reservations:
  #         cpus: "0.1"
  #         memory: 256M
  #   labels:
  #     - "autoheal=true"

  # FlareSolverr - Cloudflare bypass for Prowlarr (DISABLED)
  # flaresolverr:
  #   image: ghcr.io/flaresolverr/flaresolverr:${FLARESOLVERR_TAG:-latest}
  #   container_name: flaresolverr
  #   logging: *default-logging
  #   network_mode: "service:gluetun"
  #   environment:
  #     <<: *common-env
  #     LOG_LEVEL: info
  #     LOG_HTML: "false"
  #     CAPTCHA_SOLVER: none
  #   depends_on:
  #     gluetun:
  #       condition: service_healthy
  #       restart: true
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -f http://127.0.0.1:8191/health || exit 1"]
  #     interval: 60s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 30s
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.5"
  #         memory: 512M
  #       reservations:
  #         cpus: "0.1"
  #         memory: 256M
  #   labels:
  #     - "autoheal=true"

  # Sonarr - TV show management (DISABLED)
  # sonarr:
  #   image: lscr.io/linuxserver/sonarr:${SONARR_TAG:-latest}
  #   container_name: sonarr
  #   logging: *default-logging
  #   network_mode: "service:gluetun"
  #   environment:
  #     <<: *common-env
  #   volumes:
  #     - sonarr-config:/config
  #     - /mnt/storage/media/tv:/tv
  #     - /mnt/storage/downloads:/downloads
  #     - /mnt/storage/downloads/incomplete/sonarr:/downloads/incomplete
  #   depends_on:
  #     gluetun:
  #       condition: service_healthy
  #       restart: true
  #     storage-init:
  #       condition: service_completed_successfully
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -sf http://127.0.0.1:8989/ping || exit 1"]
  #     interval: 60s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 30s
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "1.0"
  #         memory: 256M
  #       reservations:
  #         cpus: "0.25"
  #         memory: 128M
  #   labels:
  #     - "autoheal=true"

  # Radarr - Movie management (DISABLED)
  # radarr:
  #   image: lscr.io/linuxserver/radarr:${RADARR_TAG:-latest}
  #   container_name: radarr
  #   logging: *default-logging
  #   network_mode: "service:gluetun"
  #   environment:
  #     <<: *common-env
  #   volumes:
  #     - radarr-config:/config
  #     - /mnt/storage/media/movies:/movies
  #     - /mnt/storage/downloads:/downloads
  #     - /mnt/storage/downloads/incomplete/radarr:/downloads/incomplete
  #   depends_on:
  #     gluetun:
  #       condition: service_healthy
  #       restart: true
  #     storage-init:
  #       condition: service_completed_successfully
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -f http://127.0.0.1:7878/ || exit 1"]
  #     interval: 60s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 30s
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "1.0"
  #         memory: 256M
  #       reservations:
  #         cpus: "0.25"
  #         memory: 128M
  #   labels:
  #     - "autoheal=true"

  # Lidarr - Music management (DISABLED)
  # lidarr:
  #   image: lscr.io/linuxserver/lidarr:${LIDARR_TAG:-latest}
  #   container_name: lidarr
  #   logging: *default-logging
  #   network_mode: "service:gluetun"
  #   environment:
  #     <<: *common-env
  #   volumes:
  #     - lidarr-config:/config
  #     - /mnt/storage/media/music:/music
  #     - /mnt/storage/downloads:/downloads
  #     - /mnt/storage/downloads/incomplete/lidarr:/downloads/incomplete
  #   depends_on:
  #     gluetun:
  #       condition: service_healthy
  #       restart: true
  #     storage-init:
  #       condition: service_completed_successfully
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -f http://127.0.0.1:8686/ || exit 1"]
  #     interval: 60s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 30s
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "1.0"
  #         memory: 256M
  #       reservations:
  #         cpus: "0.25"
  #         memory: 128M
  #   labels:
  #     - "autoheal=true"

  # Bookshelf - Ebook management (Readarr revival with working metadata) (DISABLED)
  # bookshelf:
  #   image: ghcr.io/pennydreadful/bookshelf:${BOOKSHELF_TAG:-hardcover}
  #   container_name: bookshelf
  #   logging: *default-logging
  #   network_mode: "service:gluetun"
  #   environment:
  #     <<: *common-env
  #   volumes:
  #     - bookshelf-config:/config
  #     - /mnt/storage/media/books:/books
  #     - /mnt/storage/downloads:/downloads
  #     - /mnt/storage/downloads/incomplete/bookshelf:/downloads/incomplete
  #   depends_on:
  #     gluetun:
  #       condition: service_healthy
  #       restart: true
  #     storage-init:
  #       condition: service_completed_successfully
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:8787/ping || exit 1"]
  #     interval: 60s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 60s
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "2.0"
  #         memory: 1536M
  #       reservations:
  #         cpus: "0.5"
  #         memory: 512M
  #   labels:
  #     - "autoheal=true"

  # Bazarr - Subtitle management (DISABLED)
  # bazarr:
  #   image: lscr.io/linuxserver/bazarr:${BAZARR_TAG:-latest}
  #   container_name: bazarr
  #   logging: *default-logging
  #   network_mode: "service:gluetun"
  #   environment:
  #     <<: *common-env
  #   volumes:
  #     - bazarr-config:/config
  #     - /mnt/storage/media/movies:/movies
  #     - /mnt/storage/media/tv:/tv
  #   tmpfs:
  #     - /tmp:exec,mode=1777,size=500m
  #   depends_on:
  #     gluetun:
  #       condition: service_healthy
  #       restart: true
  #     storage-init:
  #       condition: service_completed_successfully
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -f http://127.0.0.1:6767/ || exit 1"]
  #     interval: 60s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 30s
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.25"
  #         memory: 384M
  #       reservations:
  #         cpus: "0.1"
  #         memory: 192M
  #   labels:
  #     - "autoheal=true"

  # Maintainerr - Rules-based media library cleanup (DISABLED)
  # maintainerr:
  #   image: jorenn92/maintainerr:${MAINTAINERR_TAG:-latest}
  #   container_name: maintainerr
  #   logging: *default-logging
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:6246:6246"
  #   environment:
  #     <<: *common-env
  #   volumes:
  #     - maintainerr-data:/opt/data
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - sonarr
  #     - radarr
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD-SHELL", "wget -q --spider http://127.0.0.1:6246/api/health || exit 1"]
  #     interval: 60s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 30s
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.25"
  #         memory: 256M
  #       reservations:
  #         cpus: "0.1"
  #         memory: 128M

  # Recyclarr - TRaSH Guides quality profile sync (DISABLED)
  # recyclarr:
  #   image: ghcr.io/recyclarr/recyclarr:${RECYCLARR_TAG:-latest}
  #   container_name: recyclarr
  #   logging: *default-logging
  #   user: 1000:1000
  #   environment:
  #     <<: *common-env
  #     RECYCLARR_CREATE_CONFIG: "true"
  #   volumes:
  #     - /mnt/ssd/docker-data/recyclarr:/config
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - sonarr
  #     - radarr
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.25"
  #         memory: 128M
  #       reservations:
  #         cpus: "0.05"
  #         memory: 64M
  ################################################################################
  # Unpackerr - Extracts archived downloads (standalone mode - no arr dependency)
  ################################################################################
  unpackerr:
    image: ghcr.io/hotio/unpackerr:${UNPACKERR_TAG:-latest}
    container_name: unpackerr
    logging: *default-logging
    environment:
      <<: *common-env
      UMASK: "002"
      
      # Global Settings - Lightweight, low I/O, crash-safe
      UN_INTERVAL: "2m"
      UN_START_DELAY: "5m"
      UN_RETRY_DELAY: "5m"
      UN_MAX_RETRIES: "3"
      UN_PARALLEL: "1"
      UN_FILE_MODE: "0644"
      UN_DIR_MODE: "0755"
      UN_LOG_LEVEL: "INFO"
      
      # Web UI - No password (LAN access only)
      UN_WEBSERVER_METRICS: "false"
      UN_WEBSERVER_LISTEN_ADDR: "0.0.0.0:5656"
      
      # Folder Watchers - All download folders (incomplete folders excluded)
      # qBittorrent
      UN_FOLDER_0_PATH: "/downloads/torrents"
      UN_FOLDER_0_DELETE_ORIGINAL: "true"
      UN_FOLDER_0_DELETE_AFTER: "5m"
      UN_FOLDER_0_MOVE_BACK: "true"
      UN_FOLDER_0_DELETE_FILES: "false"
      UN_FOLDER_0_EXTRACT_ISOS: "false"
      
      # Aria2
      UN_FOLDER_1_PATH: "/downloads/aria2"
      UN_FOLDER_1_DELETE_ORIGINAL: "true"
      UN_FOLDER_1_DELETE_AFTER: "5m"
      UN_FOLDER_1_MOVE_BACK: "true"
      UN_FOLDER_1_DELETE_FILES: "false"
      UN_FOLDER_1_EXTRACT_ISOS: "false"
      
      # Slskd
      UN_FOLDER_2_PATH: "/downloads/slskd"
      UN_FOLDER_2_DELETE_ORIGINAL: "true"
      UN_FOLDER_2_DELETE_AFTER: "5m"
      UN_FOLDER_2_MOVE_BACK: "true"
      UN_FOLDER_2_DELETE_FILES: "false"
      UN_FOLDER_2_EXTRACT_ISOS: "false"
      
      # Pyload
      UN_FOLDER_3_PATH: "/downloads/pyload"
      UN_FOLDER_3_DELETE_ORIGINAL: "true"
      UN_FOLDER_3_DELETE_AFTER: "5m"
      UN_FOLDER_3_MOVE_BACK: "true"
      UN_FOLDER_3_DELETE_FILES: "false"
      UN_FOLDER_3_EXTRACT_ISOS: "false"
      
      # RDT Client
      UN_FOLDER_4_PATH: "/downloads/rdt-client"
      UN_FOLDER_4_DELETE_ORIGINAL: "true"
      UN_FOLDER_4_DELETE_AFTER: "5m"
      UN_FOLDER_4_MOVE_BACK: "true"
      UN_FOLDER_4_DELETE_FILES: "false"
      UN_FOLDER_4_EXTRACT_ISOS: "false"
      
      # TDL (Telegram)
      UN_FOLDER_5_PATH: "/downloads/telegram"
      UN_FOLDER_5_DELETE_ORIGINAL: "true"
      UN_FOLDER_5_DELETE_AFTER: "5m"
      UN_FOLDER_5_MOVE_BACK: "true"
      UN_FOLDER_5_DELETE_FILES: "false"
      UN_FOLDER_5_EXTRACT_ISOS: "false"
      
      # NTFY notification hook
      NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
      NTFY_TOKEN: ${NTFY_TOKEN:-}
      
      # Command Hook for NTFY notifications
      UN_CMDHOOK_0_NAME: "ntfy"
      UN_CMDHOOK_0_COMMAND: "/hooks/post-extract.sh"
      UN_CMDHOOK_0_SHELL: "true"
      UN_CMDHOOK_0_SILENT: "false"
      UN_CMDHOOK_0_EVENTS_0: "2"
      UN_CMDHOOK_0_EVENTS_1: "3"
      UN_CMDHOOK_0_EVENTS_2: "4"
      UN_CMDHOOK_0_EVENTS_3: "5"
      UN_CMDHOOK_0_TIMEOUT: "10s"
    
    volumes:
      - unpackerr-config:/config
      - /mnt/storage/downloads:/downloads
      - /home/daniel/potatostack/scripts/hooks/unpackerr-post-extract.sh:/hooks/post-extract.sh:ro
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:5656/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.1"
          memory: 64M
    labels:
      - "autoheal=true"
      - "wud.trigger.docker.update=true"
  ################################################################################
  # Notifiarr - Unified notifications for *arr stack (DISABLED)
  ################################################################################
  # notifiarr:
  #   image: golift/notifiarr:${NOTIFIARR_TAG:-latest}
  #   container_name: notifiarr
  #   logging: *default-logging
  #   hostname: potatostack
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:5454:5454"
  #   environment:
  #     <<: *common-env
  #     DN_API_KEY: ${NOTIFIARR_API_KEY:-}
  #     DN_UI_PASSWORD: ${NOTIFIARR_UI_PASSWORD:-}
  #     # Sonarr
  #     DN_SONARR_0_URL: http://gluetun:8989
  #     DN_SONARR_0_API_KEY: ${SONARR_API_KEY:-}
  #     # Radarr
  #     DN_RADARR_0_URL: http://gluetun:7878
  #     DN_RADARR_0_API_KEY: ${RADARR_API_KEY:-}
  #     # Lidarr
  #     DN_LIDARR_0_URL: http://gluetun:8686
  #     DN_LIDARR_0_API_KEY: ${LIDARR_API_KEY:-}
  #     # Prowlarr
  #     DN_PROWLARR_0_URL: http://gluetun:9696
  #     DN_PROWLARR_0_API_KEY: ${PROWLARR_API_KEY:-}
  #   volumes:
  #     - /mnt/ssd/docker-data/notifiarr:/config
  #     - /var/run/utmp:/var/run/utmp:ro
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - sonarr
  #     - radarr
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD", "wget", "-q", "--spider", "http://127.0.0.1:5454/"]
  #     interval: 60s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 30s
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.25"
  #         memory: 192M
  #       reservations:
  #         cpus: "0.05"
  #         memory: 64M

  # Exportarr - Prometheus metrics exporter for *arr stack (DISABLED)
  # exportarr:
  #   image: ghcr.io/onedr0p/exportarr:${EXPORTARR_TAG:-latest}
  #   container_name: exportarr-sonarr
  #   logging: *default-logging
  #   command: ["sonarr"]
  #   environment:
  #     PORT: "9707"
  #     URL: http://gluetun:8989
  #     APIKEY: ${SONARR_API_KEY:-}
  #     ENABLE_ADDITIONAL_METRICS: "true"
  #     ENABLE_UNKNOWN_QUEUE_ITEMS: "true"
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:9707:9707"
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - sonarr
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.1"
  #         memory: 64M

  # exportarr-radarr:
  #   image: ghcr.io/onedr0p/exportarr:${EXPORTARR_TAG:-latest}
  #   container_name: exportarr-radarr
  #   logging: *default-logging
  #   command: ["radarr"]
  #   environment:
  #     PORT: "9708"
  #     URL: http://gluetun:7878
  #     APIKEY: ${RADARR_API_KEY:-}
  #     ENABLE_ADDITIONAL_METRICS: "true"
  #     ENABLE_UNKNOWN_QUEUE_ITEMS: "true"
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:9708:9708"
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - radarr
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.1"
  #         memory: 64M

  # exportarr-lidarr:
  #   image: ghcr.io/onedr0p/exportarr:${EXPORTARR_TAG:-latest}
  #   container_name: exportarr-lidarr
  #   logging: *default-logging
  #   command: ["lidarr"]
  #   environment:
  #     PORT: "9709"
  #     URL: http://gluetun:8686
  #     APIKEY: ${LIDARR_API_KEY:-}
  #     ENABLE_ADDITIONAL_METRICS: "true"
  #     ENABLE_UNKNOWN_QUEUE_ITEMS: "true"
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:9709:9709"
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - lidarr
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.1"
  #         memory: 64M

  # exportarr-prowlarr:
  #   image: ghcr.io/onedr0p/exportarr:${EXPORTARR_TAG:-latest}
  #   container_name: exportarr-prowlarr
  #   logging: *default-logging
  #   command: ["prowlarr"]
  #   environment:
  #     PORT: "9710"
  #     URL: http://gluetun:9696
  #     APIKEY: ${PROWLARR_API_KEY:-}
  #     ENABLE_ADDITIONAL_METRICS: "true"
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:9710:9710"
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - prowlarr
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.1"
  #         memory: 64M
  ################################################################################
  # Notifiarr - Unified notifications for *arr stack (DISABLED)
  ################################################################################
  # notifiarr:
  #   image: golift/notifiarr:${NOTIFIARR_TAG:-latest}
  #   container_name: notifiarr
  #   logging: *default-logging
  #   hostname: potatostack
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:5454:5454"
  #   environment:
  #     <<: *common-env
  #     DN_API_KEY: ${NOTIFIARR_API_KEY:-}
  #     DN_UI_PASSWORD: ${NOTIFIARR_UI_PASSWORD:-}
  #     # Sonarr
  #     DN_SONARR_0_URL: http://gluetun:8989
  #     DN_SONARR_0_API_KEY: ${SONARR_API_KEY:-}
  #     # Radarr
  #     DN_RADARR_0_URL: http://gluetun:7878
  #     DN_RADARR_0_API_KEY: ${RADARR_API_KEY:-}
  #     # Lidarr
  #     DN_LIDARR_0_URL: http://gluetun:8686
  #     DN_LIDARR_0_API_KEY: ${LIDARR_API_KEY:-}
  #     # Prowlarr
  #     DN_PROWLARR_0_URL: http://gluetun:9696
  #     DN_PROWLARR_0_API_KEY: ${PROWLARR_API_KEY:-}
  #   volumes:
  #     - /mnt/ssd/docker-data/notifiarr:/config
  #     - /var/run/utmp:/var/run/utmp:ro
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - sonarr
  #     - radarr
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD", "wget", "-q", "--spider", "http://127.0.0.1:5454/"]
  #     interval: 60s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 30s
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.25"
  #         memory: 192M
  #       reservations:
  #         cpus: "0.05"
  #         memory: 64M

  # Exportarr - Prometheus metrics exporter for *arr stack (DISABLED)
  # exportarr:
  #   image: ghcr.io/onedr0p/exportarr:${EXPORTARR_TAG:-latest}
  #   container_name: exportarr-sonarr
  #   logging: *default-logging
  #   command: ["sonarr"]
  #   environment:
  #     PORT: "9707"
  #     URL: http://gluetun:8989
  #     APIKEY: ${SONARR_API_KEY:-}
  #     ENABLE_ADDITIONAL_METRICS: "true"
  #     ENABLE_UNKNOWN_QUEUE_ITEMS: "true"
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:9707:9707"
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - sonarr
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.1"
  #         memory: 64M

  # exportarr-radarr:
  #   image: ghcr.io/onedr0p/exportarr:${EXPORTARR_TAG:-latest}
  #   container_name: exportarr-radarr
  #   logging: *default-logging
  #   command: ["radarr"]
  #   environment:
  #     PORT: "9708"
  #     URL: http://gluetun:7878
  #     APIKEY: ${RADARR_API_KEY:-}
  #     ENABLE_ADDITIONAL_METRICS: "true"
  #     ENABLE_UNKNOWN_QUEUE_ITEMS: "true"
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:9708:9708"
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - radarr
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.1"
  #         memory: 64M

  # exportarr-lidarr:
  #   image: ghcr.io/onedr0p/exportarr:${EXPORTARR_TAG:-latest}
  #   container_name: exportarr-lidarr
  #   logging: *default-logging
  #   command: ["lidarr"]
  #   environment:
  #     PORT: "9709"
  #     URL: http://gluetun:8686
  #     APIKEY: ${LIDARR_API_KEY:-}
  #     ENABLE_ADDITIONAL_METRICS: "true"
  #     ENABLE_UNKNOWN_QUEUE_ITEMS: "true"
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:9709:9709"
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - lidarr
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.1"
  #         memory: 64M

  # exportarr-prowlarr:
  #   image: ghcr.io/onedr0p/exportarr:${EXPORTARR_TAG:-latest}
  #   container_name: exportarr-prowlarr
  #   logging: *default-logging
  #   command: ["prowlarr"]
  #   environment:
  #     PORT: "9710"
  #     URL: http://gluetun:9696
  #     APIKEY: ${PROWLARR_API_KEY:-}
  #     ENABLE_ADDITIONAL_METRICS: "true"
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:9710:9710"
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - prowlarr
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.1"
  #         memory: 64M

  ################################################################################
  # MEDIA SERVERS & REQUEST MANAGEMENT
  ################################################################################

  # Jellyfin - Media server (SOTA 2025 with HW acceleration support)
  jellyfin:
    image: lscr.io/linuxserver/jellyfin:${JELLYFIN_TAG:-latest}
    container_name: jellyfin
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:8096:8096"
      - "${HOST_BIND:-127.0.0.1}:8920:8920"
      - "${HOST_BIND:-127.0.0.1}:7359:7359/udp"
      - "${HOST_BIND:-127.0.0.1}:1900:1900/udp"
    environment:
      <<: *common-env
    # Uncomment group_add and devices for Intel/AMD hardware acceleration
    # Get render group ID with: getent group render | cut -d: -f3
    # group_add:
    #   - "107"  # render group - adjust to match your system
    # devices:
    #   - /dev/dri/renderD128:/dev/dri/renderD128
    devices:
      - /dev/dri/renderD128:/dev/dri/renderD128
      - /dev/dri/card0:/dev/dri/card0
    group_add:
      - "109"
    volumes:
      - jellyfin-config:/config
      - /mnt/storage/media/tv:/data/tvshows
      - /mnt/storage/media/movies:/data/movies
      - /mnt/storage/media/music:/data/music
      - /mnt/storage/media/audiobooks:/data/audiobooks
      - /mnt/cachehdd/media/jellyfin:/cache
    tmpfs:
      - /tmp:exec,mode=1777,size=500m
      - /transcode:exec,mode=1777,size=2g
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:8096/health || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: "2.5"
          memory: 2G
        reservations:
          cpus: "1.0"
          memory: 1G
    labels:
      - "wud.trigger.docker.update=true"

  # Jellyfin webhook to ntfy bridge
  jellyfin-webhook:
    image: python:3.12-alpine
    container_name: jellyfin-webhook
    logging: *default-logging
    command: ["python", "/app/jellyfin-webhook.py"]
    environment:
      <<: *common-env
      JELLYFIN_NTFY_PORT: 8081
      NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
      NTFY_TOKEN: ${NTFY_TOKEN:-}
    volumes:
      - ./scripts/webhooks/jellyfin-webhook.py:/app/jellyfin-webhook.py:ro
    networks:
      - potatostack
    depends_on:
      - ntfy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M
        reservations:
          cpus: "0.05"
          memory: 32M

  # Jellyseerr - Media request management for Jellyfin
  jellyseerr:
    image: fallenbagel/jellyseerr:${JELLYSEERR_TAG:-latest}
    container_name: jellyseerr
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:5055:5055"
    environment:
      <<: *common-env
    volumes:
      - /mnt/ssd/docker-data/jellyseerr:/app/config
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 256M
        reservations:
          cpus: "0.1"
          memory: 128M

  # Jellyseerr webhook to ntfy bridge
  jellyseerr-webhook:
    image: python:3.12-alpine
    container_name: jellyseerr-webhook
    logging: *default-logging
    command: ["python", "/app/jellyseerr-webhook.py"]
    environment:
      <<: *common-env
      JELLYSEERR_NTFY_PORT: 8082
      NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
      NTFY_TOKEN: ${NTFY_TOKEN:-}
    volumes:
      - ./scripts/webhooks/jellyseerr-webhook.py:/app/jellyseerr-webhook.py:ro
    networks:
      - potatostack
    depends_on:
      - ntfy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M
        reservations:
          cpus: "0.05"
          memory: 32M

  # Tdarr - Distributed transcoding system for optimizing media library
  # tdarr:
  #   image: ghcr.io/haveagitgat/tdarr:${TDARR_TAG:-latest}
  #   container_name: tdarr
  #   logging: *default-logging
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:8265:8265"
  #     - "${HOST_BIND:-127.0.0.1}:8266:8266"
  #   environment:
  #     <<: *common-env
  #     serverIP: 0.0.0.0
  #     serverPort: 8266
  #     webUIPort: 8265
  #     internalNode: "true"
  #     inContainer: "true"
  #     ffmpegVersion: "6"
  #     nodeName: potatostack-node
  #   devices:
  #     - /dev/dri/renderD128:/dev/dri/renderD128
  #     - /dev/dri/card0:/dev/dri/card0
  #   group_add:
  #     - "109"
  #   volumes:
  #     - /mnt/ssd/docker-data/tdarr/server:/app/server
  #     - /mnt/ssd/docker-data/tdarr/configs:/app/configs
  #     - /mnt/ssd/docker-data/tdarr/logs:/app/logs
  #     - /mnt/cachehdd/tdarr/transcode_cache:/temp
  #     - /mnt/storage/media/movies:/media/movies
  #     - /mnt/storage/media/tv:/media/tv
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - storage-init
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -f http://127.0.0.1:8265/ || exit 1"]
  #     interval: 60s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 60s
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "2.0"
  #         memory: 2G
  #       reservations:
  #         cpus: "0.5"
  #         memory: 512M
  #   labels:

  # Audiobookshelf - Audiobook and podcast server
  audiobookshelf:
    image: ghcr.io/advplyr/audiobookshelf:${AUDIOBOOKSHELF_TAG:-latest}
    container_name: audiobookshelf
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:13378:80"
    environment:
      <<: *common-env
    volumes:
      - audiobookshelf-config:/config
      - /mnt/cachehdd/media/audiobookshelf/metadata:/metadata
      - /mnt/storage/media/audiobooks:/audiobooks
      - /mnt/storage/media/podcasts:/podcasts
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 512M

  # Navidrome - Music streaming server (Subsonic/Airsonic compatible)
  navidrome:
    image: deluan/navidrome:${NAVIDROME_TAG:-latest}
    container_name: navidrome
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:4533:4533"
    environment:
      <<: *common-env
      ND_SCANSCHEDULE: 1h
      ND_LOGLEVEL: info
      ND_SESSIONTIMEOUT: 24h
      ND_BASEURL: ""
      ND_ENABLETRANSCODINGCONFIG: "true"
      ND_ENABLESHARING: "true"
    volumes:
      - /mnt/ssd/docker-data/navidrome:/data
      - /mnt/storage/media/music:/music:ro
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:4533/ping || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 256M
        reservations:
          cpus: "0.25"
          memory: 128M

  # SpotiFLAC - Spotify to FLAC downloader (Tidal/Qobuz/Amazon sources)
  spotiflac:
    image: neiht/spotiflac:latest@sha256:65de2009536579cda0311b2d9606faa70bfc5bc9962f96f8ad150e5e82b3da23
    container_name: spotiflac
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
      SERVER_PORT: 8080
      SERVER_HOST: 0.0.0.0
      DOWNLOAD_DIR: /downloads
      STATIC_DIR: /app/dist
    volumes:
      - /mnt/storage/downloads:/downloads
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:8080/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 128M
    labels:
      - "potatostack.alerts=critical"
      - "autoheal=true"

  ################################################################################
  # DOWNLOAD CLIENTS (behind VPN)
  ################################################################################

  # qBittorrent - Torrent client
  qbittorrent:
    image: lscr.io/linuxserver/qbittorrent:${QBITTORRENT_TAG:-latest}
    container_name: qbittorrent
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
      WEBUI_PORT: 8282
      QBITTORRENT_USER: ${QBITTORRENT_USER:-daniel}
      QBITTORRENT_PASSWORD: ${QBITTORRENT_PASSWORD:-}
      NTFY_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
      NTFY_TOKEN: ${NTFY_TOKEN:-}
    volumes:
      - qbittorrent-config:/config
      - /mnt/storage/downloads/torrents:/downloads
      - /mnt/storage/downloads/incomplete/qbittorrent:/incomplete
      - /home/daniel/potatostack/scripts/init/qbittorrent-init.sh:/custom-cont-init.d/99-qb-init.sh:ro
      - /home/daniel/potatostack/scripts/hooks/qbittorrent-post-torrent.sh:/hooks/post-torrent.sh:ro
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:8282/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 2G
        reservations:
          cpus: "1.0"
          memory: 1G
    labels:
      - "autoheal=true"

  # SABnzbd - Usenet download client (for Torbox/Eweka usenet) (DISABLED)
  # sabnzbd:
  #   image: lscr.io/linuxserver/sabnzbd:${SABNZBD_TAG:-latest}
  #   container_name: sabnzbd
  #   logging: *default-logging
  #   network_mode: "service:gluetun"
  #   environment:
  #     <<: *common-env
  #   volumes:
  #     - sabnzbd-config:/config
  #     - /mnt/storage/downloads:/downloads
  #     - /mnt/storage/downloads/incomplete/sabnzbd:/incomplete-downloads
  #   depends_on:
  #     gluetun:
  #       condition: service_healthy
  #       restart: true
  #     storage-init:
  #       condition: service_completed_successfully
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -f http://127.0.0.1:8085/api?mode=version || exit 1"]
  #     interval: 60s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 60s
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "1.0"
  #         memory: 512M
  #       reservations:
  #         cpus: "0.25"
  #         memory: 256M
  #   labels:
  #     - "autoheal=true"

  # rdt-client - Real-Debrid download client (qBittorrent API emulation)
  rdt-client:
    image: rogerfar/rdtclient:${RDT_CLIENT_TAG:-latest}
    container_name: rdt-client
    logging: *default-logging
    network_mode: "service:gluetun"
    environment:
      <<: *common-env
      NTFY_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
      NTFY_TOKEN: ${NTFY_TOKEN:-}
    volumes:
      - rdt-client-data:/data/db
      - /mnt/storage/downloads/rdt-client:/downloads
      - /mnt/storage/media:/mnt/storage/media
      - /home/daniel/potatostack/scripts/init/rdt-client-init.sh:/etc/cont-init.d/99-rdt-init.sh:ro
      - /home/daniel/potatostack/scripts/hooks/rdt-client-post-download.sh:/hooks/post-download.sh:ro
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:8000/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
        reservations:
          cpus: "0.1"
          memory: 256M
    labels:
      - "autoheal=true"

  # Aria2 - High-performance download client (RPC on port 6800)
  aria2:
    image: p3terx/aria2-pro:${ARIA2_TAG:-latest}
    container_name: aria2
    logging: *default-logging
    network_mode: "service:gluetun"
    entrypoint: sh /aria2-init.sh
    environment:
      <<: *common-env
      RPC_PORT: 6800
      LISTEN_PORT: 6888
      DISK_CACHE: 64M
      IPV6_MODE: "false"
    volumes:
      - aria2-config:/config
      - /mnt/storage/downloads/aria2:/downloads
      - /mnt/storage/downloads/incomplete/aria2:/incomplete
      - /home/daniel/potatostack/scripts/init/aria2-init.sh:/aria2-init.sh:ro
      - shared-keys:/keys:ro
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pgrep -x aria2c || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 128M
    labels:
      - "autoheal=true"

  # AriaNg - REMOVED
  # ariang:
  #   image: p3terx/ariang:${ARIANG_TAG:-latest}
  #   container_name: ariang
  #   logging: *default-logging
  #   network_mode: "service:gluetun"
  #   command: --port 80
  #   depends_on:
  #     - aria2
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.1"
  #         memory: 64M

  # slskd - Soulseek P2P client for music sharing (SOTA 2025)
  slskd:
    image: ghcr.io/slskd/slskd:${SLSKD_TAG:-latest}
    container_name: slskd
    logging: *default-logging
    network_mode: "service:gluetun"
    entrypoint: sh /init.sh
    environment:
      <<: *common-env
      SLSKD_HTTP_PORT: 2234
      SLSKD_SLSK_LISTEN_PORT: 50000
      SLSKD_USERNAME: ${SLSKD_USER:-admin}
      SLSKD_PASSWORD: ${SLSKD_PASSWORD}
      SLSKD_SOULSEEK_USERNAME: ${SLSKD_SOULSEEK_USERNAME}
      SLSKD_SOULSEEK_PASSWORD: ${SLSKD_SOULSEEK_PASSWORD}
      SLSKD_METRICS: ${SLSKD_METRICS_ENABLED:-true}
      SLSKD_API_KEY: ${SLSKD_API_KEY}

      SLSKD_UPLOAD_SLOTS: ${SLSKD_UPLOAD_SLOTS:-4}
      SLSKD_UPLOAD_SPEED_LIMIT: ${SLSKD_UPLOAD_SPEED_LIMIT:-25}
      SLSKD_DOWNLOAD_SLOTS: ${SLSKD_DOWNLOAD_SLOTS:-500}
      SLSKD_DOWNLOAD_SPEED_LIMIT: ${SLSKD_DOWNLOAD_SPEED_LIMIT:-1000}
      SLSKD_QUEUE_FILES: ${SLSKD_QUEUE_FILES:-500}
      SLSKD_QUEUE_MEGABYTES: ${SLSKD_QUEUE_MEGABYTES:-5000}
      SLSKD_GROUP_UPLOAD_SLOTS: ${SLSKD_GROUP_UPLOAD_SLOTS:-4}
      SLSKD_GROUP_UPLOAD_SPEED_LIMIT: ${SLSKD_GROUP_UPLOAD_SPEED_LIMIT:-25}
      SLSKD_GROUP_QUEUE_FILES: ${SLSKD_GROUP_QUEUE_FILES:-150}
      SLSKD_GROUP_QUEUE_MEGABYTES: ${SLSKD_GROUP_QUEUE_MEGABYTES:-1500}
      SLSKD_LOGGER_DISK: ${SLSKD_LOGGER_DISK:-true}
      SLSKD_LOGGER_NO_COLOR: ${SLSKD_LOGGER_NO_COLOR:-true}
      SLSKD_LOGGER_LOKI: ${SLSKD_LOGGER_LOKI:-null}
      SLSKD_METRICS_ENABLED: ${SLSKD_METRICS_ENABLED:-true}
      SLSKD_METRICS_URL: ${SLSKD_METRICS_URL:-/metrics}
      SLSKD_METRICS_AUTH_DISABLED: ${SLSKD_METRICS_AUTH_DISABLED:-true}
      SLSKD_METRICS_USERNAME: ${SLSKD_METRICS_USERNAME:-slskd}
      SLSKD_METRICS_PASSWORD: ${SLSKD_METRICS_PASSWORD:-slskd}
    volumes:
      - slskd-config:/app
      - /mnt/cachehdd/slskd/logs:/app/logs
      - /mnt/storage/downloads/slskd:/var/slskd/downloads
      - /mnt/storage/downloads/incomplete/slskd:/var/slskd/incomplete
      - /mnt/storage/slskd-shared:/var/slskd/shared
      - shared-keys:/keys:ro
      - /home/daniel/potatostack/scripts/init/slskd-init.sh:/init.sh:ro
      # Share music and audiobook libraries
      - /mnt/storage/media/music:/music:ro
      - /mnt/storage/media/audiobooks:/audiobooks:ro
    tmpfs:
      - /tmp:exec,mode=1777,size=256m
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://127.0.0.1:2234/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: "1.5"
          memory: 512M
        reservations:
          cpus: "0.5"
          memory: 256M
    labels:
      - "autoheal=true"
      - "potatostack.alerts=critical"

  # Soularr - Bridge between Lidarr and Soulseek (slskd) (DISABLED)
  # soularr:
  #   image: mrusse08/soularr:${SOULARR_TAG:-latest}
  #   container_name: soularr
  #   logging: *default-logging
  #   environment:
  #     <<: *common-env
  #     SCRIPT_INTERVAL: 300
  #   volumes:
  #     - /mnt/ssd/docker-data/soularr:/data
  #     - /mnt/storage/downloads/slskd:/downloads/slskd
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - lidarr
  #     - slskd
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.5"
  #         memory: 256M
  #       reservations:
  #         cpus: "0.25"
  #         memory: 128M

  # pyLoad-ng - Web-based download manager (HTTP/FTP/etc)
  # WebUI: 8076 (external) -> 8000 (internal), Click'n'Load: 9666
  pyload:
    image: lscr.io/linuxserver/pyload-ng:latest
    container_name: pyload
    logging: *default-logging
    network_mode: "service:gluetun"
    entrypoint: /pyload-init.sh
    environment:
      <<: *common-env
      PYLOAD_USER: ${PYLOAD_USER:-pyload}
      PYLOAD_PASSWORD: ${PYLOAD_PASSWORD:-}
      PYLOAD_ENABLE_NTFY_HOOKS: ${PYLOAD_ENABLE_NTFY_HOOKS:-true}
      NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
      NTFY_TOKEN: ${NTFY_TOKEN:-}
    volumes:
      - pyload-config:/config
      - /mnt/storage/downloads/pyload:/downloads
      - /mnt/storage/downloads/incomplete/pyload:/incomplete
      - /home/daniel/potatostack/scripts/init/pyload-init.sh:/pyload-init.sh:ro
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:8000/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.1"
          memory: 128M
    labels:
      - "autoheal=true"

  # tdl - Telegram downloader toolkit
  tdl:
    image: iyear/tdl:${TDL_TAG:-latest}
    container_name: tdl
    logging: *default-logging
    network_mode: "service:gluetun"
    entrypoint: ["/bin/sh", "-c", "crond -b && sh /etc/tdl/download-saved.sh >> /downloads/tdl-download.log 2>&1 & tail -f /dev/null"]
    environment:
      <<: *common-env
    volumes:
      - tdl-data:/root/.tdl
      - /mnt/storage/downloads/telegram:/downloads
      - /mnt/storage/media/adult:/adult:ro
      - /mnt/storage/media/adult/telegram:/adult-telegram
      - /home/daniel/potatostack/config/tdl/crontab:/etc/crontabs/root:ro
      - /home/daniel/potatostack/config/tdl/download-saved.sh:/etc/tdl/download-saved.sh:ro
      - /home/daniel/potatostack/config/tdl/blocklist.txt:/etc/tdl/blocklist.txt:ro
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pgrep crond || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M
        reservations:
          cpus: "0.05"
          memory: 32M

  # Bitmagnet - DHT crawler and torrent indexer (SOTA 2026)
  bitmagnet:
    image: ghcr.io/bitmagnet-io/bitmagnet:${BITMAGNET_TAG:-latest}
    container_name: bitmagnet
    logging: *default-logging
    network_mode: "service:gluetun"
    command:
      - worker
      - run
      - --keys=http_server
      - --keys=queue_server
      - --keys=dht_crawler
    environment:
      <<: *common-env
      # Note: Using container IP because network_mode:service:gluetun can't resolve Docker DNS
      # If postgres IP changes, update this or restart postgres first
      POSTGRES_HOST: 172.22.0.15
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
      POSTGRES_DATABASE: bitmagnet
      # Redis cache disabled - not exposed on host
      # REDIS_ADDR: redis-cache:6379
      REDIS_DB: "15"
      # Lightweight crawl settings for mini PC
      DHT_CRAWLER_SCALING_FACTOR: "5"
      DHT_CRAWLER_SAVE_FILES_THRESHOLD: "50"
      DHT_CRAWLER_SAVE_PIECES: "false"
      # TMDB for movie/TV classification (20 req/sec with own key)
      TMDB_ENABLED: "true"
      TMDB_API_KEY: ${TMDB_API_KEY}
      # Logging
      LOG_LEVEL: info
      LOG_JSON: "false"
    volumes:
      - /mnt/cachehdd/bitmagnet:/root/.config/bitmagnet
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      postgres:
        condition: service_healthy
      redis-cache:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://127.0.0.1:3333/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 120s
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 384M
        reservations:
          cpus: "0.25"
          memory: 128M
    labels:
      - "autoheal=true"
      - "wud.trigger.docker.update=true"

  ################################################################################
  # PHOTO MANAGEMENT
  ################################################################################

  # Immich - Photo management and AI tagging
  immich-server:
    image: ghcr.io/immich-app/immich-server:${IMMICH_TAG:-release}
    container_name: immich-server
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:2283:2283"
    environment:
      <<: *common-env
      DB_HOSTNAME: postgres
      DB_USERNAME: postgres
      DB_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
      DB_DATABASE_NAME: immich
      REDIS_HOSTNAME: redis-cache
      REDIS_DBINDEX: 0
      UPLOAD_LOCATION: /usr/src/app/upload
      IMMICH_FFMPEG_ACCEL: qsv
      IMMICH_FFMPEG_PREFERRED_HW_DEVICE: /dev/dri/renderD128
    devices:
      - /dev/dri/renderD128:/dev/dri/renderD128
    volumes:
      - /mnt/storage/photos:/usr/src/app/upload
      - /etc/localtime:/etc/localtime:ro
    networks:
      - potatostack
    depends_on:
      - postgres
      - redis-cache
      - storage-init
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:2283/api/server/ping || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 2G
        reservations:
          cpus: "0.5"
          memory: 512M
    labels:
      - "wud.trigger.docker.update=false"

  immich-machine-learning:
    image: ghcr.io/immich-app/immich-machine-learning:${IMMICH_TAG:-release}
    container_name: immich-ml
    logging: *default-logging
    environment:
      <<: *common-env
    volumes:
      - /mnt/cachehdd/media/immich-ml:/cache
    networks:
      - potatostack
    depends_on:
      immich-server:
        condition: service_healthy
    tmpfs:
      - /tmp:exec,mode=1777,size=500m
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", 'python3 -c ''import requests; requests.get("http://127.0.0.1:3003/ping")''']
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 120s
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 2G
        reservations:
          cpus: "0.5"
          memory: 1G
    labels:
      - "wud.trigger.docker.update=false"

  # Immich Log Monitor - Restart on error patterns
  immich-log-monitor:
    image: docker:${DOCKER_TAG:-cli}
    container_name: immich-log-monitor
    logging: *default-logging
    entrypoint: sh /immich-log-monitor.sh
    environment:
      IMMICH_CONTAINER: immich-server
      IMMICH_ML_CONTAINER: immich-ml
      CHECK_INTERVAL: ${IMMICH_LOG_CHECK_INTERVAL:-60}
      RESTART_COOLDOWN: ${IMMICH_RESTART_COOLDOWN:-300}
      REACHABILITY_TIMEOUT: ${IMMICH_REACHABILITY_TIMEOUT:-120}
      REACHABILITY_RETRIES: ${IMMICH_REACHABILITY_RETRIES:-6}
      IMMICH_LOG_PATTERNS: ${IMMICH_LOG_PATTERNS:-redis|Redis|ECONNREFUSED|Connection refused|connect ECONNREFUSED|socket hang up}
      IMMICH_NOTIFY_COOLDOWN: ${IMMICH_NOTIFY_COOLDOWN:-300}
      NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
      NTFY_TOKEN: ${NTFY_TOKEN:-}
      NTFY_DEFAULT_TAGS: ${NTFY_DEFAULT_TAGS:-potatostack,monitor}
      NTFY_DEFAULT_PRIORITY: ${NTFY_DEFAULT_PRIORITY:-default}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./scripts/monitor/immich-log-monitor.sh:/immich-log-monitor.sh:ro
      - ./scripts/monitor/notify.sh:/notify.sh:ro
    depends_on:
      - immich-server
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M
        reservations:
          cpus: "0.05"
          memory: 32M

  ################################################################################
  # MONITORING & OBSERVABILITY
  ################################################################################

  # Prometheus - Metrics collection and time-series database (SOTA 2025)
  prometheus:
    image: prom/prometheus:${PROMETHEUS_TAG:-latest}
    container_name: prometheus
    logging: *default-logging
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=7d"
      - "--storage.tsdb.retention.size=5GB"
      - "--storage.tsdb.min-block-duration=2h"
      - "--storage.tsdb.max-block-duration=2h"
      - "--web.console.libraries=/usr/share/prometheus/console_libraries"
      - "--web.console.templates=/usr/share/prometheus/consoles"
      - "--web.enable-lifecycle"
      - "--web.enable-admin-api"
    ports:
      - "${HOST_BIND:-127.0.0.1}:9090:9090"
    environment:
      <<: *common-env
    volumes:
      - ./config/prometheus:/etc/prometheus:ro
      - /mnt/cachehdd/observability/prometheus:/prometheus
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://127.0.0.1:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.25"
          memory: 128M
    labels:
      - "potatostack.alerts=critical"
      - "wud.trigger.docker.update=false"

  # Grafana - Metrics visualization and dashboards (SOTA 2025)
  grafana:
    image: grafana/grafana:${GRAFANA_TAG:-latest}
    container_name: grafana
    logging: *default-logging
    user: "472:472"
    ports:
      - "${HOST_BIND:-127.0.0.1}:3002:3000"
    environment:
      <<: *common-env
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_PLUGINS_PREINSTALL: grafana-clock-panel
      GF_SERVER_ROOT_URL: https://grafana.${HOST_DOMAIN:-local.domain}
      GF_DATABASE_TYPE: postgres
      GF_DATABASE_HOST: postgres:5432
      GF_DATABASE_NAME: grafana
      GF_DATABASE_USER: postgres
      GF_DATABASE_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
      GF_DATABASE_SSL_MODE: disable
    volumes:
      - /mnt/ssd/docker-data/grafana:/var/lib/grafana
      - ./config/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./config/grafana/dashboards:/var/lib/grafana/dashboards:ro
    networks:
      - potatostack
    depends_on:
      - postgres
      - pgbouncer
      - prometheus
      - loki
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:3000/api/health || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.25"
          memory: 128M
    labels:
      - "wud.trigger.docker.update=false"

  # Node Exporter - Host metrics for Prometheus (SOTA 2025)
  node-exporter:
    image: prom/node-exporter:${NODE_EXPORTER_TAG:-latest}
    container_name: node-exporter
    logging: *default-logging
    command:
      - "--path.rootfs=/host"
      - "--path.procfs=/host/proc"
      - "--path.sysfs=/host/sys"
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|run|var/lib/docker/.+)($|/)"
    volumes:
      - /:/host:ro,rslave
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 64M

  # Postgres Exporter - PostgreSQL metrics
  postgres-exporter:
    image: quay.io/prometheuscommunity/postgres-exporter:${POSTGRES_EXPORTER_TAG:-latest}
    container_name: postgres-exporter
    logging: *default-logging
    environment:
      DATA_SOURCE_NAME: postgresql://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/postgres?sslmode=disable
    networks:
      - potatostack
    depends_on:
      - postgres
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M
        reservations:
          cpus: "0.05"
          memory: 32M

  # Redis Exporter - Redis metrics
  redis-exporter:
    image: oliver006/redis_exporter:${REDIS_EXPORTER_TAG:-latest}
    container_name: redis-exporter
    logging: *default-logging
    environment:
      REDIS_ADDR: redis-cache:6379
    networks:
      - potatostack
    depends_on:
      - redis-cache
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M
        reservations:
          cpus: "0.05"
          memory: 32M

  # MongoDB Exporter - MongoDB metrics
  mongodb-exporter:
    image: percona/mongodb_exporter:${MONGODB_EXPORTER_TAG:-0.43}
    container_name: mongodb-exporter
    logging: *default-logging
    environment:
      MONGODB_URI: mongodb://root:${MONGO_ROOT_PASSWORD}@mongo:27017/admin
    networks:
      - potatostack
    depends_on:
      - mongo
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M
        reservations:
          cpus: "0.05"
          memory: 32M

  # Smartctl Exporter - SMART metrics for disk health alerting
  smartctl-exporter:
    image: prometheuscommunity/smartctl-exporter:${SMARTCTL_EXPORTER_TAG:-latest}
    container_name: smartctl-exporter
    logging: *default-logging
    privileged: true
    group_add:
      - disk
    devices:
      - /dev/sda
      - /dev/sda1
      - /dev/sda2
      - /dev/sda3
      - /dev/sdb
      - /dev/sdb1
      - /dev/sdc
      - /dev/sdc1
    command:
      - "--smartctl.path=/usr/sbin/smartctl"
      - "--smartctl.device=/dev/sda"
      - "--smartctl.device=/dev/sdb"
      - "--smartctl.device=/dev/sdc"
    volumes:
      - /dev:/dev
      - /run/udev:/run/udev:ro
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M
        reservations:
          cpus: "0.05"
          memory: 32M

  # Netdata - DISABLED: Use Prometheus + Grafana instead for monitoring
  # netdata:
  #   image: netdata/netdata:${NETDATA_TAG:-latest}
  #   container_name: netdata
  #   logging: *default-logging
  #   hostname: potatostack-main
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:19999:19999"
  #   cap_add:
  #     - SYS_PTRACE
  #     - SYS_ADMIN
  #   security_opt:
  #     - apparmor:unconfined
  #   environment:
  #     <<: *common-env
  #     NETDATA_CLAIM_TOKEN: ${NETDATA_CLAIM_TOKEN:-}
  #     NETDATA_CLAIM_ROOMS: ${NETDATA_CLAIM_ROOMS:-}
  #     NETDATA_CLAIM_URL: https://app.netdata.cloud
  #     DOCKER_HOST: /var/run/docker.sock
  #   volumes:
  #     - netdata-config:/etc/netdata
  #     - netdata-lib:/var/lib/netdata
  #     - netdata-cache:/var/cache/netdata
  #     - /etc/passwd:/host/etc/passwd:ro
  #     - /etc/group:/host/etc/group:ro
  #     - /proc:/host/proc:ro
  #     - /sys:/host/sys:ro
  #     - /etc/os-release:/host/etc/os-release:ro
  #     - /var/run/docker.sock:/var/run/docker.sock:ro
  #   networks:
  #     - potatostack
  #   restart: unless-stopped
  #   healthcheck:
  #     test:
  #       ["CMD-SHELL", "curl -f http://127.0.0.1:19999/api/v1/info || exit 1"]
  #     interval: 60s
  #     timeout: 10s
  #     retries: 3
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.5"
  #         memory: 384M
  #       reservations:
  #         cpus: "0.1"
  #         memory: 64M
  #   labels:

  # Loki - Log aggregation
  loki:
    image: grafana/loki:${LOKI_TAG:-latest}
    container_name: loki
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:3100:3100"
    command: -config.file=/etc/loki/loki.yml -target=all
    volumes:
      - ./config/loki:/etc/loki
      - /mnt/cachehdd/observability/loki/data:/loki
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 384M
    labels:
      - "wud.trigger.docker.update=false"

  # Grafana Alloy - Unified collector (Promtail replacement)
  alloy:
    image: grafana/alloy:${ALLOY_TAG:-latest}
    container_name: alloy
    logging: *default-logging
    command: run /etc/alloy/config.alloy --storage.path=/var/lib/alloy
    volumes:
      - ./config/alloy:/etc/alloy
      - /var/log:/var/log:ro
      - /mnt/cachehdd/slskd/logs:/slskd-logs:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - alloy-data:/var/lib/alloy
    networks:
      - potatostack
    depends_on:
      - loki
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 192M

  # Parseable - REMOVED (using Loki instead)
  # parseable:
  #   image: parseable/parseable:${PARSEABLE_TAG:-latest}
  #   container_name: parseable
  #   logging: *default-logging
  #   command: ["parseable", "local-store"]
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:8094:8000"
  #   environment:
  #     <<: *common-env
  #     P_USERNAME: ${PARSEABLE_USERNAME:-admin}
  #     P_PASSWORD: ${PARSEABLE_PASSWORD:-admin}
  #     P_ADDR: ${PARSEABLE_ADDR:-0.0.0.0:8000}
  #     P_FS_DIR: ${PARSEABLE_FS_DIR:-/data}
  #   volumes:
  #     - /mnt/ssd/docker-data/parseable:/data
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - storage-init
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.5"
  #         memory: 256M
  #   labels:

  # Scrutiny - HDD SMART monitoring
  scrutiny:
    image: ghcr.io/analogj/scrutiny:${SCRUTINY_TAG:-master-omnibus}
    container_name: scrutiny
    logging: *default-logging
    cap_add:
      - SYS_RAWIO
      - SYS_ADMIN
    ports:
      - "${HOST_BIND:-127.0.0.1}:8087:8080"
    volumes:
      - /run/udev:/run/udev:ro
      - /mnt/ssd/docker-data/scrutiny/config:/opt/scrutiny/config
      - /mnt/cachehdd/observability/scrutiny:/opt/scrutiny/influxdb
    devices:
      - "${SCRUTINY_DEVICE_1:-/dev/sda}"
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 192M

  # Uptime Kuma - Uptime monitoring with Docker container support (DISABLED)
  # uptime-kuma:
  #   image: louislam/uptime-kuma:${UPTIME_KUMA_TAG:-latest}
  #   container_name: uptime-kuma
  #   logging: *default-logging
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:3001:3001"
  #   environment:
  #     <<: *common-env
  #     # Docker socket access via socket-proxy for container monitoring
  #     # NOTE: When creating Docker monitors in UI, set Docker Host to: tcp://socket-proxy:2375
  #     DOCKER_HOST: tcp://socket-proxy:2375
  #   volumes:
  #     - /mnt/ssd/docker-data/uptime-kuma:/app/data
  #     # Mount docker.sock for Docker monitors (socket-proxy env var may not work for all features)
  #     - /var/run/docker.sock:/var/run/docker.sock:ro
  #   group_add:
  #     - "987"  # docker group for socket access
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - socket-proxy
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -f http://127.0.0.1:3001 || exit 1"]
  #     interval: 60s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 30s
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.25"
  #         memory: 192M

  ################################################################################
  # AUTOMATION & WORKFLOWS
  ################################################################################

  # News Pipeline - Fetches RSS + crawls NW.de, extracts full text, serves combined RSS feed
  news-pipeline:
    image: python:3.12-alpine
    container_name: news-pipeline
    logging: *default-logging
    command: ["sh", "-c", "pip install -q pg8000 && python /app/news-pipeline.py"]
    environment:
      <<: *common-env
      INTERVAL_SECONDS: ${INTERVAL_SECONDS:-900}
      MINIFLUX_DB_URL: postgres://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/miniflux
      PURGE_READ_DAYS: "7"
      PURGE_UNREAD_DAYS: "30"
      PURGE_INTERVAL_HOURS: "6"
    volumes:
      - ./scripts/webhooks/news-pipeline.py:/app/news-pipeline.py:ro
    networks:
      - potatostack
    depends_on:
      - article-extractor
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 64M

  # Article Extractor - Headless Chromium + Bypass Paywalls Clean + trafilatura
  article-extractor:
    image: python:3.12-alpine
    container_name: article-extractor
    logging: *default-logging
    entrypoint: ["/bin/sh", "/app/init.sh"]
    environment:
      <<: *common-env
    volumes:
      - ./scripts/webhooks/article-extractor.py:/app/article-extractor.py:ro
      - ./scripts/init/article-extractor-init.sh:/app/init.sh:ro
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 1G

  # Healthchecks - Cron monitoring
  healthchecks:
    image: lscr.io/linuxserver/healthchecks:${HEALTHCHECKS_TAG:-latest}
    container_name: healthchecks
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:8001:8000"
    environment:
      <<: *common-env
      SITE_ROOT: https://healthchecks.${HOST_DOMAIN:-local.domain}
      SITE_NAME: PotatoStack Healthchecks
      SUPERUSER_EMAIL: ${HEALTHCHECKS_ADMIN_EMAIL}
      SUPERUSER_PASSWORD: ${HEALTHCHECKS_ADMIN_PASSWORD}
      SECRET_KEY: ${HEALTHCHECKS_SECRET_KEY}
      DB: postgres
      DB_HOST: postgres
      DB_PORT: 5432
      DB_NAME: healthchecks
      DB_USER: postgres
      DB_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
    volumes:
      - /mnt/ssd/docker-data/healthchecks:/config
    networks:
      - potatostack
    depends_on:
      - postgres
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 256M
        reservations:
          cpus: "0.1"
          memory: 128M

  ################################################################################
  # UTILITIES & TOOLS
  ################################################################################

  # Rustypaste - Pastebin
  rustypaste:
    image: orhunp/rustypaste:${RUSTYPASTE_TAG:-latest}
    container_name: rustypaste
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:8788:8000"
    environment:
      <<: *common-env
      CONFIG: /config/config.toml
    volumes:
      - /mnt/storage/rustypaste:/data
      - ./config/rustypaste/config.toml:/config/config.toml:ro
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    healthcheck:
      disable: true
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 64M
        reservations:
          cpus: "0.1"
          memory: 32M
    labels:
      - "autoheal=true"

  # PairDrop - AirDrop alternative (P2P file sharing)
  pairdrop:
    image: lscr.io/linuxserver/pairdrop:${PAIRDROP_TAG:-latest}
    container_name: pairdrop
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:3013:3000"
    environment:
      <<: *common-env
      RATE_LIMIT: ${PAIRDROP_RATE_LIMIT:-false}
      WS_FALLBACK: ${PAIRDROP_WS_FALLBACK:-false}
      RTC_CONFIG: ${PAIRDROP_RTC_CONFIG:-}
      DEBUG_MODE: ${PAIRDROP_DEBUG_MODE:-false}
    volumes:
      - /mnt/ssd/docker-data/pairdrop:/config
      - /mnt/storage/pairdrop:/data
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    healthcheck:
      disable: true
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.1"
          memory: 64M
    labels:
      - "autoheal=true"

  # Stirling-PDF - PDF tools
  # stirling-pdf:
  #   image: frooodle/s-pdf:${STIRLING_PDF_TAG:-latest}
  #   container_name: stirling-pdf
  #   logging: *default-logging
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:8086:8080"
  #   environment:
  #     <<: *common-env
  #     DOCKER_ENABLE_SECURITY: "false"
  #     INSTALL_BOOK_AND_ADVANCED_HTML_OPS: "true"
  #     LANGS: de_DE,en_US
  #   volumes:
  #     - stirling-pdf-data:/usr/share/tessdata
  #     - stirling-pdf-configs:/configs
  #   networks:
  #     - potatostack
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.75"
  #         memory: 512M
  #       reservations:
  #         cpus: "0.25"
  #         memory: 256M
  #   labels:

  ################################################################################
  # Karakeep - AI-powered bookmark manager (DISABLED - replaced by Homer dashboard)
  ################################################################################

  # karakeep:
  #   image: ghcr.io/karakeep-app/karakeep:${KARAKEEP_TAG:-release}
  #   container_name: karakeep
  #   logging: *default-logging
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:9091:3000"
  #   environment:
  #     <<: *common-env
  #     DATA_DIR: /data
  #     NEXTAUTH_SECRET: ${KARAKEEP_NEXTAUTH_SECRET}
  #     MEILI_MASTER_KEY: ${KARAKEEP_MEILI_MASTER_KEY}
  #     MEILI_ADDR: http://karakeep-meilisearch:7700
  #     BROWSER_WEB_URL: http://karakeep-chrome:9222
  #     OPENAI_API_KEY: ${KARAKEEP_OPENAI_API_KEY:-}
  #     OLLAMA_BASE_URL: ${KARAKEEP_OLLAMA_BASE_URL:-}
  #     DISABLE_SIGNUPS: ${KARAKEEP_DISABLE_SIGNUPS:-false}
  #   volumes:
  #     - /mnt/ssd/docker-data/karakeep:/data
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - karakeep-meilisearch
  #     - karakeep-chrome
  #     - postgres
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.5"
  #         memory: 768M
  #   labels:
  #     - "wud.trigger.docker.update=true"

  # karakeep-chrome:
  #   image: gcr.io/zenika-hub/alpine-chrome:${KARAKEEP_CHROME_TAG:-123}
  #   container_name: karakeep-chrome
  #   logging: *default-logging
  #   command:
  #     - --no-sandbox
  #     - --disable-gpu
  #     - --disable-dev-shm-usage
  #     - --remote-debugging-address=0.0.0.0
  #     - --remote-debugging-port=9222
  #     - --hide-scrollbars
  #   networks:
  #     - potatostack
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.25"
  #         memory: 256M
  #   labels:
  #     - "wud.trigger.docker.update=true"

  # karakeep-meilisearch:
  #   image: getmeili/meilisearch:${KARAKEEP_MEILISEARCH_TAG:-v1.13.3}
  #   container_name: karakeep-meilisearch
  #   logging: *default-logging
  #   environment:
  #     MEILI_MASTER_KEY: ${KARAKEEP_MEILI_MASTER_KEY}
  #     MEILI_NO_ANALYTICS: "true"
  #   volumes:
  #     - /mnt/ssd/docker-data/karakeep-meilisearch:/meili_data
  #   networks:
  #     - potatostack
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.25"
  #         memory: 256M
  #   labels:
  #     - "wud.trigger.docker.update=true"

  # Code-server - DISABLED: VS Code in browser
  # code-server:
  #   image: lscr.io/linuxserver/code-server:${CODE_SERVER_TAG:-latest}
  #   container_name: code-server
  #   logging: *default-logging
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:8444:8443"
  #   environment:
  #     <<: *common-env
  #     PASSWORD: ${CODE_SERVER_PASSWORD}
  #     SUDO_PASSWORD: ${CODE_SERVER_SUDO_PASSWORD}
  #   volumes:
  #     - code-server-config:/config
  #     - /mnt/storage/projects:/config/workspace
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - storage-init
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "2.0"
  #         memory: 384M
  #   labels:

  # Atuin - Shell history sync
  atuin:
    image: ghcr.io/atuinsh/atuin:${ATUIN_TAG:-latest}
    container_name: atuin
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:8889:8888"
    environment:
      <<: *common-env
      ATUIN_DB_URI: postgres://postgres:${POSTGRES_SUPER_PASSWORD}@postgres/atuin
      # Bind to all interfaces for external access (Tailscale/LAN)
      ATUIN_HOST: "0.0.0.0"
      ATUIN_PORT: "8888"
      ATUIN_OPEN_REGISTRATION: ${ATUIN_OPEN_REGISTRATION:-false}
    command: server start
    volumes:
      - atuin-config:/config
    networks:
      - potatostack
    depends_on:
      - postgres
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
    labels:
      - "wud.trigger.docker.update=true"

  # IT-Tools - REMOVED: User requested removal
  # it-tools:
  #   image: corentinth/it-tools:${IT_TOOLS_TAG:-latest}
  #   container_name: it-tools
  #   logging: *default-logging
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:8091:80"
  #   environment:
  #     <<: *common-env
  #   networks:
  #     - potatostack
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:80/ || exit 1"]
  #     interval: 60s
  #     timeout: 10s
  #     retries: 3
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.25"
  #         memory: 128M
  #   labels:

  # DuckDB - Ad-hoc analytics container
  duckdb:
    image: alpine:latest
    container_name: duckdb
    logging: *default-logging
    entrypoint: ["/bin/sh", "-c", "tail -f /dev/null"]
    volumes:
      - /mnt/cachehdd/duckdb:/data
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M

  # Paperless-ngx - DISABLED (OOM issues with Celery workers)
  # Uncomment to re-enable document management
  # paperless-ngx:
  #   image: ghcr.io/paperless-ngx/paperless-ngx:${PAPERLESS_TAG:-latest}
  #   container_name: paperless-ngx
  #   logging: *default-logging
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:8092:8000"
  #   environment:
  #     <<: *common-env
  #     PAPERLESS_REDIS: redis://redis-cache:6379/1
  #     PAPERLESS_DBHOST: postgres
  #     PAPERLESS_DBNAME: paperless
  #     PAPERLESS_DBUSER: postgres
  #     PAPERLESS_DBPASS: ${POSTGRES_SUPER_PASSWORD}
  #     PAPERLESS_URL: https://docs.${HOST_DOMAIN:-local.domain}
  #     PAPERLESS_SECRET_KEY: ${PAPERLESS_SECRET_KEY}
  #     PAPERLESS_ADMIN_USER: ${PAPERLESS_ADMIN_USER:-admin}
  #     PAPERLESS_ADMIN_PASSWORD: ${PAPERLESS_ADMIN_PASSWORD}
  #     PAPERLESS_OCR_LANGUAGE: deu+eng
  #     PAPERLESS_TIME_ZONE: Europe/Berlin
  #   volumes:
  #     - /mnt/ssd/docker-data/paperless-data:/usr/src/paperless/data
  #     - /mnt/storage/paperless/media:/usr/src/paperless/media
  #     - /mnt/storage/paperless/consume:/usr/src/paperless/consume
  #     - /mnt/storage/paperless/export:/usr/src/paperless/export
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - postgres
  #     - redis-cache
  #     - storage-init
  #   tmpfs:
  #     - /tmp:exec,mode=1777,size=1g
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -f http://127.0.0.1:8000/ || exit 1"]
  #     interval: 60s
  #     timeout: 10s
  #     retries: 3
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "2.0"
  #         memory: 2G
  #       reservations:
  #         cpus: "0.5"
  #         memory: 768M
  #   labels:

  # Pingvin Share - Secure file sharing (SOTA 2025)
  ################################################################################
  # DEVELOPMENT & GIT
  ################################################################################

  # Gitea - Git hosting
  gitea:
    image: gitea/gitea:${GITEA_TAG:-latest}
    container_name: gitea
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:3004:3000"
      - "${HOST_BIND:-127.0.0.1}:${GITEA_SSH_PORT:-2223}:22"
    environment:
      <<: *common-env
      GITEA__database__DB_TYPE: postgres
      GITEA__database__HOST: postgres:5432
      GITEA__database__NAME: gitea
      GITEA__database__USER: postgres
      GITEA__database__PASSWD: ${POSTGRES_SUPER_PASSWORD}
      GITEA__server__DOMAIN: git.${HOST_DOMAIN:-local.domain}
      # For HTTPS via Traefik - access via https://git.danielhomelab.local
      GITEA__server__ROOT_URL: https://git.${HOST_DOMAIN:-local.domain}/
      GITEA__server__SSH_DOMAIN: git.${HOST_DOMAIN:-local.domain}
      GITEA__server__SSH_PORT: ${GITEA_SSH_PORT:-2223}
      # Allow HTTP access for local development (WebAuthn requires HTTPS for full functionality)
      GITEA__server__LOCAL_ROOT_URL: http://localhost:3000/
      GITEA__security__INSTALL_LOCK: "true"
      GITEA__cache__ENABLED: "true"
      GITEA__cache__ADAPTER: redis
      GITEA__cache__HOST: network=tcp,addr=redis-cache:6379,db=0
      GITEA__session__PROVIDER: redis
      GITEA__session__PROVIDER_CONFIG: network=tcp,addr=redis-cache:6379,db=2
      GITEA__queue__TYPE: redis
      GITEA__queue__CONN_STR: redis://redis-cache:6379/1
    volumes:
      - /mnt/ssd/docker-data/gitea:/data
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    networks:
      - potatostack
    dns:
      - 127.0.0.11
      - 1.1.1.1
    depends_on:
      postgres:
        condition: service_healthy
      redis-cache:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 256M
    labels:
      - "wud.trigger.docker.update=false"

  # Gitea Runner - CI/CD for Gitea
  gitea-runner:
    image: gitea/act_runner:${GITEA_RUNNER_TAG:-latest}
    container_name: gitea-runner
    logging: *default-logging
    environment:
      <<: *common-env
      GITEA_INSTANCE_URL: http://gitea:3000
      GITEA_RUNNER_REGISTRATION_TOKEN: ${GITEA_RUNNER_TOKEN}
      GITEA_RUNNER_NAME: docker-runner
    volumes:
      - gitea-runner-data:/data
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - potatostack
    depends_on:
      - gitea
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M

  # Woodpecker CI - Gitea integrated CI/CD (SOTA 2025)
  # woodpecker-server:
  #   image: woodpeckerci/woodpecker-server:${WOODPECKER_TAG:-latest}
  #   container_name: woodpecker-server
  #   logging: *default-logging
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:3006:8000"
  #   environment:
  #     <<: *common-env
  #     WOODPECKER_HOST: https://ci.${HOST_DOMAIN:-local.domain}
  #     WOODPECKER_SERVER_ADDR: :8000
  #     WOODPECKER_GRPC_ADDR: :9000
  #     WOODPECKER_AGENT_SECRET: ${WOODPECKER_AGENT_SECRET}
  #     WOODPECKER_DATABASE_DRIVER: postgres
  #     WOODPECKER_DATABASE_DATASOURCE: postgres://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/woodpecker?sslmode=disable
  #     WOODPECKER_OPEN: "false"
  #     WOODPECKER_ADMIN: ${WOODPECKER_ADMIN}
  #     WOODPECKER_GITEA: "true"
  #     WOODPECKER_GITEA_URL: http://gitea:3000
  #     WOODPECKER_GITEA_CLIENT: ${WOODPECKER_GITEA_CLIENT}
  #     WOODPECKER_GITEA_SECRET: ${WOODPECKER_GITEA_SECRET}
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - postgres
  #     - gitea
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.5"
  #         memory: 256M

  # woodpecker-agent:
  #   image: woodpeckerci/woodpecker-agent:${WOODPECKER_AGENT_TAG:-latest}
  #   container_name: woodpecker-agent
  #   logging: *default-logging
  #   environment:
  #     <<: *common-env
  #     WOODPECKER_SERVER: woodpecker-server:9000
  #     WOODPECKER_AGENT_SECRET: ${WOODPECKER_AGENT_SECRET}
  #     WOODPECKER_BACKEND: docker
  #     WOODPECKER_BACKEND_DOCKER_HOST: unix:///var/run/docker.sock
  #     DOCKER_API_VERSION: "1.44"
  #     WOODPECKER_BACKEND_DOCKER_NETWORK: potatostack
  #     WOODPECKER_MAX_WORKFLOWS: ${WOODPECKER_MAX_WORKFLOWS:-2}
  #     WOODPECKER_GRPC_SECURE: "false"
  #   volumes:
  #     - /var/run/docker.sock:/var/run/docker.sock
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - woodpecker-server
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.5"
  #         memory: 256M

  # Sentry - Error tracking
  ################################################################################
  # AI & SPECIAL APPLICATIONS
  ################################################################################

  # Open WebUI - LLM interface
  # open-webui:
  #   image: ghcr.io/open-webui/open-webui:${OPEN_WEBUI_TAG:-main}
  #   container_name: open-webui
  #   logging: *default-logging
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:3005:8080"
  #   environment:
  #     <<: *common-env
  #     OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
  #     WEBUI_SECRET_KEY: ${OPEN_WEBUI_SECRET_KEY}
  #     DATABASE_URL: postgresql://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/openwebui
  #     VECTOR_DB: pgvector
  #   volumes:
  #     - open-webui-data:/app/backend/data
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - postgres
  #   extra_hosts:
  #     - "host.docker.internal:host-gateway"
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "2.0"
  #         memory: 384M
  #   labels:

  # Pinchflat - YouTube downloader (disabled)
  # pinchflat:
  #   image: ghcr.io/kieraneglin/pinchflat:${PINCHFLAT_TAG:-latest}
  #   container_name: pinchflat
  #   logging: *default-logging
  #   network_mode: "service:gluetun"
  #   environment:
  #     <<: *common-env
  #   volumes:
  #     - pinchflat-config:/config
  #     - /mnt/storage/media/youtube:/downloads
    #     - /mnt/storage/downloads/incomplete/pinchflat:/incomplete
  #   depends_on:
  #     gluetun:
  #       condition: service_healthy
  #       restart: true
  #     storage-init:
  #       condition: service_completed_successfully
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "1.0"
  #         memory: 384M
  #   labels:
  #     - "autoheal=true"

  # Stash - Adult media organizer (behind VPN)
  stash:
    image: stashapp/stash:${STASH_TAG:-latest}
    container_name: stash
    logging: *default-logging
    network_mode: "service:gluetun"
    entrypoint: ["/init/stash-init.sh"]
    environment:
      <<: *common-env
      STASH_PORT: "9900"
      STASH_STASH: /data
      STASH_GENERATED: /generated
      STASH_METADATA: /metadata
      STASH_CACHE: /cache
      STASH_FFMPEG_HARDWARE_ACCELERATION: "true"
      STASH_EXTERNAL_HOST: https://potatostack.tale-iwato.ts.net:9900
    devices:
      - /dev/dri/renderD128:/dev/dri/renderD128
      - /dev/dri/card0:/dev/dri/card0
    group_add:
      - "109"
    volumes:
      - /mnt/ssd/docker-data/stash/config:/root/.stash
      - /mnt/ssd/docker-data/stash/generated:/generated
      - /mnt/ssd/docker-data/stash/metadata:/metadata
      - /mnt/cachehdd/stash/cache:/cache
      - /mnt/storage/media/adult:/data
      - /home/daniel/potatostack/scripts/init/stash-init.sh:/init/stash-init.sh:ro
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://127.0.0.1:9900"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 1024M
        reservations:
          cpus: "0.5"
          memory: 256M
    labels:
      - "autoheal=true"
      - "wud.trigger.docker.update=true"

  ################################################################################
  # DASHBOARD & CONTAINER MANAGEMENT
  ################################################################################

  # Docker Socket Proxy - Restricted Docker API for Homarr (SOTA 2025)
  socket-proxy:
    image: lscr.io/linuxserver/socket-proxy:${SOCKET_PROXY_TAG:-latest}
    container_name: socket-proxy
    logging: *default-logging
    security_opt:
      - no-new-privileges:true
    environment:
      <<: *common-env
      ALLOW_START: 1
      ALLOW_STOP: 1
      ALLOW_RESTARTS: 1
      CONTAINERS: 1
      EVENTS: 1
      INFO: 1
      IMAGES: 1
      NETWORKS: 1
      PING: 1
      POST: 1
      VERSION: 1
      LOG_LEVEL: info
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - potatostack
    restart: unless-stopped
    read_only: true
    tmpfs:
      - /run
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:2375/_ping || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M

  # Homer - Static dashboard (SOTA 2025 - lightweight, fast, YAML-configured)
  homer:
    image: b4bz/homer:${HOMER_TAG:-latest}
    container_name: homer
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:7575:8080"
    environment:
      <<: *common-env
      INIT_ASSETS: "0"
    volumes:
      - ./config/homer:/www/assets:ro
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:8080/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 32M
        reservations:
          cpus: "0.01"
          memory: 16M
    labels:
      - "wud.trigger.docker.update=true"

  # Homarr - DISABLED: Replaced by Homer (lighter weight, no DB dependency)
  # homarr:
  #   image: ghcr.io/homarr-labs/homarr:${HOMARR_TAG:-latest}
  #   container_name: homarr
  #   logging: *default-logging
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:7575:7575"
  #   environment:
  #     <<: *common-env
  #     # Encryption key (generate with: openssl rand -hex 32)
  #     SECRET_ENCRYPTION_KEY: ${HOMARR_SECRET_KEY:-f95cefcebd962e7b56b1cb4e1a166c756cf7b821c91e759cfb7fd2995be4517a}
  #     # Database - PostgreSQL for persistence
  #     DRIVER: nodePostgres
  #     DB_HOST: postgres
  #     DB_PORT: 5432
  #     DB_NAME: homarr
  #     DB_USER: postgres
  #     DB_PASSWORD: ${POSTGRES_SUPER_PASSWORD}
  #     # Docker integration via socket-proxy (v1.x format)
  #     DOCKER_HOSTNAMES: socket-proxy
  #     DOCKER_PORTS: 2375
  #     # Use external Redis (shared redis-cache)
  #     REDIS_IS_EXTERNAL: "true"
  #     REDIS_HOST: redis-cache
  #     REDIS_PORT: 6379
  #     REDIS_DATABASE_INDEX: 7
  #   volumes:
  #     - /mnt/ssd/docker-data/homarr:/appdata
  #   networks:
  #     - potatostack
  #   depends_on:
  #     postgres:
  #       condition: service_healthy
  #     redis-cache:
  #       condition: service_healthy
  #     socket-proxy:
  #       condition: service_started
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:7575/ || exit 1"]
  #     interval: 60s
  #     timeout: 10s
  #     retries: 3
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "1.0"
  #         memory: 512M
  #       reservations:
  #         cpus: "0.25"
  #         memory: 256M
  #   labels:
  #     - "wud.trigger.docker.update=true"

  # Dockge - Modern Docker Compose stack manager (SOTA 2025)
  ################################################################################
  # ENTERPRISE SECURITY & SECRETS MANAGEMENT
  ################################################################################

  # Infisical - Secrets management
  # infisical:
  #   image: infisical/infisical:${INFISICAL_TAG:-latest}
  #   container_name: infisical
  #   logging: *default-logging
  #   user: "0:0"
  #   ports:
  #     - "${HOST_BIND:-127.0.0.1}:8288:8080"
  #   environment:
  #     <<: *common-env
  #     NODE_ENV: production
  #     SITE_URL: https://secrets.${HOST_DOMAIN:-local.domain}
  #     ENCRYPTION_KEY: ${INFISICAL_ENCRYPTION_KEY}
  #     AUTH_SECRET: ${INFISICAL_AUTH_SECRET}
  #     DB_CONNECTION_URI: postgres://postgres:${POSTGRES_SUPER_PASSWORD}@postgres:5432/infisical?sslmode=disable
  #     REDIS_URL: redis://redis-cache:6379
  #   networks:
  #     - potatostack
  #   depends_on:
  #     - postgres
  #     - redis-cache
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.5"
  #         memory: 256M
  #       reservations:
  #         cpus: "0.25"
  #         memory: 128M
  #   labels:

  # fail2ban - REMOVED (using CrowdSec instead)
  # fail2ban:
  #   image: crazymax/fail2ban:${FAIL2BAN_TAG:-latest}
  #   container_name: fail2ban
  #   logging: *default-logging
  #   network_mode: host
  #   cap_add:
  #     - NET_ADMIN
  #     - NET_RAW
  #   environment:
  #     <<: *common-env
  #     F2B_DB_PURGE_AGE: 30d
  #     F2B_LOG_LEVEL: INFO
  #     F2B_IPTABLES_CHAIN: DOCKER-USER
  #   volumes:
  #     - fail2ban-data:/data
  #     - /var/log:/var/log:ro
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "0.25"
  #         memory: 128M
  #       reservations:
  #         cpus: "0.1"
  #         memory: 64M

  # Alertmanager - Alert routing and management
  alertmanager:
    image: prom/alertmanager:${ALERTMANAGER_TAG:-latest}
    container_name: alertmanager
    logging: *default-logging
    command:
      - "--config.file=/etc/alertmanager/config.yml"
      - "--storage.path=/alertmanager"
      - "--web.external-url=https://alerts.${HOST_DOMAIN:-local.domain}"
      - "--cluster.listen-address="
    environment:
      <<: *common-env
    volumes:
      - ./config/alertmanager:/etc/alertmanager:ro
      - alertmanager-data:/alertmanager
    ports:
      - "${HOST_BIND:-127.0.0.1}:9093:9093"
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://127.0.0.1:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 64M
    labels:
      - "potatostack.alerts=critical"

  # Alertmanager ntfy Formatter - Enterprise alert formatting -> ntfy
  alertmanager-ntfy:
    image: python:3.12-alpine
    container_name: alertmanager-ntfy
    logging: *default-logging
    command: ["python", "/app/alertmanager-ntfy.py"]
    environment:
      NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
      NTFY_TOPIC_CRITICAL: ${NTFY_TOPIC_CRITICAL:-potatostack-critical}
      NTFY_TOPIC_WARNING: ${NTFY_TOPIC_WARNING:-potatostack-warning}
      NTFY_TOPIC_INFO: ${NTFY_TOPIC_INFO:-potatostack-info}
      NTFY_TOKEN: ${NTFY_TOKEN:-}
    volumes:
      - ./scripts/alertmanager/alertmanager-ntfy.py:/app/alertmanager-ntfy.py:ro
    networks:
      - potatostack
    depends_on:
      ntfy:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M
        reservations:
          cpus: "0.05"
          memory: 32M

  ################################################################################
  # SYSTEM UTILITIES
  ################################################################################

  # ntfy - Notification hub (push/webhook)
  ntfy:
    image: binwiederhier/ntfy:${NTFY_TAG:-latest}
    container_name: ntfy
    logging: *default-logging
    command: serve
    ports:
      - "${HOST_BIND:-127.0.0.1}:8060:80"
    environment:
      <<: *common-env
      NTFY_BASE_URL: https://ntfy.${HOST_DOMAIN:-local.domain}
      NTFY_BEHIND_PROXY: "true"
      NTFY_AUTH_DEFAULT_ACCESS: ${NTFY_AUTH_DEFAULT_ACCESS:-read-write}
      NTFY_ENABLE_LOGIN: ${NTFY_ENABLE_LOGIN:-false}
      NTFY_ENABLE_METRICS: ${NTFY_ENABLE_METRICS:-true}
      NTFY_CACHE_FILE: /var/cache/ntfy/cache.db
      NTFY_AUTH_FILE: /var/lib/ntfy/user.db
    volumes:
      - ntfy-cache:/var/cache/ntfy
      - ntfy-data:/var/lib/ntfy
    networks:
      - potatostack
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:80/v1/health || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.2"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 64M
    labels:
      - "wud.trigger.docker.update=true"

  # What's Up Docker (WUD) - Docker Image Update Monitor with auto-update support
  wud:
    image: getwud/wud:${WUD_TAG:-latest}
    container_name: wud
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:3000:3000"
    environment:
      <<: *common-env
      WUD_WATCHER_LOCAL_CRON: "0 */6 * * *"
      WUD_WATCHER_LOCAL_WATCHBYDEFAULT: "true"
      WUD_LOG_LEVEL: info
      # Docker Hub authentication to avoid rate limits
      WUD_REGISTRY_DOCKERHUB_LOGIN: ${DOCKERHUB_LOGIN:-}
      WUD_REGISTRY_DOCKERHUB_PASSWORD: ${DOCKERHUB_PASSWORD:-}
      WUD_REGISTRY_DOCKERHUB_TOKEN: ${DOCKERHUB_TOKEN:-}
      # ntfy trigger
      WUD_TRIGGER_NTFY_POTATOSTACK_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      WUD_TRIGGER_NTFY_POTATOSTACK_TOPIC: ${NTFY_TOPIC:-potatostack}
      WUD_TRIGGER_NTFY_POTATOSTACK_PRIORITY: "3"
    volumes:
      - wud-data:/data
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - potatostack
    depends_on:
      - ntfy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.1"
          memory: 128M

  # Autoheal - Container health recovery
  autoheal:
    image: willfarrell/autoheal:${AUTOHEAL_TAG:-latest}
    container_name: autoheal
    logging: *default-logging
    environment:
      AUTOHEAL_CONTAINER_LABEL: autoheal
      AUTOHEAL_INTERVAL: 60
      AUTOHEAL_START_PERIOD: 300
      AUTOHEAL_DEFAULT_STOP_TIMEOUT: 10
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M
        reservations:
          cpus: "0.05"
          memory: 32M

  # OpenSSH Server - Secure remote shell and SFTP access
  openssh-server:
    image: lscr.io/linuxserver/openssh-server:${OPENSSH_SERVER_TAG:-latest}
    container_name: openssh-server
    logging: *default-logging
    hostname: openssh-server
    security_opt:
      - no-new-privileges:true
    ports:
      - "${HOST_BIND:-127.0.0.1}:${OPENSSH_PORT:-2222}:2222"
    environment:
      <<: *common-env
      USER_NAME: ${OPENSSH_USER:-sshuser}
      USER_PASSWORD: ${OPENSSH_PASSWORD}
      PASSWORD_ACCESS: ${OPENSSH_PASSWORD_ACCESS:-true}
      SUDO_ACCESS: ${OPENSSH_SUDO_ACCESS:-false}
      PUBLIC_KEY: ${OPENSSH_PUBLIC_KEY:-}
      PUBLIC_KEY_FILE: ${OPENSSH_PUBLIC_KEY_FILE:-}
      PUBLIC_KEY_DIR: ${OPENSSH_PUBLIC_KEY_DIR:-}
      PUBLIC_KEY_URL: ${OPENSSH_PUBLIC_KEY_URL:-}
      LOG_STDOUT: ${OPENSSH_LOG_STDOUT:-true}
    volumes:
      - openssh-config:/config
      - /mnt/storage:/data/storage
      - /mnt/cachehdd:/data/cachehdd
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
        reservations:
          cpus: "0.1"
          memory: 64M

  # Samba - Windows file sharing for HDD access
  samba:
    image: ghcr.io/servercontainers/samba:${SAMBA_TAG:-latest}
    container_name: samba
    logging: *default-logging
    network_mode: host
    cap_add:
      - NET_ADMIN
    environment:
      <<: *common-env
      SAMBA_GLOBAL_STANZA: |
        workgroup = WORKGROUP
        server string = PotatoStack Samba Server
        server role = standalone server
        log file = /var/log/samba/log.%m
        max log size = 50
        dns proxy = no
        security = user
        map to guest = Never
      # User configuration - use quoted password for special characters
      ACCOUNT_daniel: "${SAMBA_PASSWORD:-changeme}"
      UID_daniel: "${PUID:-1000}"
      SAMBA_VOLUME_CONFIG_storage: |
        [storage]
        path = /mnt/storage
        browsable = yes
        writable = yes
        read only = no
        guest ok = no
        valid users = daniel
        force user = daniel
        force group = daniel
        create mask = 0777
        directory mask = 0777
      SAMBA_VOLUME_CONFIG_cachehdd: |
        [cachehdd]
        path = /mnt/cachehdd
        browsable = yes
        writable = yes
        read only = no
        guest ok = no
        valid users = daniel
        force user = daniel
        force group = daniel
        create mask = 0777
        directory mask = 0777
      SAMBA_VOLUME_CONFIG_media: |
        [media]
        path = /mnt/storage/media
        browsable = yes
        writable = yes
        read only = no
        guest ok = no
        valid users = daniel
        force user = daniel
        force group = daniel
        create mask = 0777
        directory mask = 0777
    volumes:
      - /mnt/storage:/mnt/storage
      - /mnt/cachehdd:/mnt/cachehdd
      - samba-lib:/var/lib/samba
      - samba-cache:/var/cache/samba
      - samba-run:/run/samba
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "smbclient -L localhost -U% || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.1"
          memory: 64M

  # Home Assistant - Smart home automation with BLE/mDNS discovery
  home-assistant:
    image: ghcr.io/home-assistant/home-assistant:${HA_TAG:-stable}
    container_name: home-assistant
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:8123:8123"
    environment:
      <<: *common-env
    volumes:
      - /mnt/ssd/docker-data/home-assistant:/config
      - /etc/localtime:/etc/localtime:ro
      - /run/dbus:/run/dbus:ro
    network_mode: host
    privileged: true
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 1G

  ################################################################################
  # Gluetun Monitor - VPN Connection Monitor & Auto-Restart
  ################################################################################
  gluetun-monitor:
    image: docker:${DOCKER_TAG:-cli}
    container_name: gluetun-monitor
    logging: *default-logging
    user: "0:0"
    entrypoint: sh /monitor.sh
    environment:
      - GLUETUN_URL=http://gluetun:8008
      - CHECK_INTERVAL=${GLUETUN_CHECK_INTERVAL:-10}
      - RESTART_CONTAINERS= qbittorrent pyload slskd spotiflac stash rdt-client aria2 bitmagnet tdl
      - RESTART_ON_STOP=${GLUETUN_RESTART_ON_STOP:-true}
      - RESTART_ON_FAILURE=${GLUETUN_RESTART_ON_FAILURE:-true}
      - RESTART_COOLDOWN=${GLUETUN_RESTART_COOLDOWN:-120}
      - INITIAL_STARTUP_DELAY=${GLUETUN_INITIAL_DELAY:-120}
      - SKIP_INITIAL_CHECK=${GLUETUN_SKIP_INITIAL_CHECK:-true}
      - NTFY_INTERNAL_URL=${NTFY_INTERNAL_URL:-http://ntfy:80}
      - NTFY_TOPIC=${NTFY_TOPIC:-potatostack}
      - NTFY_TOKEN=${NTFY_TOKEN:-}
      - NTFY_DEFAULT_TAGS=${NTFY_DEFAULT_TAGS:-potatostack,monitor}
      - NTFY_DEFAULT_PRIORITY=${NTFY_DEFAULT_PRIORITY:-default}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./scripts/monitor/gluetun-monitor.sh:/monitor.sh:ro
      - ./scripts/monitor/notify.sh:/notify.sh:ro
      - .:/compose:ro
    working_dir: /compose
    networks:
      - potatostack
    depends_on:
      gluetun:
        condition: service_started
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.05"
          memory: 32M
        reservations:
          cpus: "0.025"
          memory: 16M

  # Disk Space Monitor
  disk-space-monitor:
    image: alpine:${ALPINE_TAG:-3.21}
    container_name: disk-space-monitor
    logging: *default-logging
    entrypoint: sh /disk-space-monitor.sh
    environment:
      DISK_MONITOR_PATHS: ${DISK_MONITOR_PATHS:-/mnt/storage /mnt/ssd /mnt/cachehdd}
      DISK_MONITOR_INTERVAL: ${DISK_MONITOR_INTERVAL:-300}
      DISK_MONITOR_WARN: ${DISK_MONITOR_WARN:-80}
      DISK_MONITOR_CRIT: ${DISK_MONITOR_CRIT:-90}
      NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
      NTFY_TOKEN: ${NTFY_TOKEN:-}
      NTFY_DEFAULT_TAGS: ${NTFY_DEFAULT_TAGS:-potatostack,monitor}
      NTFY_DEFAULT_PRIORITY: ${NTFY_DEFAULT_PRIORITY:-default}
    volumes:
      - /mnt/storage:/mnt/storage:ro
      - /mnt/cachehdd:/mnt/cachehdd:ro
      - /mnt/ssd:/mnt/ssd:ro
      - ./scripts/monitor/disk-space-monitor.sh:/disk-space-monitor.sh:ro
      - ./scripts/monitor/notify.sh:/notify.sh:ro
    networks:
      - potatostack
    restart: unless-stopped

  # Backup Monitor
  backup-monitor:
    image: alpine:${ALPINE_TAG:-3.21}
    container_name: backup-monitor
    logging: *default-logging
    entrypoint: sh /backup-monitor.sh
    environment:
      BACKUP_MONITOR_PATHS: ${BACKUP_MONITOR_PATHS:-/mnt/storage/stack-snapshot.log /mnt/storage/velld/backups}
      BACKUP_MAX_AGE_HOURS: ${BACKUP_MAX_AGE_HOURS:-48}
      BACKUP_MONITOR_INTERVAL: ${BACKUP_MONITOR_INTERVAL:-3600}
      NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
      NTFY_TOKEN: ${NTFY_TOKEN:-}
      NTFY_DEFAULT_TAGS: ${NTFY_DEFAULT_TAGS:-potatostack,monitor}
      NTFY_DEFAULT_PRIORITY: ${NTFY_DEFAULT_PRIORITY:-default}
    volumes:
      - /mnt/storage:/mnt/storage:ro
      - ./scripts/monitor/backup-monitor.sh:/backup-monitor.sh:ro
      - ./scripts/monitor/notify.sh:/notify.sh:ro
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M
        reservations:
          cpus: "0.05"
          memory: 32M

  # DB Health Monitor
  db-health-monitor:
    image: docker:${DOCKER_TAG:-cli}
    container_name: db-health-monitor
    logging: *default-logging
    entrypoint: sh /db-health-monitor.sh
    environment:
      DB_MONITOR_INTERVAL: ${DB_MONITOR_INTERVAL:-30}
      DB_FAIL_THRESHOLD: ${DB_FAIL_THRESHOLD:-3}
      DB_RESTART_COOLDOWN: ${DB_RESTART_COOLDOWN:-180}
      DB_RESTART_ON_FAILURE: ${DB_RESTART_ON_FAILURE:-true}
      DB_CHECK_POSTGRES: ${DB_CHECK_POSTGRES:-true}
      DB_CHECK_REDIS: ${DB_CHECK_REDIS:-true}
      DB_CHECK_MONGO: ${DB_CHECK_MONGO:-false}
      POSTGRES_CONTAINER: postgres
      REDIS_CONTAINER: redis-cache
      MONGO_CONTAINER: mongo
      NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
      NTFY_TOKEN: ${NTFY_TOKEN:-}
      NTFY_DEFAULT_TAGS: ${NTFY_DEFAULT_TAGS:-potatostack,monitor}
      NTFY_DEFAULT_PRIORITY: ${NTFY_DEFAULT_PRIORITY:-default}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./scripts/monitor/db-health-monitor.sh:/db-health-monitor.sh:ro
      - ./scripts/monitor/notify.sh:/notify.sh:ro
    depends_on:
      - postgres
      - redis-cache
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M
        reservations:
          cpus: "0.05"
          memory: 32M

  # Tailscale Connectivity Monitor
  tailscale-connectivity-monitor:
    image: docker:${DOCKER_TAG:-cli}
    container_name: tailscale-connectivity-monitor
    logging: *default-logging
    entrypoint: sh /tailscale-connectivity-monitor.sh
    environment:
      TAILSCALE_CONTAINER: tailscale
      TAILSCALE_PING_TARGET: ${TAILSCALE_PING_TARGET:-}
      TAILSCALE_PING_INTERVAL: ${TAILSCALE_PING_INTERVAL:-60}
      TAILSCALE_PING_FAIL_THRESHOLD: ${TAILSCALE_PING_FAIL_THRESHOLD:-3}
      TAILSCALE_RESTART_COOLDOWN: ${TAILSCALE_RESTART_COOLDOWN:-300}
      NTFY_INTERNAL_URL: ${NTFY_INTERNAL_URL:-http://ntfy:80}
      NTFY_TOPIC: ${NTFY_TOPIC:-potatostack}
      NTFY_TOKEN: ${NTFY_TOKEN:-}
      NTFY_DEFAULT_TAGS: ${NTFY_DEFAULT_TAGS:-potatostack,monitor}
      NTFY_DEFAULT_PRIORITY: ${NTFY_DEFAULT_PRIORITY:-default}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./scripts/monitor/tailscale-connectivity-monitor.sh:/tailscale-connectivity-monitor.sh:ro
      - ./scripts/monitor/notify.sh:/notify.sh:ro
    depends_on:
      - slskd
    networks:
      - potatostack
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M
        reservations:
          cpus: "0.05"
          memory: 32M

  ################################################################################
  # Velld - Database Backup Scheduler (Postgres, Mongo, Redis)
  ################################################################################
  # Velld - Database Backup Scheduler (Postgres, Mongo, Redis)
  ################################################################################
  velld-api:
    image: ghcr.io/dendianugerah/velld/api:${VELLD_API_TAG:-latest}
    container_name: velld-api
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:8083:8080"
    environment:
      <<: *common-env
      JWT_SECRET: ${VELLD_JWT_SECRET}
      ENCRYPTION_KEY: ${VELLD_ENCRYPTION_KEY}
      ADMIN_USERNAME_CREDENTIAL: ${VELLD_ADMIN_USERNAME}
      ADMIN_PASSWORD_CREDENTIAL: ${VELLD_ADMIN_PASSWORD}
      ALLOW_REGISTER: ${VELLD_ALLOW_REGISTER:-false}
    volumes:
      - /mnt/ssd/docker-data/velld:/app/data
      - /mnt/storage/velld/backups:/app/backups
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 128M

  velld-web:
    image: ghcr.io/dendianugerah/velld/web:${VELLD_WEB_TAG:-latest}
    container_name: velld-web
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:3010:3000"
    environment:
      <<: *common-env
      NEXT_PUBLIC_API_URL: ${VELLD_API_URL}
      ALLOW_REGISTER: ${VELLD_ALLOW_REGISTER:-false}
    networks:
      - potatostack
    depends_on:
      - velld-api
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 64M

  ################################################################################
  # Backrest - Restic Backup WebUI & Scheduler
  ################################################################################
  backrest:
    image: ghcr.io/garethgeorge/backrest:${BACKREST_TAG:-latest}
    container_name: backrest
    hostname: backrest
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:9898:9898"
    environment:
      <<: *common-env
      BACKREST_DATA: /data
      BACKREST_CONFIG: /config/config.json
      XDG_CACHE_HOME: /cache
      TMPDIR: /tmp/backrest
      BACKREST_PORT: "0.0.0.0:9898"
    volumes:
      # Config & database on SSD
      - /mnt/ssd/docker-data/backrest/data:/data
      - /mnt/ssd/docker-data/backrest/config:/config
      # Cache on cache HDD
      - /mnt/cachehdd/backrest/cache:/cache
      - /mnt/cachehdd/backrest/tmp:/tmp/backrest
      # Repos on HDD (large backup data)
      - /mnt/storage/backrest/repos:/repos
      # Read-write access to ALL data for backing up
      - /mnt/storage:/mnt/storage
      - /mnt/ssd/docker-data:/mnt/ssd/docker-data
      - /mnt/cachehdd:/mnt/cachehdd
    networks:
      - potatostack
    depends_on:
      storage-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:9898 || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 128M

  # Snapshot Scheduler - Cron-based Kopia snapshots
  snapshot-scheduler:
    image: docker:${DOCKER_CLI_TAG:-27.2.1}-cli
    container_name: snapshot-scheduler
    logging: *default-logging
    entrypoint: sh -c "chmod 600 /etc/crontabs/root 2>/dev/null || true; crond -f -l 8"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /mnt/ssd/docker-data/cron:/etc/crontabs
      - ./scripts/backup/stack-snapshot.sh:/stack-snapshot.sh:ro
    networks:
      - potatostack
    depends_on:
      - storage-init
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M

  # SearXNG - Privacy-respecting metasearch engine
  searxng:
    image: searxng/searxng:${SEARXNG_TAG:-latest}
    container_name: searxng
    logging: *default-logging
    ports:
      - "${HOST_BIND:-127.0.0.1}:8180:8080"
    environment:
      <<: *common-env
      SEARXNG_BASE_URL: https://search.${HOST_DOMAIN:-local.domain}
      SEARXNG_SECRET: ${SEARXNG_SECRET_KEY}
    volumes:
      - ./config/searxng:/etc/searxng:rw
    networks:
      - potatostack
    depends_on:
      - redis-cache
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:8080/healthz | grep -q OK || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.1"
          memory: 128M
    labels:
      - "wud.trigger.docker.update=true"

################################################################################
# NETWORKS
################################################################################
networks:
  potatostack:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: br-potato-main
    ipam:
      config:
        - subnet: 172.22.0.0/16

################################################################################
# VOLUMES
################################################################################
volumes:
  # Databases use bind mounts to /mnt/ssd/docker-data/

  # Authentication
  authentik-media:
  authentik-custom-templates:
  authentik-certs:

  # VPN
  gluetun-config:
  tailscale-data:
  tailscale-https-marker:

  # Cloud Storage
  # nextcloud_aio_mastercontainer:  # DISABLED
  syncthing-config:

  # Media Management
  prowlarr-config:
  sonarr-config:
  radarr-config:
  lidarr-config:
  # bookshelf-config:
  bazarr-config:
  maintainerr-data:
  jellyfin-config:
  jellyseerr-config:
  audiobookshelf-config:

  # Downloads
  rdt-client-data:
  qbittorrent-config:
  sabnzbd-config:
  aria2-config:
  pyload-config:
  slskd-config:
  tdl-data:
  unpackerr-config:
  
  # Shared secrets
  shared-keys:

  # Monitoring
  # prometheus uses bind mount to /mnt/cachehdd/observability/prometheus
  netdata-config:
  netdata-lib:
  netdata-cache:
  alloy-data:

  # Automation
  # n8n-data:
  # rssbridge-config: # removed
  # Utilities
  # stirling-pdf-data:
  # stirling-pdf-configs:
  code-server-config:
  atuin-config:

  # Development
  gitea-data:
  gitea-runner-data:
  sentry-data:

  # AI & Special
  # open-webui-data:
  octobot-data:
  octobot-tentacles:
  octobot-logs:
  # pinchflat-config:

  # Dashboard & Container Management
  homarr-data: # Legacy v0.x - keep for migration backup
  homarr-icons: # Legacy v0.x - keep for migration backup
  homarr-appdata: # New v1.x volume

  # Security
  crowdsec-db:
  crowdsec-config:

  # DNS & Ad Blocking (DISABLED)
  # adguard-work:
  # adguard-conf:

  # Monitoring

  # Finance
  # Utilities (DISABLED - paperless commented out)
  # paperless-data:
  # paperless-media:

  # System
  wud-data:
  ntfy-cache:
  ntfy-data:
  openssh-config:
  samba-lib:
  samba-cache:
  samba-run:

  # Enterprise Security & Secrets
  fail2ban-data:

  # Enterprise Observability
  alertmanager-data:
